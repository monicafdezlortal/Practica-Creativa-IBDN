[2025-07-11T11:46:22.841+0200] {taskinstance.py:1083} INFO - Dependencies all met for <TaskInstance: agile_data_science_batch_prediction_model_training.pyspark_train_classifier_model manual__2025-07-10T10:27:09.636040+00:00 [queued]>
[2025-07-11T11:46:22.858+0200] {taskinstance.py:1083} INFO - Dependencies all met for <TaskInstance: agile_data_science_batch_prediction_model_training.pyspark_train_classifier_model manual__2025-07-10T10:27:09.636040+00:00 [queued]>
[2025-07-11T11:46:22.858+0200] {taskinstance.py:1279} INFO - 
--------------------------------------------------------------------------------
[2025-07-11T11:46:22.858+0200] {taskinstance.py:1280} INFO - Starting attempt 4 of 4
[2025-07-11T11:46:22.858+0200] {taskinstance.py:1281} INFO - 
--------------------------------------------------------------------------------
[2025-07-11T11:46:22.880+0200] {taskinstance.py:1300} INFO - Executing <Task(BashOperator): pyspark_train_classifier_model> on 2025-07-10 10:27:09.636040+00:00
[2025-07-11T11:46:22.883+0200] {standard_task_runner.py:55} INFO - Started process 89812 to run task
[2025-07-11T11:46:22.886+0200] {standard_task_runner.py:82} INFO - Running: ['airflow', 'tasks', 'run', 'agile_data_science_batch_prediction_model_training', 'pyspark_train_classifier_model', 'manual__2025-07-10T10:27:09.636040+00:00', '--job-id', '138', '--raw', '--subdir', 'DAGS_FOLDER/setup.py', '--cfg-path', '/tmp/tmpcn3plbry']
[2025-07-11T11:46:22.888+0200] {standard_task_runner.py:83} INFO - Job 138: Subtask pyspark_train_classifier_model
[2025-07-11T11:46:22.974+0200] {task_command.py:388} INFO - Running <TaskInstance: agile_data_science_batch_prediction_model_training.pyspark_train_classifier_model manual__2025-07-10T10:27:09.636040+00:00 [running]> on host l014.lab.dit.upm.es
[2025-07-11T11:46:23.049+0200] {taskinstance.py:1507} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=airflow
AIRFLOW_CTX_DAG_ID=agile_data_science_batch_prediction_model_training
AIRFLOW_CTX_TASK_ID=pyspark_train_classifier_model
AIRFLOW_CTX_EXECUTION_DATE=2025-07-10T10:27:09.636040+00:00
AIRFLOW_CTX_TRY_NUMBER=4
AIRFLOW_CTX_DAG_RUN_ID=manual__2025-07-10T10:27:09.636040+00:00
[2025-07-11T11:46:23.052+0200] {subprocess.py:63} INFO - Tmp dir root location: 
 /tmp
[2025-07-11T11:46:23.053+0200] {subprocess.py:75} INFO - Running command: ['/usr/bin/bash', '-c', '\nsource /home/monica.fernandez/practica_creativa/venv-airflow/bin/activate && export PYSPARK_PYTHON=/home/monica.fernandez/practica_creativa/venv-airflow/bin/python && spark-submit --master local[4]   --packages com.datastax.spark:spark-cassandra-connector_2.12:3.5.0   /home/monica.fernandez/practica_creativa/resources/train_spark_mllib_model.py   /home/monica.fernandez/practica_creativa']
[2025-07-11T11:46:23.062+0200] {subprocess.py:86} INFO - Output:
[2025-07-11T11:46:26.086+0200] {subprocess.py:93} INFO - 25/07/11 11:46:26 WARN Utils: Your hostname, l014 resolves to a loopback address: 127.0.1.1; using 138.4.31.14 instead (on interface enp1s0)
[2025-07-11T11:46:26.088+0200] {subprocess.py:93} INFO - 25/07/11 11:46:26 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
[2025-07-11T11:46:26.368+0200] {subprocess.py:93} INFO - :: loading settings :: url = jar:file:/home/monica.fernandez/.sdkman/candidates/spark/3.5.3/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml
[2025-07-11T11:46:26.512+0200] {subprocess.py:93} INFO - Ivy Default Cache set to: /home/monica.fernandez/.ivy2/cache
[2025-07-11T11:46:26.513+0200] {subprocess.py:93} INFO - The jars for the packages stored in: /home/monica.fernandez/.ivy2/jars
[2025-07-11T11:46:26.518+0200] {subprocess.py:93} INFO - com.datastax.spark#spark-cassandra-connector_2.12 added as a dependency
[2025-07-11T11:46:26.520+0200] {subprocess.py:93} INFO - :: resolving dependencies :: org.apache.spark#spark-submit-parent-570f9772-7c4c-4ba8-bb30-790af138ff93;1.0
[2025-07-11T11:46:26.520+0200] {subprocess.py:93} INFO - 	confs: [default]
[2025-07-11T11:46:26.739+0200] {subprocess.py:93} INFO - 	found com.datastax.spark#spark-cassandra-connector_2.12;3.5.0 in central
[2025-07-11T11:46:26.775+0200] {subprocess.py:93} INFO - 	found com.datastax.spark#spark-cassandra-connector-driver_2.12;3.5.0 in central
[2025-07-11T11:46:26.809+0200] {subprocess.py:93} INFO - 	found org.scala-lang.modules#scala-collection-compat_2.12;2.11.0 in central
[2025-07-11T11:46:26.857+0200] {subprocess.py:93} INFO - 	found com.datastax.oss#java-driver-core-shaded;4.13.0 in central
[2025-07-11T11:46:26.892+0200] {subprocess.py:93} INFO - 	found com.datastax.oss#native-protocol;1.5.0 in central
[2025-07-11T11:46:26.924+0200] {subprocess.py:93} INFO - 	found com.datastax.oss#java-driver-shaded-guava;25.1-jre-graal-sub-1 in central
[2025-07-11T11:46:26.958+0200] {subprocess.py:93} INFO - 	found com.typesafe#config;1.4.1 in central
[2025-07-11T11:46:26.980+0200] {subprocess.py:93} INFO - 	found org.slf4j#slf4j-api;1.7.26 in central
[2025-07-11T11:46:27.007+0200] {subprocess.py:93} INFO - 	found io.dropwizard.metrics#metrics-core;4.1.18 in central
[2025-07-11T11:46:27.027+0200] {subprocess.py:93} INFO - 	found org.hdrhistogram#HdrHistogram;2.1.12 in central
[2025-07-11T11:46:27.048+0200] {subprocess.py:93} INFO - 	found org.reactivestreams#reactive-streams;1.0.3 in central
[2025-07-11T11:46:27.072+0200] {subprocess.py:93} INFO - 	found com.github.stephenc.jcip#jcip-annotations;1.0-1 in central
[2025-07-11T11:46:27.096+0200] {subprocess.py:93} INFO - 	found com.github.spotbugs#spotbugs-annotations;3.1.12 in central
[2025-07-11T11:46:27.117+0200] {subprocess.py:93} INFO - 	found com.google.code.findbugs#jsr305;3.0.2 in central
[2025-07-11T11:46:27.146+0200] {subprocess.py:93} INFO - 	found com.datastax.oss#java-driver-mapper-runtime;4.13.0 in central
[2025-07-11T11:46:27.175+0200] {subprocess.py:93} INFO - 	found com.datastax.oss#java-driver-query-builder;4.13.0 in central
[2025-07-11T11:46:27.202+0200] {subprocess.py:93} INFO - 	found org.apache.commons#commons-lang3;3.10 in central
[2025-07-11T11:46:27.223+0200] {subprocess.py:93} INFO - 	found com.thoughtworks.paranamer#paranamer;2.8 in central
[2025-07-11T11:46:27.240+0200] {subprocess.py:93} INFO - 	found org.scala-lang#scala-reflect;2.12.11 in central
[2025-07-11T11:46:27.316+0200] {subprocess.py:93} INFO - :: resolution report :: resolve 739ms :: artifacts dl 58ms
[2025-07-11T11:46:27.317+0200] {subprocess.py:93} INFO - 	:: modules in use:
[2025-07-11T11:46:27.317+0200] {subprocess.py:93} INFO - 	com.datastax.oss#java-driver-core-shaded;4.13.0 from central in [default]
[2025-07-11T11:46:27.317+0200] {subprocess.py:93} INFO - 	com.datastax.oss#java-driver-mapper-runtime;4.13.0 from central in [default]
[2025-07-11T11:46:27.318+0200] {subprocess.py:93} INFO - 	com.datastax.oss#java-driver-query-builder;4.13.0 from central in [default]
[2025-07-11T11:46:27.318+0200] {subprocess.py:93} INFO - 	com.datastax.oss#java-driver-shaded-guava;25.1-jre-graal-sub-1 from central in [default]
[2025-07-11T11:46:27.318+0200] {subprocess.py:93} INFO - 	com.datastax.oss#native-protocol;1.5.0 from central in [default]
[2025-07-11T11:46:27.318+0200] {subprocess.py:93} INFO - 	com.datastax.spark#spark-cassandra-connector-driver_2.12;3.5.0 from central in [default]
[2025-07-11T11:46:27.318+0200] {subprocess.py:93} INFO - 	com.datastax.spark#spark-cassandra-connector_2.12;3.5.0 from central in [default]
[2025-07-11T11:46:27.318+0200] {subprocess.py:93} INFO - 	com.github.spotbugs#spotbugs-annotations;3.1.12 from central in [default]
[2025-07-11T11:46:27.318+0200] {subprocess.py:93} INFO - 	com.github.stephenc.jcip#jcip-annotations;1.0-1 from central in [default]
[2025-07-11T11:46:27.319+0200] {subprocess.py:93} INFO - 	com.google.code.findbugs#jsr305;3.0.2 from central in [default]
[2025-07-11T11:46:27.319+0200] {subprocess.py:93} INFO - 	com.thoughtworks.paranamer#paranamer;2.8 from central in [default]
[2025-07-11T11:46:27.319+0200] {subprocess.py:93} INFO - 	com.typesafe#config;1.4.1 from central in [default]
[2025-07-11T11:46:27.320+0200] {subprocess.py:93} INFO - 	io.dropwizard.metrics#metrics-core;4.1.18 from central in [default]
[2025-07-11T11:46:27.320+0200] {subprocess.py:93} INFO - 	org.apache.commons#commons-lang3;3.10 from central in [default]
[2025-07-11T11:46:27.321+0200] {subprocess.py:93} INFO - 	org.hdrhistogram#HdrHistogram;2.1.12 from central in [default]
[2025-07-11T11:46:27.321+0200] {subprocess.py:93} INFO - 	org.reactivestreams#reactive-streams;1.0.3 from central in [default]
[2025-07-11T11:46:27.321+0200] {subprocess.py:93} INFO - 	org.scala-lang#scala-reflect;2.12.11 from central in [default]
[2025-07-11T11:46:27.321+0200] {subprocess.py:93} INFO - 	org.scala-lang.modules#scala-collection-compat_2.12;2.11.0 from central in [default]
[2025-07-11T11:46:27.322+0200] {subprocess.py:93} INFO - 	org.slf4j#slf4j-api;1.7.26 from central in [default]
[2025-07-11T11:46:27.322+0200] {subprocess.py:93} INFO - 	---------------------------------------------------------------------
[2025-07-11T11:46:27.322+0200] {subprocess.py:93} INFO - 	|                  |            modules            ||   artifacts   |
[2025-07-11T11:46:27.323+0200] {subprocess.py:93} INFO - 	|       conf       | number| search|dwnlded|evicted|| number|dwnlded|
[2025-07-11T11:46:27.323+0200] {subprocess.py:93} INFO - 	---------------------------------------------------------------------
[2025-07-11T11:46:27.323+0200] {subprocess.py:93} INFO - 	|      default     |   19  |   0   |   0   |   0   ||   19  |   0   |
[2025-07-11T11:46:27.323+0200] {subprocess.py:93} INFO - 	---------------------------------------------------------------------
[2025-07-11T11:46:27.335+0200] {subprocess.py:93} INFO - :: retrieving :: org.apache.spark#spark-submit-parent-570f9772-7c4c-4ba8-bb30-790af138ff93
[2025-07-11T11:46:27.335+0200] {subprocess.py:93} INFO - 	confs: [default]
[2025-07-11T11:46:27.348+0200] {subprocess.py:93} INFO - 	0 artifacts copied, 19 already retrieved (0kB/12ms)
[2025-07-11T11:46:27.695+0200] {subprocess.py:93} INFO - 25/07/11 11:46:27 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2025-07-11T11:46:32.411+0200] {subprocess.py:93} INFO - 25/07/11 11:46:32 INFO SparkContext: Running Spark version 3.5.3
[2025-07-11T11:46:32.412+0200] {subprocess.py:93} INFO - 25/07/11 11:46:32 INFO SparkContext: OS info Linux, 6.1.0-35-amd64, amd64
[2025-07-11T11:46:32.413+0200] {subprocess.py:93} INFO - 25/07/11 11:46:32 INFO SparkContext: Java version 17.0.14
[2025-07-11T11:46:32.447+0200] {subprocess.py:93} INFO - 25/07/11 11:46:32 INFO ResourceUtils: ==============================================================
[2025-07-11T11:46:32.448+0200] {subprocess.py:93} INFO - 25/07/11 11:46:32 INFO ResourceUtils: No custom resources configured for spark.driver.
[2025-07-11T11:46:32.449+0200] {subprocess.py:93} INFO - 25/07/11 11:46:32 INFO ResourceUtils: ==============================================================
[2025-07-11T11:46:32.450+0200] {subprocess.py:93} INFO - 25/07/11 11:46:32 INFO SparkContext: Submitted application: train_spark_mllib_model.py
[2025-07-11T11:46:32.485+0200] {subprocess.py:93} INFO - 25/07/11 11:46:32 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
[2025-07-11T11:46:32.496+0200] {subprocess.py:93} INFO - 25/07/11 11:46:32 INFO ResourceProfile: Limiting resource is cpu
[2025-07-11T11:46:32.498+0200] {subprocess.py:93} INFO - 25/07/11 11:46:32 INFO ResourceProfileManager: Added ResourceProfile id: 0
[2025-07-11T11:46:32.606+0200] {subprocess.py:93} INFO - 25/07/11 11:46:32 INFO SecurityManager: Changing view acls to: monica.fernandez
[2025-07-11T11:46:32.606+0200] {subprocess.py:93} INFO - 25/07/11 11:46:32 INFO SecurityManager: Changing modify acls to: monica.fernandez
[2025-07-11T11:46:32.607+0200] {subprocess.py:93} INFO - 25/07/11 11:46:32 INFO SecurityManager: Changing view acls groups to:
[2025-07-11T11:46:32.607+0200] {subprocess.py:93} INFO - 25/07/11 11:46:32 INFO SecurityManager: Changing modify acls groups to:
[2025-07-11T11:46:32.608+0200] {subprocess.py:93} INFO - 25/07/11 11:46:32 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: monica.fernandez; groups with view permissions: EMPTY; users with modify permissions: monica.fernandez; groups with modify permissions: EMPTY
[2025-07-11T11:46:32.914+0200] {subprocess.py:93} INFO - 25/07/11 11:46:32 INFO Utils: Successfully started service 'sparkDriver' on port 34911.
[2025-07-11T11:46:32.970+0200] {subprocess.py:93} INFO - 25/07/11 11:46:32 INFO SparkEnv: Registering MapOutputTracker
[2025-07-11T11:46:33.020+0200] {subprocess.py:93} INFO - 25/07/11 11:46:33 INFO SparkEnv: Registering BlockManagerMaster
[2025-07-11T11:46:33.048+0200] {subprocess.py:93} INFO - 25/07/11 11:46:33 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[2025-07-11T11:46:33.049+0200] {subprocess.py:93} INFO - 25/07/11 11:46:33 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
[2025-07-11T11:46:33.055+0200] {subprocess.py:93} INFO - 25/07/11 11:46:33 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
[2025-07-11T11:46:33.093+0200] {subprocess.py:93} INFO - 25/07/11 11:46:33 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-887573ae-16dd-4e2e-9de3-f2eb977b2eba
[2025-07-11T11:46:33.119+0200] {subprocess.py:93} INFO - 25/07/11 11:46:33 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB
[2025-07-11T11:46:33.146+0200] {subprocess.py:93} INFO - 25/07/11 11:46:33 INFO SparkEnv: Registering OutputCommitCoordinator
[2025-07-11T11:46:33.354+0200] {subprocess.py:93} INFO - 25/07/11 11:46:33 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI
[2025-07-11T11:46:33.432+0200] {subprocess.py:93} INFO - 25/07/11 11:46:33 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
[2025-07-11T11:46:33.443+0200] {subprocess.py:93} INFO - 25/07/11 11:46:33 INFO Utils: Successfully started service 'SparkUI' on port 4041.
[2025-07-11T11:46:33.500+0200] {subprocess.py:93} INFO - 25/07/11 11:46:33 INFO SparkContext: Added JAR file:///home/monica.fernandez/.ivy2/jars/com.datastax.spark_spark-cassandra-connector_2.12-3.5.0.jar at spark://138.4.31.14:34911/jars/com.datastax.spark_spark-cassandra-connector_2.12-3.5.0.jar with timestamp 1752227192403
[2025-07-11T11:46:33.500+0200] {subprocess.py:93} INFO - 25/07/11 11:46:33 INFO SparkContext: Added JAR file:///home/monica.fernandez/.ivy2/jars/com.datastax.spark_spark-cassandra-connector-driver_2.12-3.5.0.jar at spark://138.4.31.14:34911/jars/com.datastax.spark_spark-cassandra-connector-driver_2.12-3.5.0.jar with timestamp 1752227192403
[2025-07-11T11:46:33.502+0200] {subprocess.py:93} INFO - 25/07/11 11:46:33 INFO SparkContext: Added JAR file:///home/monica.fernandez/.ivy2/jars/org.scala-lang.modules_scala-collection-compat_2.12-2.11.0.jar at spark://138.4.31.14:34911/jars/org.scala-lang.modules_scala-collection-compat_2.12-2.11.0.jar with timestamp 1752227192403
[2025-07-11T11:46:33.502+0200] {subprocess.py:93} INFO - 25/07/11 11:46:33 INFO SparkContext: Added JAR file:///home/monica.fernandez/.ivy2/jars/com.datastax.oss_java-driver-core-shaded-4.13.0.jar at spark://138.4.31.14:34911/jars/com.datastax.oss_java-driver-core-shaded-4.13.0.jar with timestamp 1752227192403
[2025-07-11T11:46:33.503+0200] {subprocess.py:93} INFO - 25/07/11 11:46:33 INFO SparkContext: Added JAR file:///home/monica.fernandez/.ivy2/jars/com.datastax.oss_java-driver-mapper-runtime-4.13.0.jar at spark://138.4.31.14:34911/jars/com.datastax.oss_java-driver-mapper-runtime-4.13.0.jar with timestamp 1752227192403
[2025-07-11T11:46:33.505+0200] {subprocess.py:93} INFO - 25/07/11 11:46:33 INFO SparkContext: Added JAR file:///home/monica.fernandez/.ivy2/jars/org.apache.commons_commons-lang3-3.10.jar at spark://138.4.31.14:34911/jars/org.apache.commons_commons-lang3-3.10.jar with timestamp 1752227192403
[2025-07-11T11:46:33.508+0200] {subprocess.py:93} INFO - 25/07/11 11:46:33 INFO SparkContext: Added JAR file:///home/monica.fernandez/.ivy2/jars/com.thoughtworks.paranamer_paranamer-2.8.jar at spark://138.4.31.14:34911/jars/com.thoughtworks.paranamer_paranamer-2.8.jar with timestamp 1752227192403
[2025-07-11T11:46:33.509+0200] {subprocess.py:93} INFO - 25/07/11 11:46:33 INFO SparkContext: Added JAR file:///home/monica.fernandez/.ivy2/jars/org.scala-lang_scala-reflect-2.12.11.jar at spark://138.4.31.14:34911/jars/org.scala-lang_scala-reflect-2.12.11.jar with timestamp 1752227192403
[2025-07-11T11:46:33.511+0200] {subprocess.py:93} INFO - 25/07/11 11:46:33 INFO SparkContext: Added JAR file:///home/monica.fernandez/.ivy2/jars/com.datastax.oss_native-protocol-1.5.0.jar at spark://138.4.31.14:34911/jars/com.datastax.oss_native-protocol-1.5.0.jar with timestamp 1752227192403
[2025-07-11T11:46:33.511+0200] {subprocess.py:93} INFO - 25/07/11 11:46:33 INFO SparkContext: Added JAR file:///home/monica.fernandez/.ivy2/jars/com.datastax.oss_java-driver-shaded-guava-25.1-jre-graal-sub-1.jar at spark://138.4.31.14:34911/jars/com.datastax.oss_java-driver-shaded-guava-25.1-jre-graal-sub-1.jar with timestamp 1752227192403
[2025-07-11T11:46:33.512+0200] {subprocess.py:93} INFO - 25/07/11 11:46:33 INFO SparkContext: Added JAR file:///home/monica.fernandez/.ivy2/jars/com.typesafe_config-1.4.1.jar at spark://138.4.31.14:34911/jars/com.typesafe_config-1.4.1.jar with timestamp 1752227192403
[2025-07-11T11:46:33.513+0200] {subprocess.py:93} INFO - 25/07/11 11:46:33 INFO SparkContext: Added JAR file:///home/monica.fernandez/.ivy2/jars/org.slf4j_slf4j-api-1.7.26.jar at spark://138.4.31.14:34911/jars/org.slf4j_slf4j-api-1.7.26.jar with timestamp 1752227192403
[2025-07-11T11:46:33.514+0200] {subprocess.py:93} INFO - 25/07/11 11:46:33 INFO SparkContext: Added JAR file:///home/monica.fernandez/.ivy2/jars/io.dropwizard.metrics_metrics-core-4.1.18.jar at spark://138.4.31.14:34911/jars/io.dropwizard.metrics_metrics-core-4.1.18.jar with timestamp 1752227192403
[2025-07-11T11:46:33.514+0200] {subprocess.py:93} INFO - 25/07/11 11:46:33 INFO SparkContext: Added JAR file:///home/monica.fernandez/.ivy2/jars/org.hdrhistogram_HdrHistogram-2.1.12.jar at spark://138.4.31.14:34911/jars/org.hdrhistogram_HdrHistogram-2.1.12.jar with timestamp 1752227192403
[2025-07-11T11:46:33.514+0200] {subprocess.py:93} INFO - 25/07/11 11:46:33 INFO SparkContext: Added JAR file:///home/monica.fernandez/.ivy2/jars/org.reactivestreams_reactive-streams-1.0.3.jar at spark://138.4.31.14:34911/jars/org.reactivestreams_reactive-streams-1.0.3.jar with timestamp 1752227192403
[2025-07-11T11:46:33.515+0200] {subprocess.py:93} INFO - 25/07/11 11:46:33 INFO SparkContext: Added JAR file:///home/monica.fernandez/.ivy2/jars/com.github.stephenc.jcip_jcip-annotations-1.0-1.jar at spark://138.4.31.14:34911/jars/com.github.stephenc.jcip_jcip-annotations-1.0-1.jar with timestamp 1752227192403
[2025-07-11T11:46:33.516+0200] {subprocess.py:93} INFO - 25/07/11 11:46:33 INFO SparkContext: Added JAR file:///home/monica.fernandez/.ivy2/jars/com.github.spotbugs_spotbugs-annotations-3.1.12.jar at spark://138.4.31.14:34911/jars/com.github.spotbugs_spotbugs-annotations-3.1.12.jar with timestamp 1752227192403
[2025-07-11T11:46:33.517+0200] {subprocess.py:93} INFO - 25/07/11 11:46:33 INFO SparkContext: Added JAR file:///home/monica.fernandez/.ivy2/jars/com.google.code.findbugs_jsr305-3.0.2.jar at spark://138.4.31.14:34911/jars/com.google.code.findbugs_jsr305-3.0.2.jar with timestamp 1752227192403
[2025-07-11T11:46:33.517+0200] {subprocess.py:93} INFO - 25/07/11 11:46:33 INFO SparkContext: Added JAR file:///home/monica.fernandez/.ivy2/jars/com.datastax.oss_java-driver-query-builder-4.13.0.jar at spark://138.4.31.14:34911/jars/com.datastax.oss_java-driver-query-builder-4.13.0.jar with timestamp 1752227192403
[2025-07-11T11:46:33.520+0200] {subprocess.py:93} INFO - 25/07/11 11:46:33 INFO SparkContext: Added file file:///home/monica.fernandez/.ivy2/jars/com.datastax.spark_spark-cassandra-connector_2.12-3.5.0.jar at file:///home/monica.fernandez/.ivy2/jars/com.datastax.spark_spark-cassandra-connector_2.12-3.5.0.jar with timestamp 1752227192403
[2025-07-11T11:46:33.522+0200] {subprocess.py:93} INFO - 25/07/11 11:46:33 INFO Utils: Copying /home/monica.fernandez/.ivy2/jars/com.datastax.spark_spark-cassandra-connector_2.12-3.5.0.jar to /tmp/spark-7342345b-73e9-4615-b392-312f8b72e9b5/userFiles-70ba53b7-9586-4fbc-b243-8b39f9032e24/com.datastax.spark_spark-cassandra-connector_2.12-3.5.0.jar
[2025-07-11T11:46:33.547+0200] {subprocess.py:93} INFO - 25/07/11 11:46:33 INFO SparkContext: Added file file:///home/monica.fernandez/.ivy2/jars/com.datastax.spark_spark-cassandra-connector-driver_2.12-3.5.0.jar at file:///home/monica.fernandez/.ivy2/jars/com.datastax.spark_spark-cassandra-connector-driver_2.12-3.5.0.jar with timestamp 1752227192403
[2025-07-11T11:46:33.547+0200] {subprocess.py:93} INFO - 25/07/11 11:46:33 INFO Utils: Copying /home/monica.fernandez/.ivy2/jars/com.datastax.spark_spark-cassandra-connector-driver_2.12-3.5.0.jar to /tmp/spark-7342345b-73e9-4615-b392-312f8b72e9b5/userFiles-70ba53b7-9586-4fbc-b243-8b39f9032e24/com.datastax.spark_spark-cassandra-connector-driver_2.12-3.5.0.jar
[2025-07-11T11:46:33.561+0200] {subprocess.py:93} INFO - 25/07/11 11:46:33 INFO SparkContext: Added file file:///home/monica.fernandez/.ivy2/jars/org.scala-lang.modules_scala-collection-compat_2.12-2.11.0.jar at file:///home/monica.fernandez/.ivy2/jars/org.scala-lang.modules_scala-collection-compat_2.12-2.11.0.jar with timestamp 1752227192403
[2025-07-11T11:46:33.562+0200] {subprocess.py:93} INFO - 25/07/11 11:46:33 INFO Utils: Copying /home/monica.fernandez/.ivy2/jars/org.scala-lang.modules_scala-collection-compat_2.12-2.11.0.jar to /tmp/spark-7342345b-73e9-4615-b392-312f8b72e9b5/userFiles-70ba53b7-9586-4fbc-b243-8b39f9032e24/org.scala-lang.modules_scala-collection-compat_2.12-2.11.0.jar
[2025-07-11T11:46:33.568+0200] {subprocess.py:93} INFO - 25/07/11 11:46:33 INFO SparkContext: Added file file:///home/monica.fernandez/.ivy2/jars/com.datastax.oss_java-driver-core-shaded-4.13.0.jar at file:///home/monica.fernandez/.ivy2/jars/com.datastax.oss_java-driver-core-shaded-4.13.0.jar with timestamp 1752227192403
[2025-07-11T11:46:33.569+0200] {subprocess.py:93} INFO - 25/07/11 11:46:33 INFO Utils: Copying /home/monica.fernandez/.ivy2/jars/com.datastax.oss_java-driver-core-shaded-4.13.0.jar to /tmp/spark-7342345b-73e9-4615-b392-312f8b72e9b5/userFiles-70ba53b7-9586-4fbc-b243-8b39f9032e24/com.datastax.oss_java-driver-core-shaded-4.13.0.jar
[2025-07-11T11:46:33.631+0200] {subprocess.py:93} INFO - 25/07/11 11:46:33 INFO SparkContext: Added file file:///home/monica.fernandez/.ivy2/jars/com.datastax.oss_java-driver-mapper-runtime-4.13.0.jar at file:///home/monica.fernandez/.ivy2/jars/com.datastax.oss_java-driver-mapper-runtime-4.13.0.jar with timestamp 1752227192403
[2025-07-11T11:46:33.632+0200] {subprocess.py:93} INFO - 25/07/11 11:46:33 INFO Utils: Copying /home/monica.fernandez/.ivy2/jars/com.datastax.oss_java-driver-mapper-runtime-4.13.0.jar to /tmp/spark-7342345b-73e9-4615-b392-312f8b72e9b5/userFiles-70ba53b7-9586-4fbc-b243-8b39f9032e24/com.datastax.oss_java-driver-mapper-runtime-4.13.0.jar
[2025-07-11T11:46:33.636+0200] {subprocess.py:93} INFO - 25/07/11 11:46:33 INFO SparkContext: Added file file:///home/monica.fernandez/.ivy2/jars/org.apache.commons_commons-lang3-3.10.jar at file:///home/monica.fernandez/.ivy2/jars/org.apache.commons_commons-lang3-3.10.jar with timestamp 1752227192403
[2025-07-11T11:46:33.636+0200] {subprocess.py:93} INFO - 25/07/11 11:46:33 INFO Utils: Copying /home/monica.fernandez/.ivy2/jars/org.apache.commons_commons-lang3-3.10.jar to /tmp/spark-7342345b-73e9-4615-b392-312f8b72e9b5/userFiles-70ba53b7-9586-4fbc-b243-8b39f9032e24/org.apache.commons_commons-lang3-3.10.jar
[2025-07-11T11:46:33.645+0200] {subprocess.py:93} INFO - 25/07/11 11:46:33 INFO SparkContext: Added file file:///home/monica.fernandez/.ivy2/jars/com.thoughtworks.paranamer_paranamer-2.8.jar at file:///home/monica.fernandez/.ivy2/jars/com.thoughtworks.paranamer_paranamer-2.8.jar with timestamp 1752227192403
[2025-07-11T11:46:33.645+0200] {subprocess.py:93} INFO - 25/07/11 11:46:33 INFO Utils: Copying /home/monica.fernandez/.ivy2/jars/com.thoughtworks.paranamer_paranamer-2.8.jar to /tmp/spark-7342345b-73e9-4615-b392-312f8b72e9b5/userFiles-70ba53b7-9586-4fbc-b243-8b39f9032e24/com.thoughtworks.paranamer_paranamer-2.8.jar
[2025-07-11T11:46:33.650+0200] {subprocess.py:93} INFO - 25/07/11 11:46:33 INFO SparkContext: Added file file:///home/monica.fernandez/.ivy2/jars/org.scala-lang_scala-reflect-2.12.11.jar at file:///home/monica.fernandez/.ivy2/jars/org.scala-lang_scala-reflect-2.12.11.jar with timestamp 1752227192403
[2025-07-11T11:46:33.650+0200] {subprocess.py:93} INFO - 25/07/11 11:46:33 INFO Utils: Copying /home/monica.fernandez/.ivy2/jars/org.scala-lang_scala-reflect-2.12.11.jar to /tmp/spark-7342345b-73e9-4615-b392-312f8b72e9b5/userFiles-70ba53b7-9586-4fbc-b243-8b39f9032e24/org.scala-lang_scala-reflect-2.12.11.jar
[2025-07-11T11:46:33.684+0200] {subprocess.py:93} INFO - 25/07/11 11:46:33 INFO SparkContext: Added file file:///home/monica.fernandez/.ivy2/jars/com.datastax.oss_native-protocol-1.5.0.jar at file:///home/monica.fernandez/.ivy2/jars/com.datastax.oss_native-protocol-1.5.0.jar with timestamp 1752227192403
[2025-07-11T11:46:33.685+0200] {subprocess.py:93} INFO - 25/07/11 11:46:33 INFO Utils: Copying /home/monica.fernandez/.ivy2/jars/com.datastax.oss_native-protocol-1.5.0.jar to /tmp/spark-7342345b-73e9-4615-b392-312f8b72e9b5/userFiles-70ba53b7-9586-4fbc-b243-8b39f9032e24/com.datastax.oss_native-protocol-1.5.0.jar
[2025-07-11T11:46:33.691+0200] {subprocess.py:93} INFO - 25/07/11 11:46:33 INFO SparkContext: Added file file:///home/monica.fernandez/.ivy2/jars/com.datastax.oss_java-driver-shaded-guava-25.1-jre-graal-sub-1.jar at file:///home/monica.fernandez/.ivy2/jars/com.datastax.oss_java-driver-shaded-guava-25.1-jre-graal-sub-1.jar with timestamp 1752227192403
[2025-07-11T11:46:33.691+0200] {subprocess.py:93} INFO - 25/07/11 11:46:33 INFO Utils: Copying /home/monica.fernandez/.ivy2/jars/com.datastax.oss_java-driver-shaded-guava-25.1-jre-graal-sub-1.jar to /tmp/spark-7342345b-73e9-4615-b392-312f8b72e9b5/userFiles-70ba53b7-9586-4fbc-b243-8b39f9032e24/com.datastax.oss_java-driver-shaded-guava-25.1-jre-graal-sub-1.jar
[2025-07-11T11:46:33.717+0200] {subprocess.py:93} INFO - 25/07/11 11:46:33 INFO SparkContext: Added file file:///home/monica.fernandez/.ivy2/jars/com.typesafe_config-1.4.1.jar at file:///home/monica.fernandez/.ivy2/jars/com.typesafe_config-1.4.1.jar with timestamp 1752227192403
[2025-07-11T11:46:33.717+0200] {subprocess.py:93} INFO - 25/07/11 11:46:33 INFO Utils: Copying /home/monica.fernandez/.ivy2/jars/com.typesafe_config-1.4.1.jar to /tmp/spark-7342345b-73e9-4615-b392-312f8b72e9b5/userFiles-70ba53b7-9586-4fbc-b243-8b39f9032e24/com.typesafe_config-1.4.1.jar
[2025-07-11T11:46:33.723+0200] {subprocess.py:93} INFO - 25/07/11 11:46:33 INFO SparkContext: Added file file:///home/monica.fernandez/.ivy2/jars/org.slf4j_slf4j-api-1.7.26.jar at file:///home/monica.fernandez/.ivy2/jars/org.slf4j_slf4j-api-1.7.26.jar with timestamp 1752227192403
[2025-07-11T11:46:33.724+0200] {subprocess.py:93} INFO - 25/07/11 11:46:33 INFO Utils: Copying /home/monica.fernandez/.ivy2/jars/org.slf4j_slf4j-api-1.7.26.jar to /tmp/spark-7342345b-73e9-4615-b392-312f8b72e9b5/userFiles-70ba53b7-9586-4fbc-b243-8b39f9032e24/org.slf4j_slf4j-api-1.7.26.jar
[2025-07-11T11:46:33.728+0200] {subprocess.py:93} INFO - 25/07/11 11:46:33 INFO SparkContext: Added file file:///home/monica.fernandez/.ivy2/jars/io.dropwizard.metrics_metrics-core-4.1.18.jar at file:///home/monica.fernandez/.ivy2/jars/io.dropwizard.metrics_metrics-core-4.1.18.jar with timestamp 1752227192403
[2025-07-11T11:46:33.728+0200] {subprocess.py:93} INFO - 25/07/11 11:46:33 INFO Utils: Copying /home/monica.fernandez/.ivy2/jars/io.dropwizard.metrics_metrics-core-4.1.18.jar to /tmp/spark-7342345b-73e9-4615-b392-312f8b72e9b5/userFiles-70ba53b7-9586-4fbc-b243-8b39f9032e24/io.dropwizard.metrics_metrics-core-4.1.18.jar
[2025-07-11T11:46:33.732+0200] {subprocess.py:93} INFO - 25/07/11 11:46:33 INFO SparkContext: Added file file:///home/monica.fernandez/.ivy2/jars/org.hdrhistogram_HdrHistogram-2.1.12.jar at file:///home/monica.fernandez/.ivy2/jars/org.hdrhistogram_HdrHistogram-2.1.12.jar with timestamp 1752227192403
[2025-07-11T11:46:33.732+0200] {subprocess.py:93} INFO - 25/07/11 11:46:33 INFO Utils: Copying /home/monica.fernandez/.ivy2/jars/org.hdrhistogram_HdrHistogram-2.1.12.jar to /tmp/spark-7342345b-73e9-4615-b392-312f8b72e9b5/userFiles-70ba53b7-9586-4fbc-b243-8b39f9032e24/org.hdrhistogram_HdrHistogram-2.1.12.jar
[2025-07-11T11:46:33.738+0200] {subprocess.py:93} INFO - 25/07/11 11:46:33 INFO SparkContext: Added file file:///home/monica.fernandez/.ivy2/jars/org.reactivestreams_reactive-streams-1.0.3.jar at file:///home/monica.fernandez/.ivy2/jars/org.reactivestreams_reactive-streams-1.0.3.jar with timestamp 1752227192403
[2025-07-11T11:46:33.738+0200] {subprocess.py:93} INFO - 25/07/11 11:46:33 INFO Utils: Copying /home/monica.fernandez/.ivy2/jars/org.reactivestreams_reactive-streams-1.0.3.jar to /tmp/spark-7342345b-73e9-4615-b392-312f8b72e9b5/userFiles-70ba53b7-9586-4fbc-b243-8b39f9032e24/org.reactivestreams_reactive-streams-1.0.3.jar
[2025-07-11T11:46:33.741+0200] {subprocess.py:93} INFO - 25/07/11 11:46:33 INFO SparkContext: Added file file:///home/monica.fernandez/.ivy2/jars/com.github.stephenc.jcip_jcip-annotations-1.0-1.jar at file:///home/monica.fernandez/.ivy2/jars/com.github.stephenc.jcip_jcip-annotations-1.0-1.jar with timestamp 1752227192403
[2025-07-11T11:46:33.741+0200] {subprocess.py:93} INFO - 25/07/11 11:46:33 INFO Utils: Copying /home/monica.fernandez/.ivy2/jars/com.github.stephenc.jcip_jcip-annotations-1.0-1.jar to /tmp/spark-7342345b-73e9-4615-b392-312f8b72e9b5/userFiles-70ba53b7-9586-4fbc-b243-8b39f9032e24/com.github.stephenc.jcip_jcip-annotations-1.0-1.jar
[2025-07-11T11:46:33.745+0200] {subprocess.py:93} INFO - 25/07/11 11:46:33 INFO SparkContext: Added file file:///home/monica.fernandez/.ivy2/jars/com.github.spotbugs_spotbugs-annotations-3.1.12.jar at file:///home/monica.fernandez/.ivy2/jars/com.github.spotbugs_spotbugs-annotations-3.1.12.jar with timestamp 1752227192403
[2025-07-11T11:46:33.746+0200] {subprocess.py:93} INFO - 25/07/11 11:46:33 INFO Utils: Copying /home/monica.fernandez/.ivy2/jars/com.github.spotbugs_spotbugs-annotations-3.1.12.jar to /tmp/spark-7342345b-73e9-4615-b392-312f8b72e9b5/userFiles-70ba53b7-9586-4fbc-b243-8b39f9032e24/com.github.spotbugs_spotbugs-annotations-3.1.12.jar
[2025-07-11T11:46:33.749+0200] {subprocess.py:93} INFO - 25/07/11 11:46:33 INFO SparkContext: Added file file:///home/monica.fernandez/.ivy2/jars/com.google.code.findbugs_jsr305-3.0.2.jar at file:///home/monica.fernandez/.ivy2/jars/com.google.code.findbugs_jsr305-3.0.2.jar with timestamp 1752227192403
[2025-07-11T11:46:33.749+0200] {subprocess.py:93} INFO - 25/07/11 11:46:33 INFO Utils: Copying /home/monica.fernandez/.ivy2/jars/com.google.code.findbugs_jsr305-3.0.2.jar to /tmp/spark-7342345b-73e9-4615-b392-312f8b72e9b5/userFiles-70ba53b7-9586-4fbc-b243-8b39f9032e24/com.google.code.findbugs_jsr305-3.0.2.jar
[2025-07-11T11:46:33.753+0200] {subprocess.py:93} INFO - 25/07/11 11:46:33 INFO SparkContext: Added file file:///home/monica.fernandez/.ivy2/jars/com.datastax.oss_java-driver-query-builder-4.13.0.jar at file:///home/monica.fernandez/.ivy2/jars/com.datastax.oss_java-driver-query-builder-4.13.0.jar with timestamp 1752227192403
[2025-07-11T11:46:33.753+0200] {subprocess.py:93} INFO - 25/07/11 11:46:33 INFO Utils: Copying /home/monica.fernandez/.ivy2/jars/com.datastax.oss_java-driver-query-builder-4.13.0.jar to /tmp/spark-7342345b-73e9-4615-b392-312f8b72e9b5/userFiles-70ba53b7-9586-4fbc-b243-8b39f9032e24/com.datastax.oss_java-driver-query-builder-4.13.0.jar
[2025-07-11T11:46:33.824+0200] {subprocess.py:93} INFO - 25/07/11 11:46:33 INFO Executor: Starting executor ID driver on host 138.4.31.14
[2025-07-11T11:46:33.824+0200] {subprocess.py:93} INFO - 25/07/11 11:46:33 INFO Executor: OS info Linux, 6.1.0-35-amd64, amd64
[2025-07-11T11:46:33.825+0200] {subprocess.py:93} INFO - 25/07/11 11:46:33 INFO Executor: Java version 17.0.14
[2025-07-11T11:46:33.830+0200] {subprocess.py:93} INFO - 25/07/11 11:46:33 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''
[2025-07-11T11:46:33.831+0200] {subprocess.py:93} INFO - 25/07/11 11:46:33 INFO Executor: Created or updated repl class loader org.apache.spark.util.MutableURLClassLoader@552337c5 for default.
[2025-07-11T11:46:33.840+0200] {subprocess.py:93} INFO - 25/07/11 11:46:33 INFO Executor: Fetching file:///home/monica.fernandez/.ivy2/jars/com.datastax.spark_spark-cassandra-connector-driver_2.12-3.5.0.jar with timestamp 1752227192403
[2025-07-11T11:46:33.866+0200] {subprocess.py:93} INFO - 25/07/11 11:46:33 INFO Utils: /home/monica.fernandez/.ivy2/jars/com.datastax.spark_spark-cassandra-connector-driver_2.12-3.5.0.jar has been previously copied to /tmp/spark-7342345b-73e9-4615-b392-312f8b72e9b5/userFiles-70ba53b7-9586-4fbc-b243-8b39f9032e24/com.datastax.spark_spark-cassandra-connector-driver_2.12-3.5.0.jar
[2025-07-11T11:46:33.869+0200] {subprocess.py:93} INFO - 25/07/11 11:46:33 INFO Executor: Fetching file:///home/monica.fernandez/.ivy2/jars/com.google.code.findbugs_jsr305-3.0.2.jar with timestamp 1752227192403
[2025-07-11T11:46:33.870+0200] {subprocess.py:93} INFO - 25/07/11 11:46:33 INFO Utils: /home/monica.fernandez/.ivy2/jars/com.google.code.findbugs_jsr305-3.0.2.jar has been previously copied to /tmp/spark-7342345b-73e9-4615-b392-312f8b72e9b5/userFiles-70ba53b7-9586-4fbc-b243-8b39f9032e24/com.google.code.findbugs_jsr305-3.0.2.jar
[2025-07-11T11:46:33.873+0200] {subprocess.py:93} INFO - 25/07/11 11:46:33 INFO Executor: Fetching file:///home/monica.fernandez/.ivy2/jars/com.typesafe_config-1.4.1.jar with timestamp 1752227192403
[2025-07-11T11:46:33.874+0200] {subprocess.py:93} INFO - 25/07/11 11:46:33 INFO Utils: /home/monica.fernandez/.ivy2/jars/com.typesafe_config-1.4.1.jar has been previously copied to /tmp/spark-7342345b-73e9-4615-b392-312f8b72e9b5/userFiles-70ba53b7-9586-4fbc-b243-8b39f9032e24/com.typesafe_config-1.4.1.jar
[2025-07-11T11:46:33.878+0200] {subprocess.py:93} INFO - 25/07/11 11:46:33 INFO Executor: Fetching file:///home/monica.fernandez/.ivy2/jars/com.thoughtworks.paranamer_paranamer-2.8.jar with timestamp 1752227192403
[2025-07-11T11:46:33.879+0200] {subprocess.py:93} INFO - 25/07/11 11:46:33 INFO Utils: /home/monica.fernandez/.ivy2/jars/com.thoughtworks.paranamer_paranamer-2.8.jar has been previously copied to /tmp/spark-7342345b-73e9-4615-b392-312f8b72e9b5/userFiles-70ba53b7-9586-4fbc-b243-8b39f9032e24/com.thoughtworks.paranamer_paranamer-2.8.jar
[2025-07-11T11:46:33.882+0200] {subprocess.py:93} INFO - 25/07/11 11:46:33 INFO Executor: Fetching file:///home/monica.fernandez/.ivy2/jars/org.hdrhistogram_HdrHistogram-2.1.12.jar with timestamp 1752227192403
[2025-07-11T11:46:33.884+0200] {subprocess.py:93} INFO - 25/07/11 11:46:33 INFO Utils: /home/monica.fernandez/.ivy2/jars/org.hdrhistogram_HdrHistogram-2.1.12.jar has been previously copied to /tmp/spark-7342345b-73e9-4615-b392-312f8b72e9b5/userFiles-70ba53b7-9586-4fbc-b243-8b39f9032e24/org.hdrhistogram_HdrHistogram-2.1.12.jar
[2025-07-11T11:46:33.886+0200] {subprocess.py:93} INFO - 25/07/11 11:46:33 INFO Executor: Fetching file:///home/monica.fernandez/.ivy2/jars/com.github.spotbugs_spotbugs-annotations-3.1.12.jar with timestamp 1752227192403
[2025-07-11T11:46:33.887+0200] {subprocess.py:93} INFO - 25/07/11 11:46:33 INFO Utils: /home/monica.fernandez/.ivy2/jars/com.github.spotbugs_spotbugs-annotations-3.1.12.jar has been previously copied to /tmp/spark-7342345b-73e9-4615-b392-312f8b72e9b5/userFiles-70ba53b7-9586-4fbc-b243-8b39f9032e24/com.github.spotbugs_spotbugs-annotations-3.1.12.jar
[2025-07-11T11:46:33.894+0200] {subprocess.py:93} INFO - 25/07/11 11:46:33 INFO Executor: Fetching file:///home/monica.fernandez/.ivy2/jars/org.apache.commons_commons-lang3-3.10.jar with timestamp 1752227192403
[2025-07-11T11:46:33.895+0200] {subprocess.py:93} INFO - 25/07/11 11:46:33 INFO Utils: /home/monica.fernandez/.ivy2/jars/org.apache.commons_commons-lang3-3.10.jar has been previously copied to /tmp/spark-7342345b-73e9-4615-b392-312f8b72e9b5/userFiles-70ba53b7-9586-4fbc-b243-8b39f9032e24/org.apache.commons_commons-lang3-3.10.jar
[2025-07-11T11:46:33.898+0200] {subprocess.py:93} INFO - 25/07/11 11:46:33 INFO Executor: Fetching file:///home/monica.fernandez/.ivy2/jars/io.dropwizard.metrics_metrics-core-4.1.18.jar with timestamp 1752227192403
[2025-07-11T11:46:33.899+0200] {subprocess.py:93} INFO - 25/07/11 11:46:33 INFO Utils: /home/monica.fernandez/.ivy2/jars/io.dropwizard.metrics_metrics-core-4.1.18.jar has been previously copied to /tmp/spark-7342345b-73e9-4615-b392-312f8b72e9b5/userFiles-70ba53b7-9586-4fbc-b243-8b39f9032e24/io.dropwizard.metrics_metrics-core-4.1.18.jar
[2025-07-11T11:46:33.903+0200] {subprocess.py:93} INFO - 25/07/11 11:46:33 INFO Executor: Fetching file:///home/monica.fernandez/.ivy2/jars/com.datastax.oss_java-driver-shaded-guava-25.1-jre-graal-sub-1.jar with timestamp 1752227192403
[2025-07-11T11:46:33.907+0200] {subprocess.py:93} INFO - 25/07/11 11:46:33 INFO Utils: /home/monica.fernandez/.ivy2/jars/com.datastax.oss_java-driver-shaded-guava-25.1-jre-graal-sub-1.jar has been previously copied to /tmp/spark-7342345b-73e9-4615-b392-312f8b72e9b5/userFiles-70ba53b7-9586-4fbc-b243-8b39f9032e24/com.datastax.oss_java-driver-shaded-guava-25.1-jre-graal-sub-1.jar
[2025-07-11T11:46:33.915+0200] {subprocess.py:93} INFO - 25/07/11 11:46:33 INFO Executor: Fetching file:///home/monica.fernandez/.ivy2/jars/com.datastax.oss_java-driver-core-shaded-4.13.0.jar with timestamp 1752227192403
[2025-07-11T11:46:33.925+0200] {subprocess.py:93} INFO - 25/07/11 11:46:33 INFO Utils: /home/monica.fernandez/.ivy2/jars/com.datastax.oss_java-driver-core-shaded-4.13.0.jar has been previously copied to /tmp/spark-7342345b-73e9-4615-b392-312f8b72e9b5/userFiles-70ba53b7-9586-4fbc-b243-8b39f9032e24/com.datastax.oss_java-driver-core-shaded-4.13.0.jar
[2025-07-11T11:46:33.928+0200] {subprocess.py:93} INFO - 25/07/11 11:46:33 INFO Executor: Fetching file:///home/monica.fernandez/.ivy2/jars/com.datastax.oss_native-protocol-1.5.0.jar with timestamp 1752227192403
[2025-07-11T11:46:33.929+0200] {subprocess.py:93} INFO - 25/07/11 11:46:33 INFO Utils: /home/monica.fernandez/.ivy2/jars/com.datastax.oss_native-protocol-1.5.0.jar has been previously copied to /tmp/spark-7342345b-73e9-4615-b392-312f8b72e9b5/userFiles-70ba53b7-9586-4fbc-b243-8b39f9032e24/com.datastax.oss_native-protocol-1.5.0.jar
[2025-07-11T11:46:33.933+0200] {subprocess.py:93} INFO - 25/07/11 11:46:33 INFO Executor: Fetching file:///home/monica.fernandez/.ivy2/jars/org.slf4j_slf4j-api-1.7.26.jar with timestamp 1752227192403
[2025-07-11T11:46:33.934+0200] {subprocess.py:93} INFO - 25/07/11 11:46:33 INFO Utils: /home/monica.fernandez/.ivy2/jars/org.slf4j_slf4j-api-1.7.26.jar has been previously copied to /tmp/spark-7342345b-73e9-4615-b392-312f8b72e9b5/userFiles-70ba53b7-9586-4fbc-b243-8b39f9032e24/org.slf4j_slf4j-api-1.7.26.jar
[2025-07-11T11:46:33.937+0200] {subprocess.py:93} INFO - 25/07/11 11:46:33 INFO Executor: Fetching file:///home/monica.fernandez/.ivy2/jars/org.scala-lang.modules_scala-collection-compat_2.12-2.11.0.jar with timestamp 1752227192403
[2025-07-11T11:46:33.938+0200] {subprocess.py:93} INFO - 25/07/11 11:46:33 INFO Utils: /home/monica.fernandez/.ivy2/jars/org.scala-lang.modules_scala-collection-compat_2.12-2.11.0.jar has been previously copied to /tmp/spark-7342345b-73e9-4615-b392-312f8b72e9b5/userFiles-70ba53b7-9586-4fbc-b243-8b39f9032e24/org.scala-lang.modules_scala-collection-compat_2.12-2.11.0.jar
[2025-07-11T11:46:33.941+0200] {subprocess.py:93} INFO - 25/07/11 11:46:33 INFO Executor: Fetching file:///home/monica.fernandez/.ivy2/jars/com.datastax.spark_spark-cassandra-connector_2.12-3.5.0.jar with timestamp 1752227192403
[2025-07-11T11:46:33.945+0200] {subprocess.py:93} INFO - 25/07/11 11:46:33 INFO Utils: /home/monica.fernandez/.ivy2/jars/com.datastax.spark_spark-cassandra-connector_2.12-3.5.0.jar has been previously copied to /tmp/spark-7342345b-73e9-4615-b392-312f8b72e9b5/userFiles-70ba53b7-9586-4fbc-b243-8b39f9032e24/com.datastax.spark_spark-cassandra-connector_2.12-3.5.0.jar
[2025-07-11T11:46:33.948+0200] {subprocess.py:93} INFO - 25/07/11 11:46:33 INFO Executor: Fetching file:///home/monica.fernandez/.ivy2/jars/com.datastax.oss_java-driver-query-builder-4.13.0.jar with timestamp 1752227192403
[2025-07-11T11:46:33.949+0200] {subprocess.py:93} INFO - 25/07/11 11:46:33 INFO Utils: /home/monica.fernandez/.ivy2/jars/com.datastax.oss_java-driver-query-builder-4.13.0.jar has been previously copied to /tmp/spark-7342345b-73e9-4615-b392-312f8b72e9b5/userFiles-70ba53b7-9586-4fbc-b243-8b39f9032e24/com.datastax.oss_java-driver-query-builder-4.13.0.jar
[2025-07-11T11:46:33.953+0200] {subprocess.py:93} INFO - 25/07/11 11:46:33 INFO Executor: Fetching file:///home/monica.fernandez/.ivy2/jars/com.github.stephenc.jcip_jcip-annotations-1.0-1.jar with timestamp 1752227192403
[2025-07-11T11:46:33.953+0200] {subprocess.py:93} INFO - 25/07/11 11:46:33 INFO Utils: /home/monica.fernandez/.ivy2/jars/com.github.stephenc.jcip_jcip-annotations-1.0-1.jar has been previously copied to /tmp/spark-7342345b-73e9-4615-b392-312f8b72e9b5/userFiles-70ba53b7-9586-4fbc-b243-8b39f9032e24/com.github.stephenc.jcip_jcip-annotations-1.0-1.jar
[2025-07-11T11:46:33.956+0200] {subprocess.py:93} INFO - 25/07/11 11:46:33 INFO Executor: Fetching file:///home/monica.fernandez/.ivy2/jars/org.reactivestreams_reactive-streams-1.0.3.jar with timestamp 1752227192403
[2025-07-11T11:46:33.957+0200] {subprocess.py:93} INFO - 25/07/11 11:46:33 INFO Utils: /home/monica.fernandez/.ivy2/jars/org.reactivestreams_reactive-streams-1.0.3.jar has been previously copied to /tmp/spark-7342345b-73e9-4615-b392-312f8b72e9b5/userFiles-70ba53b7-9586-4fbc-b243-8b39f9032e24/org.reactivestreams_reactive-streams-1.0.3.jar
[2025-07-11T11:46:33.960+0200] {subprocess.py:93} INFO - 25/07/11 11:46:33 INFO Executor: Fetching file:///home/monica.fernandez/.ivy2/jars/com.datastax.oss_java-driver-mapper-runtime-4.13.0.jar with timestamp 1752227192403
[2025-07-11T11:46:33.961+0200] {subprocess.py:93} INFO - 25/07/11 11:46:33 INFO Utils: /home/monica.fernandez/.ivy2/jars/com.datastax.oss_java-driver-mapper-runtime-4.13.0.jar has been previously copied to /tmp/spark-7342345b-73e9-4615-b392-312f8b72e9b5/userFiles-70ba53b7-9586-4fbc-b243-8b39f9032e24/com.datastax.oss_java-driver-mapper-runtime-4.13.0.jar
[2025-07-11T11:46:33.966+0200] {subprocess.py:93} INFO - 25/07/11 11:46:33 INFO Executor: Fetching file:///home/monica.fernandez/.ivy2/jars/org.scala-lang_scala-reflect-2.12.11.jar with timestamp 1752227192403
[2025-07-11T11:46:33.971+0200] {subprocess.py:93} INFO - 25/07/11 11:46:33 INFO Utils: /home/monica.fernandez/.ivy2/jars/org.scala-lang_scala-reflect-2.12.11.jar has been previously copied to /tmp/spark-7342345b-73e9-4615-b392-312f8b72e9b5/userFiles-70ba53b7-9586-4fbc-b243-8b39f9032e24/org.scala-lang_scala-reflect-2.12.11.jar
[2025-07-11T11:46:33.976+0200] {subprocess.py:93} INFO - 25/07/11 11:46:33 INFO Executor: Fetching spark://138.4.31.14:34911/jars/com.datastax.oss_java-driver-shaded-guava-25.1-jre-graal-sub-1.jar with timestamp 1752227192403
[2025-07-11T11:46:34.022+0200] {subprocess.py:93} INFO - 25/07/11 11:46:34 INFO TransportClientFactory: Successfully created connection to /138.4.31.14:34911 after 31 ms (0 ms spent in bootstraps)
[2025-07-11T11:46:34.028+0200] {subprocess.py:93} INFO - 25/07/11 11:46:34 INFO Utils: Fetching spark://138.4.31.14:34911/jars/com.datastax.oss_java-driver-shaded-guava-25.1-jre-graal-sub-1.jar to /tmp/spark-7342345b-73e9-4615-b392-312f8b72e9b5/userFiles-70ba53b7-9586-4fbc-b243-8b39f9032e24/fetchFileTemp13765080467887423513.tmp
[2025-07-11T11:46:34.061+0200] {subprocess.py:93} INFO - 25/07/11 11:46:34 INFO Utils: /tmp/spark-7342345b-73e9-4615-b392-312f8b72e9b5/userFiles-70ba53b7-9586-4fbc-b243-8b39f9032e24/fetchFileTemp13765080467887423513.tmp has been previously copied to /tmp/spark-7342345b-73e9-4615-b392-312f8b72e9b5/userFiles-70ba53b7-9586-4fbc-b243-8b39f9032e24/com.datastax.oss_java-driver-shaded-guava-25.1-jre-graal-sub-1.jar
[2025-07-11T11:46:34.066+0200] {subprocess.py:93} INFO - 25/07/11 11:46:34 INFO Executor: Adding file:/tmp/spark-7342345b-73e9-4615-b392-312f8b72e9b5/userFiles-70ba53b7-9586-4fbc-b243-8b39f9032e24/com.datastax.oss_java-driver-shaded-guava-25.1-jre-graal-sub-1.jar to class loader default
[2025-07-11T11:46:34.066+0200] {subprocess.py:93} INFO - 25/07/11 11:46:34 INFO Executor: Fetching spark://138.4.31.14:34911/jars/com.google.code.findbugs_jsr305-3.0.2.jar with timestamp 1752227192403
[2025-07-11T11:46:34.066+0200] {subprocess.py:93} INFO - 25/07/11 11:46:34 INFO Utils: Fetching spark://138.4.31.14:34911/jars/com.google.code.findbugs_jsr305-3.0.2.jar to /tmp/spark-7342345b-73e9-4615-b392-312f8b72e9b5/userFiles-70ba53b7-9586-4fbc-b243-8b39f9032e24/fetchFileTemp837947321907332908.tmp
[2025-07-11T11:46:34.068+0200] {subprocess.py:93} INFO - 25/07/11 11:46:34 INFO Utils: /tmp/spark-7342345b-73e9-4615-b392-312f8b72e9b5/userFiles-70ba53b7-9586-4fbc-b243-8b39f9032e24/fetchFileTemp837947321907332908.tmp has been previously copied to /tmp/spark-7342345b-73e9-4615-b392-312f8b72e9b5/userFiles-70ba53b7-9586-4fbc-b243-8b39f9032e24/com.google.code.findbugs_jsr305-3.0.2.jar
[2025-07-11T11:46:34.071+0200] {subprocess.py:93} INFO - 25/07/11 11:46:34 INFO Executor: Adding file:/tmp/spark-7342345b-73e9-4615-b392-312f8b72e9b5/userFiles-70ba53b7-9586-4fbc-b243-8b39f9032e24/com.google.code.findbugs_jsr305-3.0.2.jar to class loader default
[2025-07-11T11:46:34.071+0200] {subprocess.py:93} INFO - 25/07/11 11:46:34 INFO Executor: Fetching spark://138.4.31.14:34911/jars/com.datastax.oss_native-protocol-1.5.0.jar with timestamp 1752227192403
[2025-07-11T11:46:34.072+0200] {subprocess.py:93} INFO - 25/07/11 11:46:34 INFO Utils: Fetching spark://138.4.31.14:34911/jars/com.datastax.oss_native-protocol-1.5.0.jar to /tmp/spark-7342345b-73e9-4615-b392-312f8b72e9b5/userFiles-70ba53b7-9586-4fbc-b243-8b39f9032e24/fetchFileTemp16202922752065055371.tmp
[2025-07-11T11:46:34.074+0200] {subprocess.py:93} INFO - 25/07/11 11:46:34 INFO Utils: /tmp/spark-7342345b-73e9-4615-b392-312f8b72e9b5/userFiles-70ba53b7-9586-4fbc-b243-8b39f9032e24/fetchFileTemp16202922752065055371.tmp has been previously copied to /tmp/spark-7342345b-73e9-4615-b392-312f8b72e9b5/userFiles-70ba53b7-9586-4fbc-b243-8b39f9032e24/com.datastax.oss_native-protocol-1.5.0.jar
[2025-07-11T11:46:34.078+0200] {subprocess.py:93} INFO - 25/07/11 11:46:34 INFO Executor: Adding file:/tmp/spark-7342345b-73e9-4615-b392-312f8b72e9b5/userFiles-70ba53b7-9586-4fbc-b243-8b39f9032e24/com.datastax.oss_native-protocol-1.5.0.jar to class loader default
[2025-07-11T11:46:34.078+0200] {subprocess.py:93} INFO - 25/07/11 11:46:34 INFO Executor: Fetching spark://138.4.31.14:34911/jars/com.github.spotbugs_spotbugs-annotations-3.1.12.jar with timestamp 1752227192403
[2025-07-11T11:46:34.079+0200] {subprocess.py:93} INFO - 25/07/11 11:46:34 INFO Utils: Fetching spark://138.4.31.14:34911/jars/com.github.spotbugs_spotbugs-annotations-3.1.12.jar to /tmp/spark-7342345b-73e9-4615-b392-312f8b72e9b5/userFiles-70ba53b7-9586-4fbc-b243-8b39f9032e24/fetchFileTemp7858446053732772377.tmp
[2025-07-11T11:46:34.080+0200] {subprocess.py:93} INFO - 25/07/11 11:46:34 INFO Utils: /tmp/spark-7342345b-73e9-4615-b392-312f8b72e9b5/userFiles-70ba53b7-9586-4fbc-b243-8b39f9032e24/fetchFileTemp7858446053732772377.tmp has been previously copied to /tmp/spark-7342345b-73e9-4615-b392-312f8b72e9b5/userFiles-70ba53b7-9586-4fbc-b243-8b39f9032e24/com.github.spotbugs_spotbugs-annotations-3.1.12.jar
[2025-07-11T11:46:34.084+0200] {subprocess.py:93} INFO - 25/07/11 11:46:34 INFO Executor: Adding file:/tmp/spark-7342345b-73e9-4615-b392-312f8b72e9b5/userFiles-70ba53b7-9586-4fbc-b243-8b39f9032e24/com.github.spotbugs_spotbugs-annotations-3.1.12.jar to class loader default
[2025-07-11T11:46:34.084+0200] {subprocess.py:93} INFO - 25/07/11 11:46:34 INFO Executor: Fetching spark://138.4.31.14:34911/jars/org.scala-lang_scala-reflect-2.12.11.jar with timestamp 1752227192403
[2025-07-11T11:46:34.085+0200] {subprocess.py:93} INFO - 25/07/11 11:46:34 INFO Utils: Fetching spark://138.4.31.14:34911/jars/org.scala-lang_scala-reflect-2.12.11.jar to /tmp/spark-7342345b-73e9-4615-b392-312f8b72e9b5/userFiles-70ba53b7-9586-4fbc-b243-8b39f9032e24/fetchFileTemp16428472227731347382.tmp
[2025-07-11T11:46:34.099+0200] {subprocess.py:93} INFO - 25/07/11 11:46:34 INFO Utils: /tmp/spark-7342345b-73e9-4615-b392-312f8b72e9b5/userFiles-70ba53b7-9586-4fbc-b243-8b39f9032e24/fetchFileTemp16428472227731347382.tmp has been previously copied to /tmp/spark-7342345b-73e9-4615-b392-312f8b72e9b5/userFiles-70ba53b7-9586-4fbc-b243-8b39f9032e24/org.scala-lang_scala-reflect-2.12.11.jar
[2025-07-11T11:46:34.103+0200] {subprocess.py:93} INFO - 25/07/11 11:46:34 INFO Executor: Adding file:/tmp/spark-7342345b-73e9-4615-b392-312f8b72e9b5/userFiles-70ba53b7-9586-4fbc-b243-8b39f9032e24/org.scala-lang_scala-reflect-2.12.11.jar to class loader default
[2025-07-11T11:46:34.103+0200] {subprocess.py:93} INFO - 25/07/11 11:46:34 INFO Executor: Fetching spark://138.4.31.14:34911/jars/com.datastax.spark_spark-cassandra-connector-driver_2.12-3.5.0.jar with timestamp 1752227192403
[2025-07-11T11:46:34.104+0200] {subprocess.py:93} INFO - 25/07/11 11:46:34 INFO Utils: Fetching spark://138.4.31.14:34911/jars/com.datastax.spark_spark-cassandra-connector-driver_2.12-3.5.0.jar to /tmp/spark-7342345b-73e9-4615-b392-312f8b72e9b5/userFiles-70ba53b7-9586-4fbc-b243-8b39f9032e24/fetchFileTemp15208133111679726136.tmp
[2025-07-11T11:46:34.110+0200] {subprocess.py:93} INFO - 25/07/11 11:46:34 INFO Utils: /tmp/spark-7342345b-73e9-4615-b392-312f8b72e9b5/userFiles-70ba53b7-9586-4fbc-b243-8b39f9032e24/fetchFileTemp15208133111679726136.tmp has been previously copied to /tmp/spark-7342345b-73e9-4615-b392-312f8b72e9b5/userFiles-70ba53b7-9586-4fbc-b243-8b39f9032e24/com.datastax.spark_spark-cassandra-connector-driver_2.12-3.5.0.jar
[2025-07-11T11:46:34.114+0200] {subprocess.py:93} INFO - 25/07/11 11:46:34 INFO Executor: Adding file:/tmp/spark-7342345b-73e9-4615-b392-312f8b72e9b5/userFiles-70ba53b7-9586-4fbc-b243-8b39f9032e24/com.datastax.spark_spark-cassandra-connector-driver_2.12-3.5.0.jar to class loader default
[2025-07-11T11:46:34.114+0200] {subprocess.py:93} INFO - 25/07/11 11:46:34 INFO Executor: Fetching spark://138.4.31.14:34911/jars/com.typesafe_config-1.4.1.jar with timestamp 1752227192403
[2025-07-11T11:46:34.114+0200] {subprocess.py:93} INFO - 25/07/11 11:46:34 INFO Utils: Fetching spark://138.4.31.14:34911/jars/com.typesafe_config-1.4.1.jar to /tmp/spark-7342345b-73e9-4615-b392-312f8b72e9b5/userFiles-70ba53b7-9586-4fbc-b243-8b39f9032e24/fetchFileTemp11492988098612926257.tmp
[2025-07-11T11:46:34.116+0200] {subprocess.py:93} INFO - 25/07/11 11:46:34 INFO Utils: /tmp/spark-7342345b-73e9-4615-b392-312f8b72e9b5/userFiles-70ba53b7-9586-4fbc-b243-8b39f9032e24/fetchFileTemp11492988098612926257.tmp has been previously copied to /tmp/spark-7342345b-73e9-4615-b392-312f8b72e9b5/userFiles-70ba53b7-9586-4fbc-b243-8b39f9032e24/com.typesafe_config-1.4.1.jar
[2025-07-11T11:46:34.120+0200] {subprocess.py:93} INFO - 25/07/11 11:46:34 INFO Executor: Adding file:/tmp/spark-7342345b-73e9-4615-b392-312f8b72e9b5/userFiles-70ba53b7-9586-4fbc-b243-8b39f9032e24/com.typesafe_config-1.4.1.jar to class loader default
[2025-07-11T11:46:34.120+0200] {subprocess.py:93} INFO - 25/07/11 11:46:34 INFO Executor: Fetching spark://138.4.31.14:34911/jars/com.datastax.spark_spark-cassandra-connector_2.12-3.5.0.jar with timestamp 1752227192403
[2025-07-11T11:46:34.120+0200] {subprocess.py:93} INFO - 25/07/11 11:46:34 INFO Utils: Fetching spark://138.4.31.14:34911/jars/com.datastax.spark_spark-cassandra-connector_2.12-3.5.0.jar to /tmp/spark-7342345b-73e9-4615-b392-312f8b72e9b5/userFiles-70ba53b7-9586-4fbc-b243-8b39f9032e24/fetchFileTemp15376161748709569937.tmp
[2025-07-11T11:46:34.126+0200] {subprocess.py:93} INFO - 25/07/11 11:46:34 INFO Utils: /tmp/spark-7342345b-73e9-4615-b392-312f8b72e9b5/userFiles-70ba53b7-9586-4fbc-b243-8b39f9032e24/fetchFileTemp15376161748709569937.tmp has been previously copied to /tmp/spark-7342345b-73e9-4615-b392-312f8b72e9b5/userFiles-70ba53b7-9586-4fbc-b243-8b39f9032e24/com.datastax.spark_spark-cassandra-connector_2.12-3.5.0.jar
[2025-07-11T11:46:34.130+0200] {subprocess.py:93} INFO - 25/07/11 11:46:34 INFO Executor: Adding file:/tmp/spark-7342345b-73e9-4615-b392-312f8b72e9b5/userFiles-70ba53b7-9586-4fbc-b243-8b39f9032e24/com.datastax.spark_spark-cassandra-connector_2.12-3.5.0.jar to class loader default
[2025-07-11T11:46:34.130+0200] {subprocess.py:93} INFO - 25/07/11 11:46:34 INFO Executor: Fetching spark://138.4.31.14:34911/jars/com.datastax.oss_java-driver-query-builder-4.13.0.jar with timestamp 1752227192403
[2025-07-11T11:46:34.131+0200] {subprocess.py:93} INFO - 25/07/11 11:46:34 INFO Utils: Fetching spark://138.4.31.14:34911/jars/com.datastax.oss_java-driver-query-builder-4.13.0.jar to /tmp/spark-7342345b-73e9-4615-b392-312f8b72e9b5/userFiles-70ba53b7-9586-4fbc-b243-8b39f9032e24/fetchFileTemp5955979055466245284.tmp
[2025-07-11T11:46:34.133+0200] {subprocess.py:93} INFO - 25/07/11 11:46:34 INFO Utils: /tmp/spark-7342345b-73e9-4615-b392-312f8b72e9b5/userFiles-70ba53b7-9586-4fbc-b243-8b39f9032e24/fetchFileTemp5955979055466245284.tmp has been previously copied to /tmp/spark-7342345b-73e9-4615-b392-312f8b72e9b5/userFiles-70ba53b7-9586-4fbc-b243-8b39f9032e24/com.datastax.oss_java-driver-query-builder-4.13.0.jar
[2025-07-11T11:46:34.136+0200] {subprocess.py:93} INFO - 25/07/11 11:46:34 INFO Executor: Adding file:/tmp/spark-7342345b-73e9-4615-b392-312f8b72e9b5/userFiles-70ba53b7-9586-4fbc-b243-8b39f9032e24/com.datastax.oss_java-driver-query-builder-4.13.0.jar to class loader default
[2025-07-11T11:46:34.136+0200] {subprocess.py:93} INFO - 25/07/11 11:46:34 INFO Executor: Fetching spark://138.4.31.14:34911/jars/com.datastax.oss_java-driver-mapper-runtime-4.13.0.jar with timestamp 1752227192403
[2025-07-11T11:46:34.136+0200] {subprocess.py:93} INFO - 25/07/11 11:46:34 INFO Utils: Fetching spark://138.4.31.14:34911/jars/com.datastax.oss_java-driver-mapper-runtime-4.13.0.jar to /tmp/spark-7342345b-73e9-4615-b392-312f8b72e9b5/userFiles-70ba53b7-9586-4fbc-b243-8b39f9032e24/fetchFileTemp12831365489631791653.tmp
[2025-07-11T11:46:34.138+0200] {subprocess.py:93} INFO - 25/07/11 11:46:34 INFO Utils: /tmp/spark-7342345b-73e9-4615-b392-312f8b72e9b5/userFiles-70ba53b7-9586-4fbc-b243-8b39f9032e24/fetchFileTemp12831365489631791653.tmp has been previously copied to /tmp/spark-7342345b-73e9-4615-b392-312f8b72e9b5/userFiles-70ba53b7-9586-4fbc-b243-8b39f9032e24/com.datastax.oss_java-driver-mapper-runtime-4.13.0.jar
[2025-07-11T11:46:34.141+0200] {subprocess.py:93} INFO - 25/07/11 11:46:34 INFO Executor: Adding file:/tmp/spark-7342345b-73e9-4615-b392-312f8b72e9b5/userFiles-70ba53b7-9586-4fbc-b243-8b39f9032e24/com.datastax.oss_java-driver-mapper-runtime-4.13.0.jar to class loader default
[2025-07-11T11:46:34.141+0200] {subprocess.py:93} INFO - 25/07/11 11:46:34 INFO Executor: Fetching spark://138.4.31.14:34911/jars/com.github.stephenc.jcip_jcip-annotations-1.0-1.jar with timestamp 1752227192403
[2025-07-11T11:46:34.142+0200] {subprocess.py:93} INFO - 25/07/11 11:46:34 INFO Utils: Fetching spark://138.4.31.14:34911/jars/com.github.stephenc.jcip_jcip-annotations-1.0-1.jar to /tmp/spark-7342345b-73e9-4615-b392-312f8b72e9b5/userFiles-70ba53b7-9586-4fbc-b243-8b39f9032e24/fetchFileTemp8552446835822187451.tmp
[2025-07-11T11:46:34.143+0200] {subprocess.py:93} INFO - 25/07/11 11:46:34 INFO Utils: /tmp/spark-7342345b-73e9-4615-b392-312f8b72e9b5/userFiles-70ba53b7-9586-4fbc-b243-8b39f9032e24/fetchFileTemp8552446835822187451.tmp has been previously copied to /tmp/spark-7342345b-73e9-4615-b392-312f8b72e9b5/userFiles-70ba53b7-9586-4fbc-b243-8b39f9032e24/com.github.stephenc.jcip_jcip-annotations-1.0-1.jar
[2025-07-11T11:46:34.146+0200] {subprocess.py:93} INFO - 25/07/11 11:46:34 INFO Executor: Adding file:/tmp/spark-7342345b-73e9-4615-b392-312f8b72e9b5/userFiles-70ba53b7-9586-4fbc-b243-8b39f9032e24/com.github.stephenc.jcip_jcip-annotations-1.0-1.jar to class loader default
[2025-07-11T11:46:34.147+0200] {subprocess.py:93} INFO - 25/07/11 11:46:34 INFO Executor: Fetching spark://138.4.31.14:34911/jars/org.slf4j_slf4j-api-1.7.26.jar with timestamp 1752227192403
[2025-07-11T11:46:34.147+0200] {subprocess.py:93} INFO - 25/07/11 11:46:34 INFO Utils: Fetching spark://138.4.31.14:34911/jars/org.slf4j_slf4j-api-1.7.26.jar to /tmp/spark-7342345b-73e9-4615-b392-312f8b72e9b5/userFiles-70ba53b7-9586-4fbc-b243-8b39f9032e24/fetchFileTemp10145436090207519152.tmp
[2025-07-11T11:46:34.148+0200] {subprocess.py:93} INFO - 25/07/11 11:46:34 INFO Utils: /tmp/spark-7342345b-73e9-4615-b392-312f8b72e9b5/userFiles-70ba53b7-9586-4fbc-b243-8b39f9032e24/fetchFileTemp10145436090207519152.tmp has been previously copied to /tmp/spark-7342345b-73e9-4615-b392-312f8b72e9b5/userFiles-70ba53b7-9586-4fbc-b243-8b39f9032e24/org.slf4j_slf4j-api-1.7.26.jar
[2025-07-11T11:46:34.152+0200] {subprocess.py:93} INFO - 25/07/11 11:46:34 INFO Executor: Adding file:/tmp/spark-7342345b-73e9-4615-b392-312f8b72e9b5/userFiles-70ba53b7-9586-4fbc-b243-8b39f9032e24/org.slf4j_slf4j-api-1.7.26.jar to class loader default
[2025-07-11T11:46:34.152+0200] {subprocess.py:93} INFO - 25/07/11 11:46:34 INFO Executor: Fetching spark://138.4.31.14:34911/jars/com.datastax.oss_java-driver-core-shaded-4.13.0.jar with timestamp 1752227192403
[2025-07-11T11:46:34.152+0200] {subprocess.py:93} INFO - 25/07/11 11:46:34 INFO Utils: Fetching spark://138.4.31.14:34911/jars/com.datastax.oss_java-driver-core-shaded-4.13.0.jar to /tmp/spark-7342345b-73e9-4615-b392-312f8b72e9b5/userFiles-70ba53b7-9586-4fbc-b243-8b39f9032e24/fetchFileTemp17088159869581319227.tmp
[2025-07-11T11:46:34.177+0200] {subprocess.py:93} INFO - 25/07/11 11:46:34 INFO Utils: /tmp/spark-7342345b-73e9-4615-b392-312f8b72e9b5/userFiles-70ba53b7-9586-4fbc-b243-8b39f9032e24/fetchFileTemp17088159869581319227.tmp has been previously copied to /tmp/spark-7342345b-73e9-4615-b392-312f8b72e9b5/userFiles-70ba53b7-9586-4fbc-b243-8b39f9032e24/com.datastax.oss_java-driver-core-shaded-4.13.0.jar
[2025-07-11T11:46:34.180+0200] {subprocess.py:93} INFO - 25/07/11 11:46:34 INFO Executor: Adding file:/tmp/spark-7342345b-73e9-4615-b392-312f8b72e9b5/userFiles-70ba53b7-9586-4fbc-b243-8b39f9032e24/com.datastax.oss_java-driver-core-shaded-4.13.0.jar to class loader default
[2025-07-11T11:46:34.181+0200] {subprocess.py:93} INFO - 25/07/11 11:46:34 INFO Executor: Fetching spark://138.4.31.14:34911/jars/org.scala-lang.modules_scala-collection-compat_2.12-2.11.0.jar with timestamp 1752227192403
[2025-07-11T11:46:34.181+0200] {subprocess.py:93} INFO - 25/07/11 11:46:34 INFO Utils: Fetching spark://138.4.31.14:34911/jars/org.scala-lang.modules_scala-collection-compat_2.12-2.11.0.jar to /tmp/spark-7342345b-73e9-4615-b392-312f8b72e9b5/userFiles-70ba53b7-9586-4fbc-b243-8b39f9032e24/fetchFileTemp4062205233426076030.tmp
[2025-07-11T11:46:34.184+0200] {subprocess.py:93} INFO - 25/07/11 11:46:34 INFO Utils: /tmp/spark-7342345b-73e9-4615-b392-312f8b72e9b5/userFiles-70ba53b7-9586-4fbc-b243-8b39f9032e24/fetchFileTemp4062205233426076030.tmp has been previously copied to /tmp/spark-7342345b-73e9-4615-b392-312f8b72e9b5/userFiles-70ba53b7-9586-4fbc-b243-8b39f9032e24/org.scala-lang.modules_scala-collection-compat_2.12-2.11.0.jar
[2025-07-11T11:46:34.187+0200] {subprocess.py:93} INFO - 25/07/11 11:46:34 INFO Executor: Adding file:/tmp/spark-7342345b-73e9-4615-b392-312f8b72e9b5/userFiles-70ba53b7-9586-4fbc-b243-8b39f9032e24/org.scala-lang.modules_scala-collection-compat_2.12-2.11.0.jar to class loader default
[2025-07-11T11:46:34.187+0200] {subprocess.py:93} INFO - 25/07/11 11:46:34 INFO Executor: Fetching spark://138.4.31.14:34911/jars/org.apache.commons_commons-lang3-3.10.jar with timestamp 1752227192403
[2025-07-11T11:46:34.188+0200] {subprocess.py:93} INFO - 25/07/11 11:46:34 INFO Utils: Fetching spark://138.4.31.14:34911/jars/org.apache.commons_commons-lang3-3.10.jar to /tmp/spark-7342345b-73e9-4615-b392-312f8b72e9b5/userFiles-70ba53b7-9586-4fbc-b243-8b39f9032e24/fetchFileTemp18095371854811604855.tmp
[2025-07-11T11:46:34.190+0200] {subprocess.py:93} INFO - 25/07/11 11:46:34 INFO Utils: /tmp/spark-7342345b-73e9-4615-b392-312f8b72e9b5/userFiles-70ba53b7-9586-4fbc-b243-8b39f9032e24/fetchFileTemp18095371854811604855.tmp has been previously copied to /tmp/spark-7342345b-73e9-4615-b392-312f8b72e9b5/userFiles-70ba53b7-9586-4fbc-b243-8b39f9032e24/org.apache.commons_commons-lang3-3.10.jar
[2025-07-11T11:46:34.193+0200] {subprocess.py:93} INFO - 25/07/11 11:46:34 INFO Executor: Adding file:/tmp/spark-7342345b-73e9-4615-b392-312f8b72e9b5/userFiles-70ba53b7-9586-4fbc-b243-8b39f9032e24/org.apache.commons_commons-lang3-3.10.jar to class loader default
[2025-07-11T11:46:34.194+0200] {subprocess.py:93} INFO - 25/07/11 11:46:34 INFO Executor: Fetching spark://138.4.31.14:34911/jars/com.thoughtworks.paranamer_paranamer-2.8.jar with timestamp 1752227192403
[2025-07-11T11:46:34.194+0200] {subprocess.py:93} INFO - 25/07/11 11:46:34 INFO Utils: Fetching spark://138.4.31.14:34911/jars/com.thoughtworks.paranamer_paranamer-2.8.jar to /tmp/spark-7342345b-73e9-4615-b392-312f8b72e9b5/userFiles-70ba53b7-9586-4fbc-b243-8b39f9032e24/fetchFileTemp6271265103510279123.tmp
[2025-07-11T11:46:34.195+0200] {subprocess.py:93} INFO - 25/07/11 11:46:34 INFO Utils: /tmp/spark-7342345b-73e9-4615-b392-312f8b72e9b5/userFiles-70ba53b7-9586-4fbc-b243-8b39f9032e24/fetchFileTemp6271265103510279123.tmp has been previously copied to /tmp/spark-7342345b-73e9-4615-b392-312f8b72e9b5/userFiles-70ba53b7-9586-4fbc-b243-8b39f9032e24/com.thoughtworks.paranamer_paranamer-2.8.jar
[2025-07-11T11:46:34.199+0200] {subprocess.py:93} INFO - 25/07/11 11:46:34 INFO Executor: Adding file:/tmp/spark-7342345b-73e9-4615-b392-312f8b72e9b5/userFiles-70ba53b7-9586-4fbc-b243-8b39f9032e24/com.thoughtworks.paranamer_paranamer-2.8.jar to class loader default
[2025-07-11T11:46:34.199+0200] {subprocess.py:93} INFO - 25/07/11 11:46:34 INFO Executor: Fetching spark://138.4.31.14:34911/jars/org.hdrhistogram_HdrHistogram-2.1.12.jar with timestamp 1752227192403
[2025-07-11T11:46:34.199+0200] {subprocess.py:93} INFO - 25/07/11 11:46:34 INFO Utils: Fetching spark://138.4.31.14:34911/jars/org.hdrhistogram_HdrHistogram-2.1.12.jar to /tmp/spark-7342345b-73e9-4615-b392-312f8b72e9b5/userFiles-70ba53b7-9586-4fbc-b243-8b39f9032e24/fetchFileTemp17294620144427026106.tmp
[2025-07-11T11:46:34.201+0200] {subprocess.py:93} INFO - 25/07/11 11:46:34 INFO Utils: /tmp/spark-7342345b-73e9-4615-b392-312f8b72e9b5/userFiles-70ba53b7-9586-4fbc-b243-8b39f9032e24/fetchFileTemp17294620144427026106.tmp has been previously copied to /tmp/spark-7342345b-73e9-4615-b392-312f8b72e9b5/userFiles-70ba53b7-9586-4fbc-b243-8b39f9032e24/org.hdrhistogram_HdrHistogram-2.1.12.jar
[2025-07-11T11:46:34.205+0200] {subprocess.py:93} INFO - 25/07/11 11:46:34 INFO Executor: Adding file:/tmp/spark-7342345b-73e9-4615-b392-312f8b72e9b5/userFiles-70ba53b7-9586-4fbc-b243-8b39f9032e24/org.hdrhistogram_HdrHistogram-2.1.12.jar to class loader default
[2025-07-11T11:46:34.205+0200] {subprocess.py:93} INFO - 25/07/11 11:46:34 INFO Executor: Fetching spark://138.4.31.14:34911/jars/io.dropwizard.metrics_metrics-core-4.1.18.jar with timestamp 1752227192403
[2025-07-11T11:46:34.206+0200] {subprocess.py:93} INFO - 25/07/11 11:46:34 INFO Utils: Fetching spark://138.4.31.14:34911/jars/io.dropwizard.metrics_metrics-core-4.1.18.jar to /tmp/spark-7342345b-73e9-4615-b392-312f8b72e9b5/userFiles-70ba53b7-9586-4fbc-b243-8b39f9032e24/fetchFileTemp932983248638024231.tmp
[2025-07-11T11:46:34.207+0200] {subprocess.py:93} INFO - 25/07/11 11:46:34 INFO Utils: /tmp/spark-7342345b-73e9-4615-b392-312f8b72e9b5/userFiles-70ba53b7-9586-4fbc-b243-8b39f9032e24/fetchFileTemp932983248638024231.tmp has been previously copied to /tmp/spark-7342345b-73e9-4615-b392-312f8b72e9b5/userFiles-70ba53b7-9586-4fbc-b243-8b39f9032e24/io.dropwizard.metrics_metrics-core-4.1.18.jar
[2025-07-11T11:46:34.210+0200] {subprocess.py:93} INFO - 25/07/11 11:46:34 INFO Executor: Adding file:/tmp/spark-7342345b-73e9-4615-b392-312f8b72e9b5/userFiles-70ba53b7-9586-4fbc-b243-8b39f9032e24/io.dropwizard.metrics_metrics-core-4.1.18.jar to class loader default
[2025-07-11T11:46:34.211+0200] {subprocess.py:93} INFO - 25/07/11 11:46:34 INFO Executor: Fetching spark://138.4.31.14:34911/jars/org.reactivestreams_reactive-streams-1.0.3.jar with timestamp 1752227192403
[2025-07-11T11:46:34.211+0200] {subprocess.py:93} INFO - 25/07/11 11:46:34 INFO Utils: Fetching spark://138.4.31.14:34911/jars/org.reactivestreams_reactive-streams-1.0.3.jar to /tmp/spark-7342345b-73e9-4615-b392-312f8b72e9b5/userFiles-70ba53b7-9586-4fbc-b243-8b39f9032e24/fetchFileTemp13298345318855124043.tmp
[2025-07-11T11:46:34.212+0200] {subprocess.py:93} INFO - 25/07/11 11:46:34 INFO Utils: /tmp/spark-7342345b-73e9-4615-b392-312f8b72e9b5/userFiles-70ba53b7-9586-4fbc-b243-8b39f9032e24/fetchFileTemp13298345318855124043.tmp has been previously copied to /tmp/spark-7342345b-73e9-4615-b392-312f8b72e9b5/userFiles-70ba53b7-9586-4fbc-b243-8b39f9032e24/org.reactivestreams_reactive-streams-1.0.3.jar
[2025-07-11T11:46:34.216+0200] {subprocess.py:93} INFO - 25/07/11 11:46:34 INFO Executor: Adding file:/tmp/spark-7342345b-73e9-4615-b392-312f8b72e9b5/userFiles-70ba53b7-9586-4fbc-b243-8b39f9032e24/org.reactivestreams_reactive-streams-1.0.3.jar to class loader default
[2025-07-11T11:46:34.224+0200] {subprocess.py:93} INFO - 25/07/11 11:46:34 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 39113.
[2025-07-11T11:46:34.224+0200] {subprocess.py:93} INFO - 25/07/11 11:46:34 INFO NettyBlockTransferService: Server created on 138.4.31.14:39113
[2025-07-11T11:46:34.226+0200] {subprocess.py:93} INFO - 25/07/11 11:46:34 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[2025-07-11T11:46:34.232+0200] {subprocess.py:93} INFO - 25/07/11 11:46:34 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 138.4.31.14, 39113, None)
[2025-07-11T11:46:34.236+0200] {subprocess.py:93} INFO - 25/07/11 11:46:34 INFO BlockManagerMasterEndpoint: Registering block manager 138.4.31.14:39113 with 434.4 MiB RAM, BlockManagerId(driver, 138.4.31.14, 39113, None)
[2025-07-11T11:46:34.239+0200] {subprocess.py:93} INFO - 25/07/11 11:46:34 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 138.4.31.14, 39113, None)
[2025-07-11T11:46:34.240+0200] {subprocess.py:93} INFO - 25/07/11 11:46:34 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 138.4.31.14, 39113, None)
[2025-07-11T11:46:34.834+0200] {subprocess.py:93} INFO - 25/07/11 11:46:34 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
[2025-07-11T11:46:34.836+0200] {subprocess.py:93} INFO - 25/07/11 11:46:34 INFO SharedState: Warehouse path is 'file:/tmp/airflowtmpgh1ux6l1/spark-warehouse'.
[2025-07-11T11:46:36.088+0200] {subprocess.py:93} INFO - 25/07/11 11:46:36 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.
[2025-07-11T11:46:37.638+0200] {subprocess.py:93} INFO - MLflow Run ID: acdfa4eb07534d6d898ea5e99c7abf36
[2025-07-11T11:46:37.639+0200] {subprocess.py:93} INFO - MLflow Tracking URI: file:///home/monica.fernandez/practica_creativa/mlruns
[2025-07-11T11:46:37.861+0200] {subprocess.py:93} INFO - 25/07/11 11:46:37 INFO InMemoryFileIndex: It took 40 ms to list leaf files for 1 paths.
[2025-07-11T11:46:40.075+0200] {subprocess.py:93} INFO - 25/07/11 11:46:40 INFO DefaultMavenCoordinates: DataStax Java driver for Apache Cassandra(R) (com.datastax.oss:java-driver-core-shaded) version 4.13.0
[2025-07-11T11:46:40.215+0200] {subprocess.py:93} INFO - 25/07/11 11:46:40 INFO Native: Unable to load JNR native implementation. This could be normal if JNR is excluded from the classpath
[2025-07-11T11:46:40.215+0200] {subprocess.py:93} INFO - java.lang.NoClassDefFoundError: jnr/posix/POSIXHandler
[2025-07-11T11:46:40.215+0200] {subprocess.py:93} INFO - 	at com.datastax.oss.driver.internal.core.os.Native$LibcLoader.load(Native.java:42)
[2025-07-11T11:46:40.215+0200] {subprocess.py:93} INFO - 	at com.datastax.oss.driver.internal.core.os.Native.<clinit>(Native.java:59)
[2025-07-11T11:46:40.215+0200] {subprocess.py:93} INFO - 	at com.datastax.oss.driver.internal.core.time.Clock.getInstance(Clock.java:41)
[2025-07-11T11:46:40.215+0200] {subprocess.py:93} INFO - 	at com.datastax.oss.driver.internal.core.time.MonotonicTimestampGenerator.buildClock(MonotonicTimestampGenerator.java:109)
[2025-07-11T11:46:40.215+0200] {subprocess.py:93} INFO - 	at com.datastax.oss.driver.internal.core.time.MonotonicTimestampGenerator.<init>(MonotonicTimestampGenerator.java:43)
[2025-07-11T11:46:40.215+0200] {subprocess.py:93} INFO - 	at com.datastax.oss.driver.internal.core.time.AtomicTimestampGenerator.<init>(AtomicTimestampGenerator.java:52)
[2025-07-11T11:46:40.215+0200] {subprocess.py:93} INFO - 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
[2025-07-11T11:46:40.215+0200] {subprocess.py:93} INFO - 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:77)
[2025-07-11T11:46:40.215+0200] {subprocess.py:93} INFO - 	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
[2025-07-11T11:46:40.215+0200] {subprocess.py:93} INFO - 	at java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:500)
[2025-07-11T11:46:40.215+0200] {subprocess.py:93} INFO - 	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:481)
[2025-07-11T11:46:40.215+0200] {subprocess.py:93} INFO - 	at com.datastax.oss.driver.internal.core.util.Reflection.resolveClass(Reflection.java:329)
[2025-07-11T11:46:40.216+0200] {subprocess.py:93} INFO - 	at com.datastax.oss.driver.internal.core.util.Reflection.buildFromConfig(Reflection.java:235)
[2025-07-11T11:46:40.216+0200] {subprocess.py:93} INFO - 	at com.datastax.oss.driver.internal.core.util.Reflection.buildFromConfig(Reflection.java:110)
[2025-07-11T11:46:40.216+0200] {subprocess.py:93} INFO - 	at com.datastax.oss.driver.internal.core.context.DefaultDriverContext.buildTimestampGenerator(DefaultDriverContext.java:377)
[2025-07-11T11:46:40.216+0200] {subprocess.py:93} INFO - 	at com.datastax.oss.driver.internal.core.util.concurrent.LazyReference.get(LazyReference.java:55)
[2025-07-11T11:46:40.216+0200] {subprocess.py:93} INFO - 	at com.datastax.oss.driver.internal.core.context.DefaultDriverContext.getTimestampGenerator(DefaultDriverContext.java:773)
[2025-07-11T11:46:40.216+0200] {subprocess.py:93} INFO - 	at com.datastax.oss.driver.internal.core.session.DefaultSession$SingleThreaded.init(DefaultSession.java:349)
[2025-07-11T11:46:40.216+0200] {subprocess.py:93} INFO - 	at com.datastax.oss.driver.internal.core.session.DefaultSession$SingleThreaded.access$1100(DefaultSession.java:300)
[2025-07-11T11:46:40.216+0200] {subprocess.py:93} INFO - 	at com.datastax.oss.driver.internal.core.session.DefaultSession.lambda$init$0(DefaultSession.java:146)
[2025-07-11T11:46:40.216+0200] {subprocess.py:93} INFO - 	at com.datastax.oss.driver.shaded.netty.util.concurrent.PromiseTask.runTask(PromiseTask.java:98)
[2025-07-11T11:46:40.216+0200] {subprocess.py:93} INFO - 	at com.datastax.oss.driver.shaded.netty.util.concurrent.PromiseTask.run(PromiseTask.java:106)
[2025-07-11T11:46:40.216+0200] {subprocess.py:93} INFO - 	at com.datastax.oss.driver.shaded.netty.channel.DefaultEventLoop.run(DefaultEventLoop.java:54)
[2025-07-11T11:46:40.216+0200] {subprocess.py:93} INFO - 	at com.datastax.oss.driver.shaded.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989)
[2025-07-11T11:46:40.216+0200] {subprocess.py:93} INFO - 	at com.datastax.oss.driver.shaded.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
[2025-07-11T11:46:40.216+0200] {subprocess.py:93} INFO - 	at com.datastax.oss.driver.shaded.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
[2025-07-11T11:46:40.216+0200] {subprocess.py:93} INFO - 	at java.base/java.lang.Thread.run(Thread.java:840)
[2025-07-11T11:46:40.216+0200] {subprocess.py:93} INFO - Caused by: java.lang.ClassNotFoundException: jnr.posix.POSIXHandler
[2025-07-11T11:46:40.216+0200] {subprocess.py:93} INFO - 	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:445)
[2025-07-11T11:46:40.217+0200] {subprocess.py:93} INFO - 	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:592)
[2025-07-11T11:46:40.217+0200] {subprocess.py:93} INFO - 	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:525)
[2025-07-11T11:46:40.217+0200] {subprocess.py:93} INFO - 	... 27 more
[2025-07-11T11:46:40.217+0200] {subprocess.py:93} INFO - 25/07/11 11:46:40 INFO Clock: Could not access native clock (see debug logs for details), falling back to Java system clock
[2025-07-11T11:46:40.648+0200] {subprocess.py:93} INFO - 25/07/11 11:46:40 INFO CassandraConnector: Connected to Cassandra cluster.
[2025-07-11T11:46:41.168+0200] {subprocess.py:93} INFO - 25/07/11 11:46:41 INFO FileSystemOverwrite: Path /home/monica.fernandez/practica_creativa/models/arrival_bucketizer_2.0.bin already exists. It will be overwritten.
[2025-07-11T11:46:41.422+0200] {subprocess.py:93} INFO - 25/07/11 11:46:41 INFO deprecation: mapred.output.dir is deprecated. Instead, use mapreduce.output.fileoutputformat.outputdir
[2025-07-11T11:46:41.426+0200] {subprocess.py:93} INFO - 25/07/11 11:46:41 INFO HadoopMapRedCommitProtocol: Using output committer class org.apache.hadoop.mapred.FileOutputCommitter
[2025-07-11T11:46:41.428+0200] {subprocess.py:93} INFO - 25/07/11 11:46:41 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2025-07-11T11:46:41.428+0200] {subprocess.py:93} INFO - 25/07/11 11:46:41 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2025-07-11T11:46:41.486+0200] {subprocess.py:93} INFO - 25/07/11 11:46:41 INFO SparkContext: Starting job: runJob at SparkHadoopWriter.scala:83
[2025-07-11T11:46:41.501+0200] {subprocess.py:93} INFO - 25/07/11 11:46:41 INFO DAGScheduler: Got job 0 (runJob at SparkHadoopWriter.scala:83) with 1 output partitions
[2025-07-11T11:46:41.501+0200] {subprocess.py:93} INFO - 25/07/11 11:46:41 INFO DAGScheduler: Final stage: ResultStage 0 (runJob at SparkHadoopWriter.scala:83)
[2025-07-11T11:46:41.502+0200] {subprocess.py:93} INFO - 25/07/11 11:46:41 INFO DAGScheduler: Parents of final stage: List()
[2025-07-11T11:46:41.503+0200] {subprocess.py:93} INFO - 25/07/11 11:46:41 INFO DAGScheduler: Missing parents: List()
[2025-07-11T11:46:41.515+0200] {subprocess.py:93} INFO - 25/07/11 11:46:41 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[1] at saveAsTextFile at ReadWrite.scala:413), which has no missing parents
[2025-07-11T11:46:41.601+0200] {subprocess.py:93} INFO - 25/07/11 11:46:41 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 101.3 KiB, free 434.3 MiB)
[2025-07-11T11:46:41.640+0200] {subprocess.py:93} INFO - 25/07/11 11:46:41 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 36.5 KiB, free 434.3 MiB)
[2025-07-11T11:46:41.643+0200] {subprocess.py:93} INFO - 25/07/11 11:46:41 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 138.4.31.14:39113 (size: 36.5 KiB, free: 434.4 MiB)
[2025-07-11T11:46:41.646+0200] {subprocess.py:93} INFO - 25/07/11 11:46:41 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1585
[2025-07-11T11:46:41.667+0200] {subprocess.py:93} INFO - 25/07/11 11:46:41 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[1] at saveAsTextFile at ReadWrite.scala:413) (first 15 tasks are for partitions Vector(0))
[2025-07-11T11:46:41.669+0200] {subprocess.py:93} INFO - 25/07/11 11:46:41 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
[2025-07-11T11:46:41.773+0200] {subprocess.py:93} INFO - 25/07/11 11:46:41 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (138.4.31.14, executor driver, partition 0, PROCESS_LOCAL, 13275 bytes)
[2025-07-11T11:46:41.797+0200] {subprocess.py:93} INFO - 25/07/11 11:46:41 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
[2025-07-11T11:46:41.990+0200] {subprocess.py:93} INFO - 25/07/11 11:46:41 INFO HadoopMapRedCommitProtocol: Using output committer class org.apache.hadoop.mapred.FileOutputCommitter
[2025-07-11T11:46:41.991+0200] {subprocess.py:93} INFO - 25/07/11 11:46:41 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2025-07-11T11:46:41.994+0200] {subprocess.py:93} INFO - 25/07/11 11:46:41 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2025-07-11T11:46:42.082+0200] {subprocess.py:93} INFO - 25/07/11 11:46:42 INFO FileOutputCommitter: Saved output of task 'attempt_202507111146416554562399931169927_0001_m_000000_0' to file:/home/monica.fernandez/practica_creativa/models/arrival_bucketizer_2.0.bin/metadata/_temporary/0/task_202507111146416554562399931169927_0001_m_000000
[2025-07-11T11:46:42.083+0200] {subprocess.py:93} INFO - 25/07/11 11:46:42 INFO SparkHadoopMapRedUtil: attempt_202507111146416554562399931169927_0001_m_000000_0: Committed. Elapsed time: 4 ms.
[2025-07-11T11:46:42.113+0200] {subprocess.py:93} INFO - 25/07/11 11:46:42 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 1256 bytes result sent to driver
[2025-07-11T11:46:42.132+0200] {subprocess.py:93} INFO - 25/07/11 11:46:42 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 403 ms on 138.4.31.14 (executor driver) (1/1)
[2025-07-11T11:46:42.138+0200] {subprocess.py:93} INFO - 25/07/11 11:46:42 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool
[2025-07-11T11:46:42.141+0200] {subprocess.py:93} INFO - 25/07/11 11:46:42 INFO DAGScheduler: ResultStage 0 (runJob at SparkHadoopWriter.scala:83) finished in 0,612 s
[2025-07-11T11:46:42.145+0200] {subprocess.py:93} INFO - 25/07/11 11:46:42 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
[2025-07-11T11:46:42.145+0200] {subprocess.py:93} INFO - 25/07/11 11:46:42 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
[2025-07-11T11:46:42.148+0200] {subprocess.py:93} INFO - 25/07/11 11:46:42 INFO DAGScheduler: Job 0 finished: runJob at SparkHadoopWriter.scala:83, took 0,661051 s
[2025-07-11T11:46:42.153+0200] {subprocess.py:93} INFO - 25/07/11 11:46:42 INFO SparkHadoopWriter: Start to commit write Job job_202507111146416554562399931169927_0001.
[2025-07-11T11:46:42.202+0200] {subprocess.py:93} INFO - 25/07/11 11:46:42 INFO SparkHadoopWriter: Write Job job_202507111146416554562399931169927_0001 committed. Elapsed time: 39 ms.
[2025-07-11T11:46:42.790+0200] {subprocess.py:93} INFO - 25/07/11 11:46:42 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 138.4.31.14:39113 in memory (size: 36.5 KiB, free: 434.4 MiB)
[2025-07-11T11:46:42.815+0200] {subprocess.py:93} INFO - 25/07/11 11:46:42 INFO V2ScanRelationPushDown:
[2025-07-11T11:46:42.815+0200] {subprocess.py:93} INFO - Pushing operators to origin_dest_distances
[2025-07-11T11:46:42.815+0200] {subprocess.py:93} INFO - Pushed Filters:
[2025-07-11T11:46:42.815+0200] {subprocess.py:93} INFO - Post-Scan Filters:
[2025-07-11T11:46:42.815+0200] {subprocess.py:93} INFO - 
[2025-07-11T11:46:42.906+0200] {subprocess.py:93} INFO - 25/07/11 11:46:42 INFO V2ScanRelationPushDown:
[2025-07-11T11:46:42.906+0200] {subprocess.py:93} INFO - Output: origin#42, dest#43
[2025-07-11T11:46:42.906+0200] {subprocess.py:93} INFO - 
[2025-07-11T11:46:43.275+0200] {subprocess.py:93} INFO - 25/07/11 11:46:43 INFO FileSourceStrategy: Pushed Filters:
[2025-07-11T11:46:43.276+0200] {subprocess.py:93} INFO - 25/07/11 11:46:43 INFO FileSourceStrategy: Post-Scan Filters:
[2025-07-11T11:46:43.533+0200] {subprocess.py:93} INFO - 25/07/11 11:46:43 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 209.9 KiB, free 434.2 MiB)
[2025-07-11T11:46:43.555+0200] {subprocess.py:93} INFO - 25/07/11 11:46:43 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 37.7 KiB, free 434.2 MiB)
[2025-07-11T11:46:43.555+0200] {subprocess.py:93} INFO - 25/07/11 11:46:43 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 138.4.31.14:39113 (size: 37.7 KiB, free: 434.4 MiB)
[2025-07-11T11:46:43.556+0200] {subprocess.py:93} INFO - 25/07/11 11:46:43 INFO SparkContext: Created broadcast 1 from collect at StringIndexer.scala:204
[2025-07-11T11:46:43.583+0200] {subprocess.py:93} INFO - 25/07/11 11:46:43 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
[2025-07-11T11:46:43.648+0200] {subprocess.py:93} INFO - 25/07/11 11:46:43 INFO DAGScheduler: Registering RDD 4 (collect at StringIndexer.scala:204) as input to shuffle 0
[2025-07-11T11:46:43.654+0200] {subprocess.py:93} INFO - 25/07/11 11:46:43 INFO DAGScheduler: Got map stage job 1 (collect at StringIndexer.scala:204) with 2 output partitions
[2025-07-11T11:46:43.654+0200] {subprocess.py:93} INFO - 25/07/11 11:46:43 INFO DAGScheduler: Final stage: ShuffleMapStage 1 (collect at StringIndexer.scala:204)
[2025-07-11T11:46:43.655+0200] {subprocess.py:93} INFO - 25/07/11 11:46:43 INFO DAGScheduler: Parents of final stage: List()
[2025-07-11T11:46:43.656+0200] {subprocess.py:93} INFO - 25/07/11 11:46:43 INFO DAGScheduler: Missing parents: List()
[2025-07-11T11:46:43.657+0200] {subprocess.py:93} INFO - 25/07/11 11:46:43 INFO DAGScheduler: Submitting ShuffleMapStage 1 (MapPartitionsRDD[4] at collect at StringIndexer.scala:204), which has no missing parents
[2025-07-11T11:46:43.719+0200] {subprocess.py:93} INFO - 25/07/11 11:46:43 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 13.8 KiB, free 434.1 MiB)
[2025-07-11T11:46:43.736+0200] {subprocess.py:93} INFO - 25/07/11 11:46:43 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 7.1 KiB, free 434.1 MiB)
[2025-07-11T11:46:43.737+0200] {subprocess.py:93} INFO - 25/07/11 11:46:43 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 138.4.31.14:39113 (size: 7.1 KiB, free: 434.4 MiB)
[2025-07-11T11:46:43.738+0200] {subprocess.py:93} INFO - 25/07/11 11:46:43 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1585
[2025-07-11T11:46:43.739+0200] {subprocess.py:93} INFO - 25/07/11 11:46:43 INFO DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 1 (MapPartitionsRDD[4] at collect at StringIndexer.scala:204) (first 15 tasks are for partitions Vector(0, 1))
[2025-07-11T11:46:43.740+0200] {subprocess.py:93} INFO - 25/07/11 11:46:43 INFO TaskSchedulerImpl: Adding task set 1.0 with 2 tasks resource profile 0
[2025-07-11T11:46:43.746+0200] {subprocess.py:93} INFO - 25/07/11 11:46:43 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (138.4.31.14, executor driver, partition 0, PROCESS_LOCAL, 13595 bytes)
[2025-07-11T11:46:43.747+0200] {subprocess.py:93} INFO - 25/07/11 11:46:43 INFO TaskSetManager: Starting task 1.0 in stage 1.0 (TID 2) (138.4.31.14, executor driver, partition 1, PROCESS_LOCAL, 13595 bytes)
[2025-07-11T11:46:43.748+0200] {subprocess.py:93} INFO - 25/07/11 11:46:43 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)
[2025-07-11T11:46:43.766+0200] {subprocess.py:93} INFO - 25/07/11 11:46:43 INFO Executor: Running task 1.0 in stage 1.0 (TID 2)
[2025-07-11T11:46:44.040+0200] {subprocess.py:93} INFO - 25/07/11 11:46:44 INFO CodeGenerator: Code generated in 322.519238 ms
[2025-07-11T11:46:44.043+0200] {subprocess.py:93} INFO - 25/07/11 11:46:44 INFO CodeGenerator: Code generated in 163.585047 ms
[2025-07-11T11:46:44.058+0200] {subprocess.py:93} INFO - 25/07/11 11:46:44 INFO DAGScheduler: Registering RDD 8 (collect at StringIndexer.scala:204) as input to shuffle 1
[2025-07-11T11:46:44.058+0200] {subprocess.py:93} INFO - 25/07/11 11:46:44 INFO DAGScheduler: Got map stage job 2 (collect at StringIndexer.scala:204) with 13 output partitions
[2025-07-11T11:46:44.058+0200] {subprocess.py:93} INFO - 25/07/11 11:46:44 INFO DAGScheduler: Final stage: ShuffleMapStage 2 (collect at StringIndexer.scala:204)
[2025-07-11T11:46:44.058+0200] {subprocess.py:93} INFO - 25/07/11 11:46:44 INFO DAGScheduler: Parents of final stage: List()
[2025-07-11T11:46:44.058+0200] {subprocess.py:93} INFO - 25/07/11 11:46:44 INFO DAGScheduler: Missing parents: List()
[2025-07-11T11:46:44.058+0200] {subprocess.py:93} INFO - 25/07/11 11:46:44 INFO DAGScheduler: Submitting ShuffleMapStage 2 (MapPartitionsRDD[8] at collect at StringIndexer.scala:204), which has no missing parents
[2025-07-11T11:46:44.064+0200] {subprocess.py:93} INFO - 25/07/11 11:46:44 INFO FileScanRDD: Reading File path: file:///home/monica.fernandez/practica_creativa/data/simple_flight_delay_features.jsonl.bz2, range: 0-4194304, partition values: [empty row]
[2025-07-11T11:46:44.064+0200] {subprocess.py:93} INFO - 25/07/11 11:46:44 INFO FileScanRDD: Reading File path: file:///home/monica.fernandez/practica_creativa/data/simple_flight_delay_features.jsonl.bz2, range: 4194304-4676447, partition values: [empty row]
[2025-07-11T11:46:44.096+0200] {subprocess.py:93} INFO - 25/07/11 11:46:44 INFO CodeGenerator: Code generated in 28.581871 ms
[2025-07-11T11:46:44.144+0200] {subprocess.py:93} INFO - 25/07/11 11:46:44 INFO CodecPool: Got brand-new decompressor [.bz2]
[2025-07-11T11:46:44.144+0200] {subprocess.py:93} INFO - 25/07/11 11:46:44 INFO CodecPool: Got brand-new decompressor [.bz2]
[2025-07-11T11:46:44.164+0200] {subprocess.py:93} INFO - 25/07/11 11:46:44 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 18.2 KiB, free 434.1 MiB)
[2025-07-11T11:46:44.179+0200] {subprocess.py:93} INFO - 25/07/11 11:46:44 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 9.0 KiB, free 434.1 MiB)
[2025-07-11T11:46:44.183+0200] {subprocess.py:93} INFO - 25/07/11 11:46:44 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 138.4.31.14:39113 (size: 9.0 KiB, free: 434.3 MiB)
[2025-07-11T11:46:44.186+0200] {subprocess.py:93} INFO - 25/07/11 11:46:44 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1585
[2025-07-11T11:46:44.186+0200] {subprocess.py:93} INFO - 25/07/11 11:46:44 INFO DAGScheduler: Submitting 13 missing tasks from ShuffleMapStage 2 (MapPartitionsRDD[8] at collect at StringIndexer.scala:204) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12))
[2025-07-11T11:46:44.186+0200] {subprocess.py:93} INFO - 25/07/11 11:46:44 INFO TaskSchedulerImpl: Adding task set 2.0 with 13 tasks resource profile 0
[2025-07-11T11:46:44.197+0200] {subprocess.py:93} INFO - 25/07/11 11:46:44 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 3) (138.4.31.14, executor driver, partition 0, ANY, 14843 bytes)
[2025-07-11T11:46:44.204+0200] {subprocess.py:93} INFO - 25/07/11 11:46:44 INFO TaskSetManager: Starting task 1.0 in stage 2.0 (TID 4) (138.4.31.14, executor driver, partition 1, ANY, 14843 bytes)
[2025-07-11T11:46:44.211+0200] {subprocess.py:93} INFO - 25/07/11 11:46:44 INFO Executor: Running task 0.0 in stage 2.0 (TID 3)
[2025-07-11T11:46:44.246+0200] {subprocess.py:93} INFO - 25/07/11 11:46:44 INFO Executor: Running task 1.0 in stage 2.0 (TID 4)
[2025-07-11T11:46:44.302+0200] {subprocess.py:93} INFO - 25/07/11 11:46:44 INFO CodeGenerator: Code generated in 24.65054 ms
[2025-07-11T11:46:44.349+0200] {subprocess.py:93} INFO - 25/07/11 11:46:44 INFO CodeGenerator: Code generated in 43.285728 ms
[2025-07-11T11:46:44.717+0200] {subprocess.py:93} INFO - 25/07/11 11:46:44 INFO CodeGenerator: Code generated in 19.055876 ms
[2025-07-11T11:46:44.908+0200] {subprocess.py:93} INFO - 25/07/11 11:46:44 INFO Executor: Finished task 1.0 in stage 2.0 (TID 4). 1988 bytes result sent to driver
[2025-07-11T11:46:44.909+0200] {subprocess.py:93} INFO - 25/07/11 11:46:44 INFO Executor: Finished task 0.0 in stage 2.0 (TID 3). 2031 bytes result sent to driver
[2025-07-11T11:46:44.916+0200] {subprocess.py:93} INFO - 25/07/11 11:46:44 INFO TaskSetManager: Starting task 2.0 in stage 2.0 (TID 5) (138.4.31.14, executor driver, partition 2, ANY, 14963 bytes)
[2025-07-11T11:46:44.921+0200] {subprocess.py:93} INFO - 25/07/11 11:46:44 INFO Executor: Running task 2.0 in stage 2.0 (TID 5)
[2025-07-11T11:46:44.924+0200] {subprocess.py:93} INFO - 25/07/11 11:46:44 INFO TaskSetManager: Starting task 3.0 in stage 2.0 (TID 6) (138.4.31.14, executor driver, partition 3, ANY, 14843 bytes)
[2025-07-11T11:46:44.934+0200] {subprocess.py:93} INFO - 25/07/11 11:46:44 INFO Executor: Running task 3.0 in stage 2.0 (TID 6)
[2025-07-11T11:46:44.936+0200] {subprocess.py:93} INFO - 25/07/11 11:46:44 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 3) in 747 ms on 138.4.31.14 (executor driver) (1/13)
[2025-07-11T11:46:44.943+0200] {subprocess.py:93} INFO - 25/07/11 11:46:44 INFO TaskSetManager: Finished task 1.0 in stage 2.0 (TID 4) in 741 ms on 138.4.31.14 (executor driver) (2/13)
[2025-07-11T11:46:45.181+0200] {subprocess.py:93} INFO - 25/07/11 11:46:45 INFO Executor: Finished task 3.0 in stage 2.0 (TID 6). 2031 bytes result sent to driver
[2025-07-11T11:46:45.186+0200] {subprocess.py:93} INFO - 25/07/11 11:46:45 INFO TaskSetManager: Starting task 4.0 in stage 2.0 (TID 7) (138.4.31.14, executor driver, partition 4, ANY, 14843 bytes)
[2025-07-11T11:46:45.187+0200] {subprocess.py:93} INFO - 25/07/11 11:46:45 INFO Executor: Running task 4.0 in stage 2.0 (TID 7)
[2025-07-11T11:46:45.217+0200] {subprocess.py:93} INFO - 25/07/11 11:46:45 INFO TaskSetManager: Finished task 3.0 in stage 2.0 (TID 6) in 293 ms on 138.4.31.14 (executor driver) (3/13)
[2025-07-11T11:46:45.247+0200] {subprocess.py:93} INFO - 25/07/11 11:46:45 INFO Executor: Finished task 2.0 in stage 2.0 (TID 5). 1988 bytes result sent to driver
[2025-07-11T11:46:45.247+0200] {subprocess.py:93} INFO - 25/07/11 11:46:45 INFO TaskSetManager: Starting task 5.0 in stage 2.0 (TID 8) (138.4.31.14, executor driver, partition 5, ANY, 14843 bytes)
[2025-07-11T11:46:45.247+0200] {subprocess.py:93} INFO - 25/07/11 11:46:45 INFO TaskSetManager: Finished task 2.0 in stage 2.0 (TID 5) in 328 ms on 138.4.31.14 (executor driver) (4/13)
[2025-07-11T11:46:45.254+0200] {subprocess.py:93} INFO - 25/07/11 11:46:45 INFO Executor: Running task 5.0 in stage 2.0 (TID 8)
[2025-07-11T11:46:45.298+0200] {subprocess.py:93} INFO - 25/07/11 11:46:45 INFO Executor: Finished task 4.0 in stage 2.0 (TID 7). 1945 bytes result sent to driver
[2025-07-11T11:46:45.299+0200] {subprocess.py:93} INFO - 25/07/11 11:46:45 INFO TaskSetManager: Starting task 6.0 in stage 2.0 (TID 9) (138.4.31.14, executor driver, partition 6, ANY, 14843 bytes)
[2025-07-11T11:46:45.302+0200] {subprocess.py:93} INFO - 25/07/11 11:46:45 INFO TaskSetManager: Finished task 4.0 in stage 2.0 (TID 7) in 117 ms on 138.4.31.14 (executor driver) (5/13)
[2025-07-11T11:46:45.305+0200] {subprocess.py:93} INFO - 25/07/11 11:46:45 INFO Executor: Running task 6.0 in stage 2.0 (TID 9)
[2025-07-11T11:46:45.477+0200] {subprocess.py:93} INFO - 25/07/11 11:46:45 INFO Executor: Finished task 5.0 in stage 2.0 (TID 8). 1988 bytes result sent to driver
[2025-07-11T11:46:45.478+0200] {subprocess.py:93} INFO - 25/07/11 11:46:45 INFO TaskSetManager: Starting task 7.0 in stage 2.0 (TID 10) (138.4.31.14, executor driver, partition 7, ANY, 14963 bytes)
[2025-07-11T11:46:45.480+0200] {subprocess.py:93} INFO - 25/07/11 11:46:45 INFO TaskSetManager: Finished task 5.0 in stage 2.0 (TID 8) in 237 ms on 138.4.31.14 (executor driver) (6/13)
[2025-07-11T11:46:45.487+0200] {subprocess.py:93} INFO - 25/07/11 11:46:45 INFO Executor: Running task 7.0 in stage 2.0 (TID 10)
[2025-07-11T11:46:45.489+0200] {subprocess.py:93} INFO - 25/07/11 11:46:45 INFO Executor: Finished task 6.0 in stage 2.0 (TID 9). 1988 bytes result sent to driver
[2025-07-11T11:46:45.493+0200] {subprocess.py:93} INFO - 25/07/11 11:46:45 INFO TaskSetManager: Starting task 8.0 in stage 2.0 (TID 11) (138.4.31.14, executor driver, partition 8, ANY, 14959 bytes)
[2025-07-11T11:46:45.494+0200] {subprocess.py:93} INFO - 25/07/11 11:46:45 INFO Executor: Running task 8.0 in stage 2.0 (TID 11)
[2025-07-11T11:46:45.494+0200] {subprocess.py:93} INFO - 25/07/11 11:46:45 INFO TaskSetManager: Finished task 6.0 in stage 2.0 (TID 9) in 196 ms on 138.4.31.14 (executor driver) (7/13)
[2025-07-11T11:46:45.612+0200] {subprocess.py:93} INFO - 25/07/11 11:46:45 INFO Executor: Finished task 7.0 in stage 2.0 (TID 10). 1988 bytes result sent to driver
[2025-07-11T11:46:45.616+0200] {subprocess.py:93} INFO - 25/07/11 11:46:45 INFO TaskSetManager: Starting task 9.0 in stage 2.0 (TID 12) (138.4.31.14, executor driver, partition 9, ANY, 14843 bytes)
[2025-07-11T11:46:45.616+0200] {subprocess.py:93} INFO - 25/07/11 11:46:45 INFO TaskSetManager: Finished task 7.0 in stage 2.0 (TID 10) in 139 ms on 138.4.31.14 (executor driver) (8/13)
[2025-07-11T11:46:45.617+0200] {subprocess.py:93} INFO - 25/07/11 11:46:45 INFO Executor: Running task 9.0 in stage 2.0 (TID 12)
[2025-07-11T11:46:45.692+0200] {subprocess.py:93} INFO - 25/07/11 11:46:45 INFO Executor: Finished task 8.0 in stage 2.0 (TID 11). 1988 bytes result sent to driver
[2025-07-11T11:46:45.693+0200] {subprocess.py:93} INFO - 25/07/11 11:46:45 INFO TaskSetManager: Starting task 10.0 in stage 2.0 (TID 13) (138.4.31.14, executor driver, partition 10, ANY, 14843 bytes)
[2025-07-11T11:46:45.694+0200] {subprocess.py:93} INFO - 25/07/11 11:46:45 INFO TaskSetManager: Finished task 8.0 in stage 2.0 (TID 11) in 202 ms on 138.4.31.14 (executor driver) (9/13)
[2025-07-11T11:46:45.696+0200] {subprocess.py:93} INFO - 25/07/11 11:46:45 INFO Executor: Running task 10.0 in stage 2.0 (TID 13)
[2025-07-11T11:46:45.723+0200] {subprocess.py:93} INFO - 25/07/11 11:46:45 INFO Executor: Finished task 9.0 in stage 2.0 (TID 12). 1988 bytes result sent to driver
[2025-07-11T11:46:45.724+0200] {subprocess.py:93} INFO - 25/07/11 11:46:45 INFO TaskSetManager: Starting task 11.0 in stage 2.0 (TID 14) (138.4.31.14, executor driver, partition 11, ANY, 14843 bytes)
[2025-07-11T11:46:45.725+0200] {subprocess.py:93} INFO - 25/07/11 11:46:45 INFO Executor: Running task 11.0 in stage 2.0 (TID 14)
[2025-07-11T11:46:45.725+0200] {subprocess.py:93} INFO - 25/07/11 11:46:45 INFO TaskSetManager: Finished task 9.0 in stage 2.0 (TID 12) in 110 ms on 138.4.31.14 (executor driver) (10/13)
[2025-07-11T11:46:45.795+0200] {subprocess.py:93} INFO - 25/07/11 11:46:45 INFO Executor: Finished task 10.0 in stage 2.0 (TID 13). 2031 bytes result sent to driver
[2025-07-11T11:46:45.799+0200] {subprocess.py:93} INFO - 25/07/11 11:46:45 INFO TaskSetManager: Starting task 12.0 in stage 2.0 (TID 15) (138.4.31.14, executor driver, partition 12, ANY, 14843 bytes)
[2025-07-11T11:46:45.801+0200] {subprocess.py:93} INFO - 25/07/11 11:46:45 INFO TaskSetManager: Finished task 10.0 in stage 2.0 (TID 13) in 108 ms on 138.4.31.14 (executor driver) (11/13)
[2025-07-11T11:46:45.802+0200] {subprocess.py:93} INFO - 25/07/11 11:46:45 INFO Executor: Running task 12.0 in stage 2.0 (TID 15)
[2025-07-11T11:46:45.804+0200] {subprocess.py:93} INFO - 25/07/11 11:46:45 INFO Executor: Finished task 11.0 in stage 2.0 (TID 14). 1988 bytes result sent to driver
[2025-07-11T11:46:45.807+0200] {subprocess.py:93} INFO - 25/07/11 11:46:45 INFO TaskSetManager: Finished task 11.0 in stage 2.0 (TID 14) in 83 ms on 138.4.31.14 (executor driver) (12/13)
[2025-07-11T11:46:45.945+0200] {subprocess.py:93} INFO - 25/07/11 11:46:45 INFO Executor: Finished task 12.0 in stage 2.0 (TID 15). 1988 bytes result sent to driver
[2025-07-11T11:46:45.947+0200] {subprocess.py:93} INFO - 25/07/11 11:46:45 INFO TaskSetManager: Finished task 12.0 in stage 2.0 (TID 15) in 152 ms on 138.4.31.14 (executor driver) (13/13)
[2025-07-11T11:46:45.950+0200] {subprocess.py:93} INFO - 25/07/11 11:46:45 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool
[2025-07-11T11:46:45.950+0200] {subprocess.py:93} INFO - 25/07/11 11:46:45 INFO DAGScheduler: ShuffleMapStage 2 (collect at StringIndexer.scala:204) finished in 1,858 s
[2025-07-11T11:46:45.950+0200] {subprocess.py:93} INFO - 25/07/11 11:46:45 INFO DAGScheduler: looking for newly runnable stages
[2025-07-11T11:46:45.950+0200] {subprocess.py:93} INFO - 25/07/11 11:46:45 INFO DAGScheduler: running: Set(ShuffleMapStage 1)
[2025-07-11T11:46:45.950+0200] {subprocess.py:93} INFO - 25/07/11 11:46:45 INFO DAGScheduler: waiting: Set()
[2025-07-11T11:46:45.954+0200] {subprocess.py:93} INFO - 25/07/11 11:46:45 INFO DAGScheduler: failed: Set()
[2025-07-11T11:46:46.061+0200] {subprocess.py:93} INFO - 25/07/11 11:46:46 INFO ShufflePartitionsUtil: For shuffle(1), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576
[2025-07-11T11:46:46.165+0200] {subprocess.py:93} INFO - 25/07/11 11:46:46 INFO SparkContext: Starting job: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264
[2025-07-11T11:46:46.168+0200] {subprocess.py:93} INFO - 25/07/11 11:46:46 INFO DAGScheduler: Got job 3 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) with 1 output partitions
[2025-07-11T11:46:46.168+0200] {subprocess.py:93} INFO - 25/07/11 11:46:46 INFO DAGScheduler: Final stage: ResultStage 4 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264)
[2025-07-11T11:46:46.171+0200] {subprocess.py:93} INFO - 25/07/11 11:46:46 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 3)
[2025-07-11T11:46:46.171+0200] {subprocess.py:93} INFO - 25/07/11 11:46:46 INFO DAGScheduler: Missing parents: List()
[2025-07-11T11:46:46.171+0200] {subprocess.py:93} INFO - 25/07/11 11:46:46 INFO DAGScheduler: Submitting ResultStage 4 (MapPartitionsRDD[10] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264), which has no missing parents
[2025-07-11T11:46:46.177+0200] {subprocess.py:93} INFO - 25/07/11 11:46:46 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 8.2 KiB, free 434.1 MiB)
[2025-07-11T11:46:46.182+0200] {subprocess.py:93} INFO - 25/07/11 11:46:46 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 4.2 KiB, free 434.1 MiB)
[2025-07-11T11:46:46.182+0200] {subprocess.py:93} INFO - 25/07/11 11:46:46 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 138.4.31.14:39113 (size: 4.2 KiB, free: 434.3 MiB)
[2025-07-11T11:46:46.184+0200] {subprocess.py:93} INFO - 25/07/11 11:46:46 INFO SparkContext: Created broadcast 4 from broadcast at DAGScheduler.scala:1585
[2025-07-11T11:46:46.188+0200] {subprocess.py:93} INFO - 25/07/11 11:46:46 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 4 (MapPartitionsRDD[10] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) (first 15 tasks are for partitions Vector(0))
[2025-07-11T11:46:46.188+0200] {subprocess.py:93} INFO - 25/07/11 11:46:46 INFO TaskSchedulerImpl: Adding task set 4.0 with 1 tasks resource profile 0
[2025-07-11T11:46:46.192+0200] {subprocess.py:93} INFO - 25/07/11 11:46:46 INFO TaskSetManager: Starting task 0.0 in stage 4.0 (TID 16) (138.4.31.14, executor driver, partition 0, NODE_LOCAL, 12968 bytes)
[2025-07-11T11:46:46.193+0200] {subprocess.py:93} INFO - 25/07/11 11:46:46 INFO Executor: Running task 0.0 in stage 4.0 (TID 16)
[2025-07-11T11:46:46.289+0200] {subprocess.py:93} INFO - 25/07/11 11:46:46 INFO ShuffleBlockFetcherIterator: Getting 13 (196.8 KiB) non-empty blocks including 13 (196.8 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2025-07-11T11:46:46.302+0200] {subprocess.py:93} INFO - 25/07/11 11:46:46 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 36 ms
[2025-07-11T11:46:46.391+0200] {subprocess.py:93} INFO - 25/07/11 11:46:46 INFO Executor: Finished task 0.0 in stage 4.0 (TID 16). 40614 bytes result sent to driver
[2025-07-11T11:46:46.394+0200] {subprocess.py:93} INFO - 25/07/11 11:46:46 INFO TaskSetManager: Finished task 0.0 in stage 4.0 (TID 16) in 204 ms on 138.4.31.14 (executor driver) (1/1)
[2025-07-11T11:46:46.394+0200] {subprocess.py:93} INFO - 25/07/11 11:46:46 INFO TaskSchedulerImpl: Removed TaskSet 4.0, whose tasks have all completed, from pool
[2025-07-11T11:46:46.397+0200] {subprocess.py:93} INFO - 25/07/11 11:46:46 INFO DAGScheduler: ResultStage 4 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) finished in 0,222 s
[2025-07-11T11:46:46.399+0200] {subprocess.py:93} INFO - 25/07/11 11:46:46 INFO DAGScheduler: Job 3 is finished. Cancelling potential speculative or zombie tasks for this job
[2025-07-11T11:46:46.400+0200] {subprocess.py:93} INFO - 25/07/11 11:46:46 INFO TaskSchedulerImpl: Killing all running tasks in stage 4: Stage finished
[2025-07-11T11:46:46.400+0200] {subprocess.py:93} INFO - 25/07/11 11:46:46 INFO DAGScheduler: Job 3 finished: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264, took 0,234287 s
[2025-07-11T11:46:46.463+0200] {subprocess.py:93} INFO - 25/07/11 11:46:46 INFO CodeGenerator: Code generated in 37.541734 ms
[2025-07-11T11:46:46.503+0200] {subprocess.py:93} INFO - 25/07/11 11:46:46 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 4.3 MiB, free 429.8 MiB)
[2025-07-11T11:46:46.521+0200] {subprocess.py:93} INFO - 25/07/11 11:46:46 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 89.6 KiB, free 429.8 MiB)
[2025-07-11T11:46:46.522+0200] {subprocess.py:93} INFO - 25/07/11 11:46:46 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 138.4.31.14:39113 (size: 89.6 KiB, free: 434.3 MiB)
[2025-07-11T11:46:46.523+0200] {subprocess.py:93} INFO - 25/07/11 11:46:46 INFO SparkContext: Created broadcast 5 from $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264
[2025-07-11T11:46:46.839+0200] {subprocess.py:93} INFO - 25/07/11 11:46:46 INFO Executor: Finished task 1.0 in stage 1.0 (TID 2). 2002 bytes result sent to driver
[2025-07-11T11:46:46.842+0200] {subprocess.py:93} INFO - 25/07/11 11:46:46 INFO TaskSetManager: Finished task 1.0 in stage 1.0 (TID 2) in 3094 ms on 138.4.31.14 (executor driver) (1/2)
[2025-07-11T11:46:47.058+0200] {subprocess.py:93} INFO - 25/07/11 11:46:47 INFO BlockManagerInfo: Removed broadcast_4_piece0 on 138.4.31.14:39113 in memory (size: 4.2 KiB, free: 434.3 MiB)
[2025-07-11T11:46:53.045+0200] {subprocess.py:93} INFO - 25/07/11 11:46:53 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 1959 bytes result sent to driver
[2025-07-11T11:46:53.046+0200] {subprocess.py:93} INFO - 25/07/11 11:46:53 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 9305 ms on 138.4.31.14 (executor driver) (2/2)
[2025-07-11T11:46:53.047+0200] {subprocess.py:93} INFO - 25/07/11 11:46:53 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool
[2025-07-11T11:46:53.049+0200] {subprocess.py:93} INFO - 25/07/11 11:46:53 INFO DAGScheduler: ShuffleMapStage 1 (collect at StringIndexer.scala:204) finished in 9,378 s
[2025-07-11T11:46:53.049+0200] {subprocess.py:93} INFO - 25/07/11 11:46:53 INFO DAGScheduler: looking for newly runnable stages
[2025-07-11T11:46:53.049+0200] {subprocess.py:93} INFO - 25/07/11 11:46:53 INFO DAGScheduler: running: Set()
[2025-07-11T11:46:53.049+0200] {subprocess.py:93} INFO - 25/07/11 11:46:53 INFO DAGScheduler: waiting: Set()
[2025-07-11T11:46:53.050+0200] {subprocess.py:93} INFO - 25/07/11 11:46:53 INFO DAGScheduler: failed: Set()
[2025-07-11T11:46:53.058+0200] {subprocess.py:93} INFO - 25/07/11 11:46:53 INFO ShufflePartitionsUtil: For shuffle(0), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576
[2025-07-11T11:46:53.089+0200] {subprocess.py:93} INFO - 25/07/11 11:46:53 INFO CodeGenerator: Code generated in 11.359638 ms
[2025-07-11T11:46:53.098+0200] {subprocess.py:93} INFO - 25/07/11 11:46:53 INFO DAGScheduler: Registering RDD 14 (collect at StringIndexer.scala:204) as input to shuffle 2
[2025-07-11T11:46:53.099+0200] {subprocess.py:93} INFO - 25/07/11 11:46:53 INFO DAGScheduler: Got map stage job 4 (collect at StringIndexer.scala:204) with 1 output partitions
[2025-07-11T11:46:53.099+0200] {subprocess.py:93} INFO - 25/07/11 11:46:53 INFO DAGScheduler: Final stage: ShuffleMapStage 6 (collect at StringIndexer.scala:204)
[2025-07-11T11:46:53.099+0200] {subprocess.py:93} INFO - 25/07/11 11:46:53 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 5)
[2025-07-11T11:46:53.099+0200] {subprocess.py:93} INFO - 25/07/11 11:46:53 INFO DAGScheduler: Missing parents: List()
[2025-07-11T11:46:53.099+0200] {subprocess.py:93} INFO - 25/07/11 11:46:53 INFO DAGScheduler: Submitting ShuffleMapStage 6 (MapPartitionsRDD[14] at collect at StringIndexer.scala:204), which has no missing parents
[2025-07-11T11:46:53.143+0200] {subprocess.py:93} INFO - 25/07/11 11:46:53 INFO MemoryStore: Block broadcast_6 stored as values in memory (estimated size 49.5 KiB, free 429.7 MiB)
[2025-07-11T11:46:53.145+0200] {subprocess.py:93} INFO - 25/07/11 11:46:53 INFO MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 23.5 KiB, free 429.7 MiB)
[2025-07-11T11:46:53.147+0200] {subprocess.py:93} INFO - 25/07/11 11:46:53 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on 138.4.31.14:39113 (size: 23.5 KiB, free: 434.2 MiB)
[2025-07-11T11:46:53.149+0200] {subprocess.py:93} INFO - 25/07/11 11:46:53 INFO SparkContext: Created broadcast 6 from broadcast at DAGScheduler.scala:1585
[2025-07-11T11:46:53.150+0200] {subprocess.py:93} INFO - 25/07/11 11:46:53 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 6 (MapPartitionsRDD[14] at collect at StringIndexer.scala:204) (first 15 tasks are for partitions Vector(0))
[2025-07-11T11:46:53.151+0200] {subprocess.py:93} INFO - 25/07/11 11:46:53 INFO TaskSchedulerImpl: Adding task set 6.0 with 1 tasks resource profile 0
[2025-07-11T11:46:53.152+0200] {subprocess.py:93} INFO - 25/07/11 11:46:53 INFO TaskSetManager: Starting task 0.0 in stage 6.0 (TID 17) (138.4.31.14, executor driver, partition 0, NODE_LOCAL, 12957 bytes)
[2025-07-11T11:46:53.153+0200] {subprocess.py:93} INFO - 25/07/11 11:46:53 INFO Executor: Running task 0.0 in stage 6.0 (TID 17)
[2025-07-11T11:46:53.175+0200] {subprocess.py:93} INFO - 25/07/11 11:46:53 INFO ShuffleBlockFetcherIterator: Getting 2 (1126.9 KiB) non-empty blocks including 2 (1126.9 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2025-07-11T11:46:53.175+0200] {subprocess.py:93} INFO - 25/07/11 11:46:53 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2025-07-11T11:46:53.187+0200] {subprocess.py:93} INFO - 25/07/11 11:46:53 INFO CodeGenerator: Code generated in 11.712968 ms
[2025-07-11T11:46:53.213+0200] {subprocess.py:93} INFO - 25/07/11 11:46:53 INFO CodeGenerator: Code generated in 5.196944 ms
[2025-07-11T11:46:53.222+0200] {subprocess.py:93} INFO - 25/07/11 11:46:53 INFO CodeGenerator: Code generated in 5.282677 ms
[2025-07-11T11:46:53.229+0200] {subprocess.py:93} INFO - 25/07/11 11:46:53 INFO CodeGenerator: Code generated in 5.597206 ms
[2025-07-11T11:46:53.257+0200] {subprocess.py:93} INFO - 25/07/11 11:46:53 INFO CodeGenerator: Code generated in 6.127817 ms
[2025-07-11T11:46:53.680+0200] {subprocess.py:93} INFO - 25/07/11 11:46:53 INFO CodeGenerator: Code generated in 24.739908 ms
[2025-07-11T11:46:53.907+0200] {subprocess.py:93} INFO - 25/07/11 11:46:53 INFO Executor: Finished task 0.0 in stage 6.0 (TID 17). 6487 bytes result sent to driver
[2025-07-11T11:46:53.908+0200] {subprocess.py:93} INFO - 25/07/11 11:46:53 INFO TaskSetManager: Finished task 0.0 in stage 6.0 (TID 17) in 756 ms on 138.4.31.14 (executor driver) (1/1)
[2025-07-11T11:46:53.908+0200] {subprocess.py:93} INFO - 25/07/11 11:46:53 INFO TaskSchedulerImpl: Removed TaskSet 6.0, whose tasks have all completed, from pool
[2025-07-11T11:46:53.909+0200] {subprocess.py:93} INFO - 25/07/11 11:46:53 INFO DAGScheduler: ShuffleMapStage 6 (collect at StringIndexer.scala:204) finished in 0,807 s
[2025-07-11T11:46:53.909+0200] {subprocess.py:93} INFO - 25/07/11 11:46:53 INFO DAGScheduler: looking for newly runnable stages
[2025-07-11T11:46:53.909+0200] {subprocess.py:93} INFO - 25/07/11 11:46:53 INFO DAGScheduler: running: Set()
[2025-07-11T11:46:53.910+0200] {subprocess.py:93} INFO - 25/07/11 11:46:53 INFO DAGScheduler: waiting: Set()
[2025-07-11T11:46:53.910+0200] {subprocess.py:93} INFO - 25/07/11 11:46:53 INFO DAGScheduler: failed: Set()
[2025-07-11T11:46:53.937+0200] {subprocess.py:93} INFO - 25/07/11 11:46:53 INFO SparkContext: Starting job: collect at StringIndexer.scala:204
[2025-07-11T11:46:53.938+0200] {subprocess.py:93} INFO - 25/07/11 11:46:53 INFO DAGScheduler: Got job 5 (collect at StringIndexer.scala:204) with 1 output partitions
[2025-07-11T11:46:53.938+0200] {subprocess.py:93} INFO - 25/07/11 11:46:53 INFO DAGScheduler: Final stage: ResultStage 9 (collect at StringIndexer.scala:204)
[2025-07-11T11:46:53.938+0200] {subprocess.py:93} INFO - 25/07/11 11:46:53 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 8)
[2025-07-11T11:46:53.938+0200] {subprocess.py:93} INFO - 25/07/11 11:46:53 INFO DAGScheduler: Missing parents: List()
[2025-07-11T11:46:53.939+0200] {subprocess.py:93} INFO - 25/07/11 11:46:53 INFO DAGScheduler: Submitting ResultStage 9 (MapPartitionsRDD[17] at collect at StringIndexer.scala:204), which has no missing parents
[2025-07-11T11:46:53.946+0200] {subprocess.py:93} INFO - 25/07/11 11:46:53 INFO MemoryStore: Block broadcast_7 stored as values in memory (estimated size 48.9 KiB, free 429.7 MiB)
[2025-07-11T11:46:53.947+0200] {subprocess.py:93} INFO - 25/07/11 11:46:53 INFO MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 23.4 KiB, free 429.6 MiB)
[2025-07-11T11:46:53.948+0200] {subprocess.py:93} INFO - 25/07/11 11:46:53 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on 138.4.31.14:39113 (size: 23.4 KiB, free: 434.2 MiB)
[2025-07-11T11:46:53.948+0200] {subprocess.py:93} INFO - 25/07/11 11:46:53 INFO SparkContext: Created broadcast 7 from broadcast at DAGScheduler.scala:1585
[2025-07-11T11:46:53.949+0200] {subprocess.py:93} INFO - 25/07/11 11:46:53 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 9 (MapPartitionsRDD[17] at collect at StringIndexer.scala:204) (first 15 tasks are for partitions Vector(0))
[2025-07-11T11:46:53.949+0200] {subprocess.py:93} INFO - 25/07/11 11:46:53 INFO TaskSchedulerImpl: Adding task set 9.0 with 1 tasks resource profile 0
[2025-07-11T11:46:53.950+0200] {subprocess.py:93} INFO - 25/07/11 11:46:53 INFO TaskSetManager: Starting task 0.0 in stage 9.0 (TID 18) (138.4.31.14, executor driver, partition 0, NODE_LOCAL, 12952 bytes)
[2025-07-11T11:46:53.951+0200] {subprocess.py:93} INFO - 25/07/11 11:46:53 INFO Executor: Running task 0.0 in stage 9.0 (TID 18)
[2025-07-11T11:46:53.959+0200] {subprocess.py:93} INFO - 25/07/11 11:46:53 INFO ShuffleBlockFetcherIterator: Getting 1 (445.0 B) non-empty blocks including 1 (445.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2025-07-11T11:46:53.959+0200] {subprocess.py:93} INFO - 25/07/11 11:46:53 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2025-07-11T11:46:53.978+0200] {subprocess.py:93} INFO - 25/07/11 11:46:53 INFO CodeGenerator: Code generated in 9.089585 ms
[2025-07-11T11:46:54.022+0200] {subprocess.py:93} INFO - 25/07/11 11:46:54 INFO Executor: Finished task 0.0 in stage 9.0 (TID 18). 8089 bytes result sent to driver
[2025-07-11T11:46:54.023+0200] {subprocess.py:93} INFO - 25/07/11 11:46:54 INFO TaskSetManager: Finished task 0.0 in stage 9.0 (TID 18) in 72 ms on 138.4.31.14 (executor driver) (1/1)
[2025-07-11T11:46:54.023+0200] {subprocess.py:93} INFO - 25/07/11 11:46:54 INFO TaskSchedulerImpl: Removed TaskSet 9.0, whose tasks have all completed, from pool
[2025-07-11T11:46:54.023+0200] {subprocess.py:93} INFO - 25/07/11 11:46:54 INFO DAGScheduler: ResultStage 9 (collect at StringIndexer.scala:204) finished in 0,080 s
[2025-07-11T11:46:54.024+0200] {subprocess.py:93} INFO - 25/07/11 11:46:54 INFO DAGScheduler: Job 5 is finished. Cancelling potential speculative or zombie tasks for this job
[2025-07-11T11:46:54.024+0200] {subprocess.py:93} INFO - 25/07/11 11:46:54 INFO TaskSchedulerImpl: Killing all running tasks in stage 9: Stage finished
[2025-07-11T11:46:54.025+0200] {subprocess.py:93} INFO - 25/07/11 11:46:54 INFO DAGScheduler: Job 5 finished: collect at StringIndexer.scala:204, took 0,087494 s
[2025-07-11T11:46:54.037+0200] {subprocess.py:93} INFO - 25/07/11 11:46:54 INFO CodeGenerator: Code generated in 8.023936 ms
[2025-07-11T11:46:54.159+0200] {subprocess.py:93} INFO - 25/07/11 11:46:54 INFO FileSystemOverwrite: Path /home/monica.fernandez/practica_creativa/models/string_indexer_model_Carrier.bin already exists. It will be overwritten.
[2025-07-11T11:46:54.176+0200] {subprocess.py:93} INFO - 25/07/11 11:46:54 INFO HadoopMapRedCommitProtocol: Using output committer class org.apache.hadoop.mapred.FileOutputCommitter
[2025-07-11T11:46:54.176+0200] {subprocess.py:93} INFO - 25/07/11 11:46:54 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2025-07-11T11:46:54.176+0200] {subprocess.py:93} INFO - 25/07/11 11:46:54 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2025-07-11T11:46:54.212+0200] {subprocess.py:93} INFO - 25/07/11 11:46:54 INFO SparkContext: Starting job: runJob at SparkHadoopWriter.scala:83
[2025-07-11T11:46:54.213+0200] {subprocess.py:93} INFO - 25/07/11 11:46:54 INFO DAGScheduler: Got job 6 (runJob at SparkHadoopWriter.scala:83) with 1 output partitions
[2025-07-11T11:46:54.213+0200] {subprocess.py:93} INFO - 25/07/11 11:46:54 INFO DAGScheduler: Final stage: ResultStage 10 (runJob at SparkHadoopWriter.scala:83)
[2025-07-11T11:46:54.213+0200] {subprocess.py:93} INFO - 25/07/11 11:46:54 INFO DAGScheduler: Parents of final stage: List()
[2025-07-11T11:46:54.213+0200] {subprocess.py:93} INFO - 25/07/11 11:46:54 INFO DAGScheduler: Missing parents: List()
[2025-07-11T11:46:54.215+0200] {subprocess.py:93} INFO - 25/07/11 11:46:54 INFO DAGScheduler: Submitting ResultStage 10 (MapPartitionsRDD[19] at saveAsTextFile at ReadWrite.scala:413), which has no missing parents
[2025-07-11T11:46:54.222+0200] {subprocess.py:93} INFO - 25/07/11 11:46:54 INFO MemoryStore: Block broadcast_8 stored as values in memory (estimated size 101.3 KiB, free 429.5 MiB)
[2025-07-11T11:46:54.223+0200] {subprocess.py:93} INFO - 25/07/11 11:46:54 INFO MemoryStore: Block broadcast_8_piece0 stored as bytes in memory (estimated size 36.5 KiB, free 429.5 MiB)
[2025-07-11T11:46:54.224+0200] {subprocess.py:93} INFO - 25/07/11 11:46:54 INFO BlockManagerInfo: Added broadcast_8_piece0 in memory on 138.4.31.14:39113 (size: 36.5 KiB, free: 434.2 MiB)
[2025-07-11T11:46:54.224+0200] {subprocess.py:93} INFO - 25/07/11 11:46:54 INFO SparkContext: Created broadcast 8 from broadcast at DAGScheduler.scala:1585
[2025-07-11T11:46:54.225+0200] {subprocess.py:93} INFO - 25/07/11 11:46:54 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 10 (MapPartitionsRDD[19] at saveAsTextFile at ReadWrite.scala:413) (first 15 tasks are for partitions Vector(0))
[2025-07-11T11:46:54.225+0200] {subprocess.py:93} INFO - 25/07/11 11:46:54 INFO TaskSchedulerImpl: Adding task set 10.0 with 1 tasks resource profile 0
[2025-07-11T11:46:54.227+0200] {subprocess.py:93} INFO - 25/07/11 11:46:54 INFO TaskSetManager: Starting task 0.0 in stage 10.0 (TID 19) (138.4.31.14, executor driver, partition 0, PROCESS_LOCAL, 13282 bytes)
[2025-07-11T11:46:54.227+0200] {subprocess.py:93} INFO - 25/07/11 11:46:54 INFO Executor: Running task 0.0 in stage 10.0 (TID 19)
[2025-07-11T11:46:54.238+0200] {subprocess.py:93} INFO - 25/07/11 11:46:54 INFO HadoopMapRedCommitProtocol: Using output committer class org.apache.hadoop.mapred.FileOutputCommitter
[2025-07-11T11:46:54.238+0200] {subprocess.py:93} INFO - 25/07/11 11:46:54 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2025-07-11T11:46:54.238+0200] {subprocess.py:93} INFO - 25/07/11 11:46:54 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2025-07-11T11:46:54.271+0200] {subprocess.py:93} INFO - 25/07/11 11:46:54 INFO FileOutputCommitter: Saved output of task 'attempt_202507111146543739615161263252238_0019_m_000000_0' to file:/home/monica.fernandez/practica_creativa/models/string_indexer_model_Carrier.bin/metadata/_temporary/0/task_202507111146543739615161263252238_0019_m_000000
[2025-07-11T11:46:54.272+0200] {subprocess.py:93} INFO - 25/07/11 11:46:54 INFO SparkHadoopMapRedUtil: attempt_202507111146543739615161263252238_0019_m_000000_0: Committed. Elapsed time: 2 ms.
[2025-07-11T11:46:54.274+0200] {subprocess.py:93} INFO - 25/07/11 11:46:54 INFO Executor: Finished task 0.0 in stage 10.0 (TID 19). 1170 bytes result sent to driver
[2025-07-11T11:46:54.274+0200] {subprocess.py:93} INFO - 25/07/11 11:46:54 INFO TaskSetManager: Finished task 0.0 in stage 10.0 (TID 19) in 49 ms on 138.4.31.14 (executor driver) (1/1)
[2025-07-11T11:46:54.275+0200] {subprocess.py:93} INFO - 25/07/11 11:46:54 INFO TaskSchedulerImpl: Removed TaskSet 10.0, whose tasks have all completed, from pool
[2025-07-11T11:46:54.275+0200] {subprocess.py:93} INFO - 25/07/11 11:46:54 INFO DAGScheduler: ResultStage 10 (runJob at SparkHadoopWriter.scala:83) finished in 0,061 s
[2025-07-11T11:46:54.276+0200] {subprocess.py:93} INFO - 25/07/11 11:46:54 INFO DAGScheduler: Job 6 is finished. Cancelling potential speculative or zombie tasks for this job
[2025-07-11T11:46:54.276+0200] {subprocess.py:93} INFO - 25/07/11 11:46:54 INFO TaskSchedulerImpl: Killing all running tasks in stage 10: Stage finished
[2025-07-11T11:46:54.277+0200] {subprocess.py:93} INFO - 25/07/11 11:46:54 INFO DAGScheduler: Job 6 finished: runJob at SparkHadoopWriter.scala:83, took 0,064069 s
[2025-07-11T11:46:54.277+0200] {subprocess.py:93} INFO - 25/07/11 11:46:54 INFO SparkHadoopWriter: Start to commit write Job job_202507111146543739615161263252238_0019.
[2025-07-11T11:46:54.315+0200] {subprocess.py:93} INFO - 25/07/11 11:46:54 INFO SparkHadoopWriter: Write Job job_202507111146543739615161263252238_0019 committed. Elapsed time: 38 ms.
[2025-07-11T11:46:54.442+0200] {subprocess.py:93} INFO - 25/07/11 11:46:54 INFO CodeGenerator: Code generated in 9.140787 ms
[2025-07-11T11:46:54.450+0200] {subprocess.py:93} INFO - 25/07/11 11:46:54 INFO DAGScheduler: Registering RDD 22 (parquet at StringIndexer.scala:499) as input to shuffle 3
[2025-07-11T11:46:54.450+0200] {subprocess.py:93} INFO - 25/07/11 11:46:54 INFO DAGScheduler: Got map stage job 7 (parquet at StringIndexer.scala:499) with 1 output partitions
[2025-07-11T11:46:54.450+0200] {subprocess.py:93} INFO - 25/07/11 11:46:54 INFO DAGScheduler: Final stage: ShuffleMapStage 11 (parquet at StringIndexer.scala:499)
[2025-07-11T11:46:54.450+0200] {subprocess.py:93} INFO - 25/07/11 11:46:54 INFO DAGScheduler: Parents of final stage: List()
[2025-07-11T11:46:54.450+0200] {subprocess.py:93} INFO - 25/07/11 11:46:54 INFO DAGScheduler: Missing parents: List()
[2025-07-11T11:46:54.450+0200] {subprocess.py:93} INFO - 25/07/11 11:46:54 INFO DAGScheduler: Submitting ShuffleMapStage 11 (MapPartitionsRDD[22] at parquet at StringIndexer.scala:499), which has no missing parents
[2025-07-11T11:46:54.453+0200] {subprocess.py:93} INFO - 25/07/11 11:46:54 INFO MemoryStore: Block broadcast_9 stored as values in memory (estimated size 8.1 KiB, free 429.5 MiB)
[2025-07-11T11:46:54.455+0200] {subprocess.py:93} INFO - 25/07/11 11:46:54 INFO MemoryStore: Block broadcast_9_piece0 stored as bytes in memory (estimated size 4.4 KiB, free 429.5 MiB)
[2025-07-11T11:46:54.455+0200] {subprocess.py:93} INFO - 25/07/11 11:46:54 INFO BlockManagerInfo: Added broadcast_9_piece0 in memory on 138.4.31.14:39113 (size: 4.4 KiB, free: 434.2 MiB)
[2025-07-11T11:46:54.456+0200] {subprocess.py:93} INFO - 25/07/11 11:46:54 INFO SparkContext: Created broadcast 9 from broadcast at DAGScheduler.scala:1585
[2025-07-11T11:46:54.456+0200] {subprocess.py:93} INFO - 25/07/11 11:46:54 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 11 (MapPartitionsRDD[22] at parquet at StringIndexer.scala:499) (first 15 tasks are for partitions Vector(0))
[2025-07-11T11:46:54.456+0200] {subprocess.py:93} INFO - 25/07/11 11:46:54 INFO TaskSchedulerImpl: Adding task set 11.0 with 1 tasks resource profile 0
[2025-07-11T11:46:54.465+0200] {subprocess.py:93} INFO - 25/07/11 11:46:54 INFO TaskSetManager: Starting task 0.0 in stage 11.0 (TID 20) (138.4.31.14, executor driver, partition 0, PROCESS_LOCAL, 13380 bytes)
[2025-07-11T11:46:54.472+0200] {subprocess.py:93} INFO - 25/07/11 11:46:54 INFO Executor: Running task 0.0 in stage 11.0 (TID 20)
[2025-07-11T11:46:54.477+0200] {subprocess.py:93} INFO - 25/07/11 11:46:54 INFO Executor: Finished task 0.0 in stage 11.0 (TID 20). 1628 bytes result sent to driver
[2025-07-11T11:46:54.477+0200] {subprocess.py:93} INFO - 25/07/11 11:46:54 INFO TaskSetManager: Finished task 0.0 in stage 11.0 (TID 20) in 13 ms on 138.4.31.14 (executor driver) (1/1)
[2025-07-11T11:46:54.477+0200] {subprocess.py:93} INFO - 25/07/11 11:46:54 INFO TaskSchedulerImpl: Removed TaskSet 11.0, whose tasks have all completed, from pool
[2025-07-11T11:46:54.479+0200] {subprocess.py:93} INFO - 25/07/11 11:46:54 INFO DAGScheduler: ShuffleMapStage 11 (parquet at StringIndexer.scala:499) finished in 0,027 s
[2025-07-11T11:46:54.480+0200] {subprocess.py:93} INFO - 25/07/11 11:46:54 INFO DAGScheduler: looking for newly runnable stages
[2025-07-11T11:46:54.480+0200] {subprocess.py:93} INFO - 25/07/11 11:46:54 INFO DAGScheduler: running: Set()
[2025-07-11T11:46:54.480+0200] {subprocess.py:93} INFO - 25/07/11 11:46:54 INFO DAGScheduler: waiting: Set()
[2025-07-11T11:46:54.480+0200] {subprocess.py:93} INFO - 25/07/11 11:46:54 INFO DAGScheduler: failed: Set()
[2025-07-11T11:46:54.526+0200] {subprocess.py:93} INFO - 25/07/11 11:46:54 INFO ParquetUtils: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
[2025-07-11T11:46:54.552+0200] {subprocess.py:93} INFO - 25/07/11 11:46:54 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2025-07-11T11:46:54.552+0200] {subprocess.py:93} INFO - 25/07/11 11:46:54 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2025-07-11T11:46:54.552+0200] {subprocess.py:93} INFO - 25/07/11 11:46:54 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[2025-07-11T11:46:54.553+0200] {subprocess.py:93} INFO - 25/07/11 11:46:54 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2025-07-11T11:46:54.553+0200] {subprocess.py:93} INFO - 25/07/11 11:46:54 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2025-07-11T11:46:54.553+0200] {subprocess.py:93} INFO - 25/07/11 11:46:54 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[2025-07-11T11:46:54.572+0200] {subprocess.py:93} INFO - 25/07/11 11:46:54 INFO SparkContext: Starting job: parquet at StringIndexer.scala:499
[2025-07-11T11:46:54.573+0200] {subprocess.py:93} INFO - 25/07/11 11:46:54 INFO DAGScheduler: Got job 8 (parquet at StringIndexer.scala:499) with 1 output partitions
[2025-07-11T11:46:54.573+0200] {subprocess.py:93} INFO - 25/07/11 11:46:54 INFO DAGScheduler: Final stage: ResultStage 13 (parquet at StringIndexer.scala:499)
[2025-07-11T11:46:54.573+0200] {subprocess.py:93} INFO - 25/07/11 11:46:54 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 12)
[2025-07-11T11:46:54.573+0200] {subprocess.py:93} INFO - 25/07/11 11:46:54 INFO DAGScheduler: Missing parents: List()
[2025-07-11T11:46:54.573+0200] {subprocess.py:93} INFO - 25/07/11 11:46:54 INFO DAGScheduler: Submitting ResultStage 13 (MapPartitionsRDD[24] at parquet at StringIndexer.scala:499), which has no missing parents
[2025-07-11T11:46:54.592+0200] {subprocess.py:93} INFO - 25/07/11 11:46:54 INFO MemoryStore: Block broadcast_10 stored as values in memory (estimated size 230.2 KiB, free 429.3 MiB)
[2025-07-11T11:46:54.596+0200] {subprocess.py:93} INFO - 25/07/11 11:46:54 INFO MemoryStore: Block broadcast_10_piece0 stored as bytes in memory (estimated size 81.0 KiB, free 429.2 MiB)
[2025-07-11T11:46:54.596+0200] {subprocess.py:93} INFO - 25/07/11 11:46:54 INFO BlockManagerInfo: Added broadcast_10_piece0 in memory on 138.4.31.14:39113 (size: 81.0 KiB, free: 434.1 MiB)
[2025-07-11T11:46:54.597+0200] {subprocess.py:93} INFO - 25/07/11 11:46:54 INFO SparkContext: Created broadcast 10 from broadcast at DAGScheduler.scala:1585
[2025-07-11T11:46:54.597+0200] {subprocess.py:93} INFO - 25/07/11 11:46:54 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 13 (MapPartitionsRDD[24] at parquet at StringIndexer.scala:499) (first 15 tasks are for partitions Vector(0))
[2025-07-11T11:46:54.597+0200] {subprocess.py:93} INFO - 25/07/11 11:46:54 INFO TaskSchedulerImpl: Adding task set 13.0 with 1 tasks resource profile 0
[2025-07-11T11:46:54.599+0200] {subprocess.py:93} INFO - 25/07/11 11:46:54 INFO TaskSetManager: Starting task 0.0 in stage 13.0 (TID 21) (138.4.31.14, executor driver, partition 0, NODE_LOCAL, 12952 bytes)
[2025-07-11T11:46:54.599+0200] {subprocess.py:93} INFO - 25/07/11 11:46:54 INFO Executor: Running task 0.0 in stage 13.0 (TID 21)
[2025-07-11T11:46:54.618+0200] {subprocess.py:93} INFO - 25/07/11 11:46:54 INFO ShuffleBlockFetcherIterator: Getting 1 (228.0 B) non-empty blocks including 1 (228.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2025-07-11T11:46:54.618+0200] {subprocess.py:93} INFO - 25/07/11 11:46:54 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2025-07-11T11:46:54.619+0200] {subprocess.py:93} INFO - 25/07/11 11:46:54 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2025-07-11T11:46:54.620+0200] {subprocess.py:93} INFO - 25/07/11 11:46:54 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2025-07-11T11:46:54.620+0200] {subprocess.py:93} INFO - 25/07/11 11:46:54 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[2025-07-11T11:46:54.620+0200] {subprocess.py:93} INFO - 25/07/11 11:46:54 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2025-07-11T11:46:54.620+0200] {subprocess.py:93} INFO - 25/07/11 11:46:54 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2025-07-11T11:46:54.620+0200] {subprocess.py:93} INFO - 25/07/11 11:46:54 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[2025-07-11T11:46:54.624+0200] {subprocess.py:93} INFO - 25/07/11 11:46:54 INFO CodecConfig: Compression: SNAPPY
[2025-07-11T11:46:54.626+0200] {subprocess.py:93} INFO - 25/07/11 11:46:54 INFO CodecConfig: Compression: SNAPPY
[2025-07-11T11:46:54.645+0200] {subprocess.py:93} INFO - 25/07/11 11:46:54 INFO ParquetOutputFormat: ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]
[2025-07-11T11:46:54.667+0200] {subprocess.py:93} INFO - 25/07/11 11:46:54 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
[2025-07-11T11:46:54.667+0200] {subprocess.py:93} INFO - {
[2025-07-11T11:46:54.667+0200] {subprocess.py:93} INFO -   "type" : "struct",
[2025-07-11T11:46:54.667+0200] {subprocess.py:93} INFO -   "fields" : [ {
[2025-07-11T11:46:54.667+0200] {subprocess.py:93} INFO -     "name" : "labelsArray",
[2025-07-11T11:46:54.667+0200] {subprocess.py:93} INFO -     "type" : {
[2025-07-11T11:46:54.667+0200] {subprocess.py:93} INFO -       "type" : "array",
[2025-07-11T11:46:54.667+0200] {subprocess.py:93} INFO -       "elementType" : {
[2025-07-11T11:46:54.667+0200] {subprocess.py:93} INFO -         "type" : "array",
[2025-07-11T11:46:54.667+0200] {subprocess.py:93} INFO -         "elementType" : "string",
[2025-07-11T11:46:54.667+0200] {subprocess.py:93} INFO -         "containsNull" : true
[2025-07-11T11:46:54.667+0200] {subprocess.py:93} INFO -       },
[2025-07-11T11:46:54.668+0200] {subprocess.py:93} INFO -       "containsNull" : true
[2025-07-11T11:46:54.668+0200] {subprocess.py:93} INFO -     },
[2025-07-11T11:46:54.668+0200] {subprocess.py:93} INFO -     "nullable" : true,
[2025-07-11T11:46:54.668+0200] {subprocess.py:93} INFO -     "metadata" : { }
[2025-07-11T11:46:54.668+0200] {subprocess.py:93} INFO -   } ]
[2025-07-11T11:46:54.668+0200] {subprocess.py:93} INFO - }
[2025-07-11T11:46:54.668+0200] {subprocess.py:93} INFO - and corresponding Parquet message type:
[2025-07-11T11:46:54.668+0200] {subprocess.py:93} INFO - message spark_schema {
[2025-07-11T11:46:54.668+0200] {subprocess.py:93} INFO -   optional group labelsArray (LIST) {
[2025-07-11T11:46:54.668+0200] {subprocess.py:93} INFO -     repeated group list {
[2025-07-11T11:46:54.668+0200] {subprocess.py:93} INFO -       optional group element (LIST) {
[2025-07-11T11:46:54.668+0200] {subprocess.py:93} INFO -         repeated group list {
[2025-07-11T11:46:54.668+0200] {subprocess.py:93} INFO -           optional binary element (STRING);
[2025-07-11T11:46:54.669+0200] {subprocess.py:93} INFO -         }
[2025-07-11T11:46:54.669+0200] {subprocess.py:93} INFO -       }
[2025-07-11T11:46:54.669+0200] {subprocess.py:93} INFO -     }
[2025-07-11T11:46:54.669+0200] {subprocess.py:93} INFO -   }
[2025-07-11T11:46:54.669+0200] {subprocess.py:93} INFO - }
[2025-07-11T11:46:54.669+0200] {subprocess.py:93} INFO - 
[2025-07-11T11:46:54.669+0200] {subprocess.py:93} INFO - 
[2025-07-11T11:46:54.726+0200] {subprocess.py:93} INFO - 25/07/11 11:46:54 INFO CodecPool: Got brand-new compressor [.snappy]
[2025-07-11T11:46:55.289+0200] {subprocess.py:93} INFO - 25/07/11 11:46:55 INFO FileOutputCommitter: Saved output of task 'attempt_202507111146549092377606741325308_0013_m_000000_21' to file:/home/monica.fernandez/practica_creativa/models/string_indexer_model_Carrier.bin/data/_temporary/0/task_202507111146549092377606741325308_0013_m_000000
[2025-07-11T11:46:55.289+0200] {subprocess.py:93} INFO - 25/07/11 11:46:55 INFO SparkHadoopMapRedUtil: attempt_202507111146549092377606741325308_0013_m_000000_21: Committed. Elapsed time: 1 ms.
[2025-07-11T11:46:55.294+0200] {subprocess.py:93} INFO - 25/07/11 11:46:55 INFO Executor: Finished task 0.0 in stage 13.0 (TID 21). 4783 bytes result sent to driver
[2025-07-11T11:46:55.295+0200] {subprocess.py:93} INFO - 25/07/11 11:46:55 INFO TaskSetManager: Finished task 0.0 in stage 13.0 (TID 21) in 697 ms on 138.4.31.14 (executor driver) (1/1)
[2025-07-11T11:46:55.295+0200] {subprocess.py:93} INFO - 25/07/11 11:46:55 INFO TaskSchedulerImpl: Removed TaskSet 13.0, whose tasks have all completed, from pool
[2025-07-11T11:46:55.296+0200] {subprocess.py:93} INFO - 25/07/11 11:46:55 INFO DAGScheduler: ResultStage 13 (parquet at StringIndexer.scala:499) finished in 0,721 s
[2025-07-11T11:46:55.297+0200] {subprocess.py:93} INFO - 25/07/11 11:46:55 INFO DAGScheduler: Job 8 is finished. Cancelling potential speculative or zombie tasks for this job
[2025-07-11T11:46:55.297+0200] {subprocess.py:93} INFO - 25/07/11 11:46:55 INFO TaskSchedulerImpl: Killing all running tasks in stage 13: Stage finished
[2025-07-11T11:46:55.297+0200] {subprocess.py:93} INFO - 25/07/11 11:46:55 INFO DAGScheduler: Job 8 finished: parquet at StringIndexer.scala:499, took 0,724388 s
[2025-07-11T11:46:55.298+0200] {subprocess.py:93} INFO - 25/07/11 11:46:55 INFO FileFormatWriter: Start to commit write Job 82ea9fdb-22b9-4883-860a-fc79c82094bd.
[2025-07-11T11:46:55.321+0200] {subprocess.py:93} INFO - 25/07/11 11:46:55 INFO FileFormatWriter: Write Job 82ea9fdb-22b9-4883-860a-fc79c82094bd committed. Elapsed time: 22 ms.
[2025-07-11T11:46:55.326+0200] {subprocess.py:93} INFO - 25/07/11 11:46:55 INFO FileFormatWriter: Finished processing stats for write job 82ea9fdb-22b9-4883-860a-fc79c82094bd.
[2025-07-11T11:46:55.388+0200] {subprocess.py:93} INFO - 25/07/11 11:46:55 INFO V2ScanRelationPushDown:
[2025-07-11T11:46:55.388+0200] {subprocess.py:93} INFO - Pushing operators to origin_dest_distances
[2025-07-11T11:46:55.388+0200] {subprocess.py:93} INFO - Pushed Filters:
[2025-07-11T11:46:55.388+0200] {subprocess.py:93} INFO - Post-Scan Filters:
[2025-07-11T11:46:55.388+0200] {subprocess.py:93} INFO - 
[2025-07-11T11:46:55.397+0200] {subprocess.py:93} INFO - 25/07/11 11:46:55 INFO V2ScanRelationPushDown:
[2025-07-11T11:46:55.397+0200] {subprocess.py:93} INFO - Output: origin#42, dest#43
[2025-07-11T11:46:55.397+0200] {subprocess.py:93} INFO - 
[2025-07-11T11:46:55.421+0200] {subprocess.py:93} INFO - 25/07/11 11:46:55 INFO FileSourceStrategy: Pushed Filters:
[2025-07-11T11:46:55.421+0200] {subprocess.py:93} INFO - 25/07/11 11:46:55 INFO FileSourceStrategy: Post-Scan Filters:
[2025-07-11T11:46:55.445+0200] {subprocess.py:93} INFO - 25/07/11 11:46:55 INFO MemoryStore: Block broadcast_11 stored as values in memory (estimated size 209.9 KiB, free 429.0 MiB)
[2025-07-11T11:46:55.462+0200] {subprocess.py:93} INFO - 25/07/11 11:46:55 INFO BlockManagerInfo: Removed broadcast_8_piece0 on 138.4.31.14:39113 in memory (size: 36.5 KiB, free: 434.1 MiB)
[2025-07-11T11:46:55.465+0200] {subprocess.py:93} INFO - 25/07/11 11:46:55 INFO MemoryStore: Block broadcast_11_piece0 stored as bytes in memory (estimated size 37.7 KiB, free 429.1 MiB)
[2025-07-11T11:46:55.466+0200] {subprocess.py:93} INFO - 25/07/11 11:46:55 INFO BlockManagerInfo: Added broadcast_11_piece0 in memory on 138.4.31.14:39113 (size: 37.7 KiB, free: 434.1 MiB)
[2025-07-11T11:46:55.467+0200] {subprocess.py:93} INFO - 25/07/11 11:46:55 INFO BlockManagerInfo: Removed broadcast_7_piece0 on 138.4.31.14:39113 in memory (size: 23.4 KiB, free: 434.1 MiB)
[2025-07-11T11:46:55.469+0200] {subprocess.py:93} INFO - 25/07/11 11:46:55 INFO SparkContext: Created broadcast 11 from collect at StringIndexer.scala:204
[2025-07-11T11:46:55.470+0200] {subprocess.py:93} INFO - 25/07/11 11:46:55 INFO BlockManagerInfo: Removed broadcast_10_piece0 on 138.4.31.14:39113 in memory (size: 81.0 KiB, free: 434.2 MiB)
[2025-07-11T11:46:55.470+0200] {subprocess.py:93} INFO - 25/07/11 11:46:55 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
[2025-07-11T11:46:55.478+0200] {subprocess.py:93} INFO - 25/07/11 11:46:55 INFO DAGScheduler: Registering RDD 27 (collect at StringIndexer.scala:204) as input to shuffle 4
[2025-07-11T11:46:55.478+0200] {subprocess.py:93} INFO - 25/07/11 11:46:55 INFO DAGScheduler: Got map stage job 9 (collect at StringIndexer.scala:204) with 2 output partitions
[2025-07-11T11:46:55.478+0200] {subprocess.py:93} INFO - 25/07/11 11:46:55 INFO DAGScheduler: Final stage: ShuffleMapStage 14 (collect at StringIndexer.scala:204)
[2025-07-11T11:46:55.478+0200] {subprocess.py:93} INFO - 25/07/11 11:46:55 INFO DAGScheduler: Parents of final stage: List()
[2025-07-11T11:46:55.478+0200] {subprocess.py:93} INFO - 25/07/11 11:46:55 INFO DAGScheduler: Missing parents: List()
[2025-07-11T11:46:55.483+0200] {subprocess.py:93} INFO - 25/07/11 11:46:55 INFO DAGScheduler: Submitting ShuffleMapStage 14 (MapPartitionsRDD[27] at collect at StringIndexer.scala:204), which has no missing parents
[2025-07-11T11:46:55.483+0200] {subprocess.py:93} INFO - 25/07/11 11:46:55 INFO MemoryStore: Block broadcast_12 stored as values in memory (estimated size 13.7 KiB, free 429.4 MiB)
[2025-07-11T11:46:55.483+0200] {subprocess.py:93} INFO - 25/07/11 11:46:55 INFO MemoryStore: Block broadcast_12_piece0 stored as bytes in memory (estimated size 7.0 KiB, free 429.4 MiB)
[2025-07-11T11:46:55.484+0200] {subprocess.py:93} INFO - 25/07/11 11:46:55 INFO BlockManagerInfo: Added broadcast_12_piece0 in memory on 138.4.31.14:39113 (size: 7.0 KiB, free: 434.2 MiB)
[2025-07-11T11:46:55.484+0200] {subprocess.py:93} INFO - 25/07/11 11:46:55 INFO SparkContext: Created broadcast 12 from broadcast at DAGScheduler.scala:1585
[2025-07-11T11:46:55.485+0200] {subprocess.py:93} INFO - 25/07/11 11:46:55 INFO DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 14 (MapPartitionsRDD[27] at collect at StringIndexer.scala:204) (first 15 tasks are for partitions Vector(0, 1))
[2025-07-11T11:46:55.485+0200] {subprocess.py:93} INFO - 25/07/11 11:46:55 INFO TaskSchedulerImpl: Adding task set 14.0 with 2 tasks resource profile 0
[2025-07-11T11:46:55.486+0200] {subprocess.py:93} INFO - 25/07/11 11:46:55 INFO DAGScheduler: Registering RDD 31 (collect at StringIndexer.scala:204) as input to shuffle 5
[2025-07-11T11:46:55.486+0200] {subprocess.py:93} INFO - 25/07/11 11:46:55 INFO DAGScheduler: Got map stage job 10 (collect at StringIndexer.scala:204) with 13 output partitions
[2025-07-11T11:46:55.486+0200] {subprocess.py:93} INFO - 25/07/11 11:46:55 INFO DAGScheduler: Final stage: ShuffleMapStage 15 (collect at StringIndexer.scala:204)
[2025-07-11T11:46:55.486+0200] {subprocess.py:93} INFO - 25/07/11 11:46:55 INFO DAGScheduler: Parents of final stage: List()
[2025-07-11T11:46:55.487+0200] {subprocess.py:93} INFO - 25/07/11 11:46:55 INFO DAGScheduler: Missing parents: List()
[2025-07-11T11:46:55.488+0200] {subprocess.py:93} INFO - 25/07/11 11:46:55 INFO DAGScheduler: Submitting ShuffleMapStage 15 (MapPartitionsRDD[31] at collect at StringIndexer.scala:204), which has no missing parents
[2025-07-11T11:46:55.491+0200] {subprocess.py:93} INFO - 25/07/11 11:46:55 INFO MemoryStore: Block broadcast_13 stored as values in memory (estimated size 18.2 KiB, free 429.4 MiB)
[2025-07-11T11:46:55.492+0200] {subprocess.py:93} INFO - 25/07/11 11:46:55 INFO TaskSetManager: Starting task 0.0 in stage 14.0 (TID 22) (138.4.31.14, executor driver, partition 0, PROCESS_LOCAL, 13595 bytes)
[2025-07-11T11:46:55.492+0200] {subprocess.py:93} INFO - 25/07/11 11:46:55 INFO TaskSetManager: Starting task 1.0 in stage 14.0 (TID 23) (138.4.31.14, executor driver, partition 1, PROCESS_LOCAL, 13595 bytes)
[2025-07-11T11:46:55.493+0200] {subprocess.py:93} INFO - 25/07/11 11:46:55 INFO MemoryStore: Block broadcast_13_piece0 stored as bytes in memory (estimated size 9.0 KiB, free 429.4 MiB)
[2025-07-11T11:46:55.493+0200] {subprocess.py:93} INFO - 25/07/11 11:46:55 INFO BlockManagerInfo: Added broadcast_13_piece0 in memory on 138.4.31.14:39113 (size: 9.0 KiB, free: 434.2 MiB)
[2025-07-11T11:46:55.493+0200] {subprocess.py:93} INFO - 25/07/11 11:46:55 INFO Executor: Running task 0.0 in stage 14.0 (TID 22)
[2025-07-11T11:46:55.493+0200] {subprocess.py:93} INFO - 25/07/11 11:46:55 INFO Executor: Running task 1.0 in stage 14.0 (TID 23)
[2025-07-11T11:46:55.499+0200] {subprocess.py:93} INFO - 25/07/11 11:46:55 INFO BlockManagerInfo: Removed broadcast_9_piece0 on 138.4.31.14:39113 in memory (size: 4.4 KiB, free: 434.2 MiB)
[2025-07-11T11:46:55.506+0200] {subprocess.py:93} INFO - 25/07/11 11:46:55 INFO SparkContext: Created broadcast 13 from broadcast at DAGScheduler.scala:1585
[2025-07-11T11:46:55.506+0200] {subprocess.py:93} INFO - 25/07/11 11:46:55 INFO DAGScheduler: Submitting 13 missing tasks from ShuffleMapStage 15 (MapPartitionsRDD[31] at collect at StringIndexer.scala:204) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12))
[2025-07-11T11:46:55.507+0200] {subprocess.py:93} INFO - 25/07/11 11:46:55 INFO TaskSchedulerImpl: Adding task set 15.0 with 13 tasks resource profile 0
[2025-07-11T11:46:55.509+0200] {subprocess.py:93} INFO - 25/07/11 11:46:55 INFO CodeGenerator: Code generated in 11.692403 ms
[2025-07-11T11:46:55.511+0200] {subprocess.py:93} INFO - 25/07/11 11:46:55 INFO FileScanRDD: Reading File path: file:///home/monica.fernandez/practica_creativa/data/simple_flight_delay_features.jsonl.bz2, range: 4194304-4676447, partition values: [empty row]
[2025-07-11T11:46:55.514+0200] {subprocess.py:93} INFO - 25/07/11 11:46:55 INFO FileScanRDD: Reading File path: file:///home/monica.fernandez/practica_creativa/data/simple_flight_delay_features.jsonl.bz2, range: 0-4194304, partition values: [empty row]
[2025-07-11T11:46:55.517+0200] {subprocess.py:93} INFO - 25/07/11 11:46:55 INFO TaskSetManager: Starting task 0.0 in stage 15.0 (TID 24) (138.4.31.14, executor driver, partition 0, ANY, 14843 bytes)
[2025-07-11T11:46:55.518+0200] {subprocess.py:93} INFO - 25/07/11 11:46:55 INFO TaskSetManager: Starting task 1.0 in stage 15.0 (TID 25) (138.4.31.14, executor driver, partition 1, ANY, 14959 bytes)
[2025-07-11T11:46:55.518+0200] {subprocess.py:93} INFO - 25/07/11 11:46:55 INFO Executor: Running task 0.0 in stage 15.0 (TID 24)
[2025-07-11T11:46:55.519+0200] {subprocess.py:93} INFO - 25/07/11 11:46:55 INFO Executor: Running task 1.0 in stage 15.0 (TID 25)
[2025-07-11T11:46:55.566+0200] {subprocess.py:93} INFO - 25/07/11 11:46:55 INFO BlockManagerInfo: Removed broadcast_6_piece0 on 138.4.31.14:39113 in memory (size: 23.5 KiB, free: 434.2 MiB)
[2025-07-11T11:46:55.634+0200] {subprocess.py:93} INFO - 25/07/11 11:46:55 INFO Executor: Finished task 0.0 in stage 15.0 (TID 24). 1945 bytes result sent to driver
[2025-07-11T11:46:55.637+0200] {subprocess.py:93} INFO - 25/07/11 11:46:55 INFO TaskSetManager: Starting task 2.0 in stage 15.0 (TID 26) (138.4.31.14, executor driver, partition 2, ANY, 14963 bytes)
[2025-07-11T11:46:55.637+0200] {subprocess.py:93} INFO - 25/07/11 11:46:55 INFO TaskSetManager: Finished task 0.0 in stage 15.0 (TID 24) in 124 ms on 138.4.31.14 (executor driver) (1/13)
[2025-07-11T11:46:55.642+0200] {subprocess.py:93} INFO - 25/07/11 11:46:55 INFO Executor: Running task 2.0 in stage 15.0 (TID 26)
[2025-07-11T11:46:55.643+0200] {subprocess.py:93} INFO - 25/07/11 11:46:55 INFO Executor: Finished task 1.0 in stage 15.0 (TID 25). 1945 bytes result sent to driver
[2025-07-11T11:46:55.644+0200] {subprocess.py:93} INFO - 25/07/11 11:46:55 INFO TaskSetManager: Starting task 3.0 in stage 15.0 (TID 27) (138.4.31.14, executor driver, partition 3, ANY, 14843 bytes)
[2025-07-11T11:46:55.645+0200] {subprocess.py:93} INFO - 25/07/11 11:46:55 INFO Executor: Running task 3.0 in stage 15.0 (TID 27)
[2025-07-11T11:46:55.646+0200] {subprocess.py:93} INFO - 25/07/11 11:46:55 INFO TaskSetManager: Finished task 1.0 in stage 15.0 (TID 25) in 128 ms on 138.4.31.14 (executor driver) (2/13)
[2025-07-11T11:46:55.703+0200] {subprocess.py:93} INFO - 25/07/11 11:46:55 INFO Executor: Finished task 3.0 in stage 15.0 (TID 27). 1945 bytes result sent to driver
[2025-07-11T11:46:55.705+0200] {subprocess.py:93} INFO - 25/07/11 11:46:55 INFO TaskSetManager: Starting task 4.0 in stage 15.0 (TID 28) (138.4.31.14, executor driver, partition 4, ANY, 14843 bytes)
[2025-07-11T11:46:55.705+0200] {subprocess.py:93} INFO - 25/07/11 11:46:55 INFO Executor: Running task 4.0 in stage 15.0 (TID 28)
[2025-07-11T11:46:55.705+0200] {subprocess.py:93} INFO - 25/07/11 11:46:55 INFO TaskSetManager: Finished task 3.0 in stage 15.0 (TID 27) in 61 ms on 138.4.31.14 (executor driver) (3/13)
[2025-07-11T11:46:55.726+0200] {subprocess.py:93} INFO - 25/07/11 11:46:55 INFO Executor: Finished task 2.0 in stage 15.0 (TID 26). 1945 bytes result sent to driver
[2025-07-11T11:46:55.728+0200] {subprocess.py:93} INFO - 25/07/11 11:46:55 INFO TaskSetManager: Starting task 5.0 in stage 15.0 (TID 29) (138.4.31.14, executor driver, partition 5, ANY, 14843 bytes)
[2025-07-11T11:46:55.729+0200] {subprocess.py:93} INFO - 25/07/11 11:46:55 INFO TaskSetManager: Finished task 2.0 in stage 15.0 (TID 26) in 93 ms on 138.4.31.14 (executor driver) (4/13)
[2025-07-11T11:46:55.730+0200] {subprocess.py:93} INFO - 25/07/11 11:46:55 INFO Executor: Running task 5.0 in stage 15.0 (TID 29)
[2025-07-11T11:46:55.793+0200] {subprocess.py:93} INFO - 25/07/11 11:46:55 INFO Executor: Finished task 4.0 in stage 15.0 (TID 28). 1945 bytes result sent to driver
[2025-07-11T11:46:55.795+0200] {subprocess.py:93} INFO - 25/07/11 11:46:55 INFO TaskSetManager: Starting task 6.0 in stage 15.0 (TID 30) (138.4.31.14, executor driver, partition 6, ANY, 14843 bytes)
[2025-07-11T11:46:55.796+0200] {subprocess.py:93} INFO - 25/07/11 11:46:55 INFO Executor: Running task 6.0 in stage 15.0 (TID 30)
[2025-07-11T11:46:55.796+0200] {subprocess.py:93} INFO - 25/07/11 11:46:55 INFO TaskSetManager: Finished task 4.0 in stage 15.0 (TID 28) in 91 ms on 138.4.31.14 (executor driver) (5/13)
[2025-07-11T11:46:55.842+0200] {subprocess.py:93} INFO - 25/07/11 11:46:55 INFO Executor: Finished task 5.0 in stage 15.0 (TID 29). 1945 bytes result sent to driver
[2025-07-11T11:46:55.843+0200] {subprocess.py:93} INFO - 25/07/11 11:46:55 INFO TaskSetManager: Starting task 7.0 in stage 15.0 (TID 31) (138.4.31.14, executor driver, partition 7, ANY, 14843 bytes)
[2025-07-11T11:46:55.846+0200] {subprocess.py:93} INFO - 25/07/11 11:46:55 INFO Executor: Running task 7.0 in stage 15.0 (TID 31)
[2025-07-11T11:46:55.846+0200] {subprocess.py:93} INFO - 25/07/11 11:46:55 INFO TaskSetManager: Finished task 5.0 in stage 15.0 (TID 29) in 117 ms on 138.4.31.14 (executor driver) (6/13)
[2025-07-11T11:46:55.969+0200] {subprocess.py:93} INFO - 25/07/11 11:46:55 INFO Executor: Finished task 6.0 in stage 15.0 (TID 30). 1988 bytes result sent to driver
[2025-07-11T11:46:55.969+0200] {subprocess.py:93} INFO - 25/07/11 11:46:55 INFO Executor: Finished task 7.0 in stage 15.0 (TID 31). 1988 bytes result sent to driver
[2025-07-11T11:46:55.980+0200] {subprocess.py:93} INFO - 25/07/11 11:46:55 INFO TaskSetManager: Starting task 8.0 in stage 15.0 (TID 32) (138.4.31.14, executor driver, partition 8, ANY, 14843 bytes)
[2025-07-11T11:46:55.980+0200] {subprocess.py:93} INFO - 25/07/11 11:46:55 INFO Executor: Running task 8.0 in stage 15.0 (TID 32)
[2025-07-11T11:46:55.980+0200] {subprocess.py:93} INFO - 25/07/11 11:46:55 INFO TaskSetManager: Starting task 9.0 in stage 15.0 (TID 33) (138.4.31.14, executor driver, partition 9, ANY, 14843 bytes)
[2025-07-11T11:46:55.980+0200] {subprocess.py:93} INFO - 25/07/11 11:46:55 INFO Executor: Running task 9.0 in stage 15.0 (TID 33)
[2025-07-11T11:46:55.980+0200] {subprocess.py:93} INFO - 25/07/11 11:46:55 INFO TaskSetManager: Finished task 6.0 in stage 15.0 (TID 30) in 186 ms on 138.4.31.14 (executor driver) (7/13)
[2025-07-11T11:46:55.981+0200] {subprocess.py:93} INFO - 25/07/11 11:46:55 INFO TaskSetManager: Finished task 7.0 in stage 15.0 (TID 31) in 138 ms on 138.4.31.14 (executor driver) (8/13)
[2025-07-11T11:46:56.093+0200] {subprocess.py:93} INFO - 25/07/11 11:46:56 INFO Executor: Finished task 8.0 in stage 15.0 (TID 32). 1945 bytes result sent to driver
[2025-07-11T11:46:56.094+0200] {subprocess.py:93} INFO - 25/07/11 11:46:56 INFO TaskSetManager: Starting task 10.0 in stage 15.0 (TID 34) (138.4.31.14, executor driver, partition 10, ANY, 14843 bytes)
[2025-07-11T11:46:56.095+0200] {subprocess.py:93} INFO - 25/07/11 11:46:56 INFO TaskSetManager: Finished task 8.0 in stage 15.0 (TID 32) in 125 ms on 138.4.31.14 (executor driver) (9/13)
[2025-07-11T11:46:56.096+0200] {subprocess.py:93} INFO - 25/07/11 11:46:56 INFO Executor: Running task 10.0 in stage 15.0 (TID 34)
[2025-07-11T11:46:56.100+0200] {subprocess.py:93} INFO - 25/07/11 11:46:56 INFO Executor: Finished task 9.0 in stage 15.0 (TID 33). 1945 bytes result sent to driver
[2025-07-11T11:46:56.106+0200] {subprocess.py:93} INFO - 25/07/11 11:46:56 INFO TaskSetManager: Starting task 11.0 in stage 15.0 (TID 35) (138.4.31.14, executor driver, partition 11, ANY, 14843 bytes)
[2025-07-11T11:46:56.106+0200] {subprocess.py:93} INFO - 25/07/11 11:46:56 INFO TaskSetManager: Finished task 9.0 in stage 15.0 (TID 33) in 133 ms on 138.4.31.14 (executor driver) (10/13)
[2025-07-11T11:46:56.106+0200] {subprocess.py:93} INFO - 25/07/11 11:46:56 INFO Executor: Running task 11.0 in stage 15.0 (TID 35)
[2025-07-11T11:46:56.205+0200] {subprocess.py:93} INFO - 25/07/11 11:46:56 INFO Executor: Finished task 10.0 in stage 15.0 (TID 34). 1988 bytes result sent to driver
[2025-07-11T11:46:56.206+0200] {subprocess.py:93} INFO - 25/07/11 11:46:56 INFO TaskSetManager: Starting task 12.0 in stage 15.0 (TID 36) (138.4.31.14, executor driver, partition 12, ANY, 14963 bytes)
[2025-07-11T11:46:56.207+0200] {subprocess.py:93} INFO - 25/07/11 11:46:56 INFO Executor: Running task 12.0 in stage 15.0 (TID 36)
[2025-07-11T11:46:56.207+0200] {subprocess.py:93} INFO - 25/07/11 11:46:56 INFO TaskSetManager: Finished task 10.0 in stage 15.0 (TID 34) in 113 ms on 138.4.31.14 (executor driver) (11/13)
[2025-07-11T11:46:56.209+0200] {subprocess.py:93} INFO - 25/07/11 11:46:56 INFO Executor: Finished task 11.0 in stage 15.0 (TID 35). 1945 bytes result sent to driver
[2025-07-11T11:46:56.210+0200] {subprocess.py:93} INFO - 25/07/11 11:46:56 INFO TaskSetManager: Finished task 11.0 in stage 15.0 (TID 35) in 106 ms on 138.4.31.14 (executor driver) (12/13)
[2025-07-11T11:46:56.281+0200] {subprocess.py:93} INFO - 25/07/11 11:46:56 INFO Executor: Finished task 12.0 in stage 15.0 (TID 36). 2031 bytes result sent to driver
[2025-07-11T11:46:56.282+0200] {subprocess.py:93} INFO - 25/07/11 11:46:56 INFO TaskSetManager: Finished task 12.0 in stage 15.0 (TID 36) in 77 ms on 138.4.31.14 (executor driver) (13/13)
[2025-07-11T11:46:56.282+0200] {subprocess.py:93} INFO - 25/07/11 11:46:56 INFO TaskSchedulerImpl: Removed TaskSet 15.0, whose tasks have all completed, from pool
[2025-07-11T11:46:56.285+0200] {subprocess.py:93} INFO - 25/07/11 11:46:56 INFO DAGScheduler: ShuffleMapStage 15 (collect at StringIndexer.scala:204) finished in 0,794 s
[2025-07-11T11:46:56.285+0200] {subprocess.py:93} INFO - 25/07/11 11:46:56 INFO DAGScheduler: looking for newly runnable stages
[2025-07-11T11:46:56.285+0200] {subprocess.py:93} INFO - 25/07/11 11:46:56 INFO DAGScheduler: running: Set(ShuffleMapStage 14)
[2025-07-11T11:46:56.285+0200] {subprocess.py:93} INFO - 25/07/11 11:46:56 INFO DAGScheduler: waiting: Set()
[2025-07-11T11:46:56.285+0200] {subprocess.py:93} INFO - 25/07/11 11:46:56 INFO DAGScheduler: failed: Set()
[2025-07-11T11:46:56.298+0200] {subprocess.py:93} INFO - 25/07/11 11:46:56 INFO ShufflePartitionsUtil: For shuffle(5), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576
[2025-07-11T11:46:56.353+0200] {subprocess.py:93} INFO - 25/07/11 11:46:56 INFO SparkContext: Starting job: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264
[2025-07-11T11:46:56.355+0200] {subprocess.py:93} INFO - 25/07/11 11:46:56 INFO DAGScheduler: Got job 11 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) with 1 output partitions
[2025-07-11T11:46:56.355+0200] {subprocess.py:93} INFO - 25/07/11 11:46:56 INFO DAGScheduler: Final stage: ResultStage 17 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264)
[2025-07-11T11:46:56.356+0200] {subprocess.py:93} INFO - 25/07/11 11:46:56 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 16)
[2025-07-11T11:46:56.356+0200] {subprocess.py:93} INFO - 25/07/11 11:46:56 INFO DAGScheduler: Missing parents: List()
[2025-07-11T11:46:56.356+0200] {subprocess.py:93} INFO - 25/07/11 11:46:56 INFO DAGScheduler: Submitting ResultStage 17 (MapPartitionsRDD[33] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264), which has no missing parents
[2025-07-11T11:46:56.360+0200] {subprocess.py:93} INFO - 25/07/11 11:46:56 INFO MemoryStore: Block broadcast_14 stored as values in memory (estimated size 8.2 KiB, free 429.5 MiB)
[2025-07-11T11:46:56.361+0200] {subprocess.py:93} INFO - 25/07/11 11:46:56 INFO MemoryStore: Block broadcast_14_piece0 stored as bytes in memory (estimated size 4.2 KiB, free 429.5 MiB)
[2025-07-11T11:46:56.361+0200] {subprocess.py:93} INFO - 25/07/11 11:46:56 INFO BlockManagerInfo: Added broadcast_14_piece0 in memory on 138.4.31.14:39113 (size: 4.2 KiB, free: 434.2 MiB)
[2025-07-11T11:46:56.363+0200] {subprocess.py:93} INFO - 25/07/11 11:46:56 INFO SparkContext: Created broadcast 14 from broadcast at DAGScheduler.scala:1585
[2025-07-11T11:46:56.363+0200] {subprocess.py:93} INFO - 25/07/11 11:46:56 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 17 (MapPartitionsRDD[33] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) (first 15 tasks are for partitions Vector(0))
[2025-07-11T11:46:56.364+0200] {subprocess.py:93} INFO - 25/07/11 11:46:56 INFO TaskSchedulerImpl: Adding task set 17.0 with 1 tasks resource profile 0
[2025-07-11T11:46:56.365+0200] {subprocess.py:93} INFO - 25/07/11 11:46:56 INFO TaskSetManager: Starting task 0.0 in stage 17.0 (TID 37) (138.4.31.14, executor driver, partition 0, NODE_LOCAL, 12968 bytes)
[2025-07-11T11:46:56.375+0200] {subprocess.py:93} INFO - 25/07/11 11:46:56 INFO Executor: Running task 0.0 in stage 17.0 (TID 37)
[2025-07-11T11:46:56.375+0200] {subprocess.py:93} INFO - 25/07/11 11:46:56 INFO ShuffleBlockFetcherIterator: Getting 13 (196.8 KiB) non-empty blocks including 13 (196.8 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2025-07-11T11:46:56.376+0200] {subprocess.py:93} INFO - 25/07/11 11:46:56 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
[2025-07-11T11:46:56.426+0200] {subprocess.py:93} INFO - 25/07/11 11:46:56 INFO Executor: Finished task 0.0 in stage 17.0 (TID 37). 40798 bytes result sent to driver
[2025-07-11T11:46:56.433+0200] {subprocess.py:93} INFO - 25/07/11 11:46:56 INFO TaskSetManager: Finished task 0.0 in stage 17.0 (TID 37) in 69 ms on 138.4.31.14 (executor driver) (1/1)
[2025-07-11T11:46:56.433+0200] {subprocess.py:93} INFO - 25/07/11 11:46:56 INFO TaskSchedulerImpl: Removed TaskSet 17.0, whose tasks have all completed, from pool
[2025-07-11T11:46:56.437+0200] {subprocess.py:93} INFO - 25/07/11 11:46:56 INFO DAGScheduler: ResultStage 17 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) finished in 0,075 s
[2025-07-11T11:46:56.437+0200] {subprocess.py:93} INFO - 25/07/11 11:46:56 INFO DAGScheduler: Job 11 is finished. Cancelling potential speculative or zombie tasks for this job
[2025-07-11T11:46:56.437+0200] {subprocess.py:93} INFO - 25/07/11 11:46:56 INFO TaskSchedulerImpl: Killing all running tasks in stage 17: Stage finished
[2025-07-11T11:46:56.437+0200] {subprocess.py:93} INFO - 25/07/11 11:46:56 INFO DAGScheduler: Job 11 finished: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264, took 0,080954 s
[2025-07-11T11:46:56.466+0200] {subprocess.py:93} INFO - 25/07/11 11:46:56 INFO MemoryStore: Block broadcast_15 stored as values in memory (estimated size 4.3 MiB, free 425.2 MiB)
[2025-07-11T11:46:56.474+0200] {subprocess.py:93} INFO - 25/07/11 11:46:56 INFO BlockManagerInfo: Removed broadcast_2_piece0 on 138.4.31.14:39113 in memory (size: 7.1 KiB, free: 434.2 MiB)
[2025-07-11T11:46:56.476+0200] {subprocess.py:93} INFO - 25/07/11 11:46:56 INFO MemoryStore: Block broadcast_15_piece0 stored as bytes in memory (estimated size 89.9 KiB, free 425.2 MiB)
[2025-07-11T11:46:56.477+0200] {subprocess.py:93} INFO - 25/07/11 11:46:56 INFO BlockManagerInfo: Added broadcast_15_piece0 in memory on 138.4.31.14:39113 (size: 89.9 KiB, free: 434.1 MiB)
[2025-07-11T11:46:56.478+0200] {subprocess.py:93} INFO - 25/07/11 11:46:56 INFO SparkContext: Created broadcast 15 from $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264
[2025-07-11T11:46:56.556+0200] {subprocess.py:93} INFO - 25/07/11 11:46:56 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 138.4.31.14:39113 in memory (size: 37.7 KiB, free: 434.2 MiB)
[2025-07-11T11:46:56.573+0200] {subprocess.py:93} INFO - 25/07/11 11:46:56 INFO BlockManagerInfo: Removed broadcast_5_piece0 on 138.4.31.14:39113 in memory (size: 89.6 KiB, free: 434.2 MiB)
[2025-07-11T11:46:56.584+0200] {subprocess.py:93} INFO - 25/07/11 11:46:56 INFO BlockManagerInfo: Removed broadcast_3_piece0 on 138.4.31.14:39113 in memory (size: 9.0 KiB, free: 434.3 MiB)
[2025-07-11T11:46:57.079+0200] {subprocess.py:93} INFO - 25/07/11 11:46:57 INFO Executor: Finished task 1.0 in stage 14.0 (TID 23). 1959 bytes result sent to driver
[2025-07-11T11:46:57.081+0200] {subprocess.py:93} INFO - 25/07/11 11:46:57 INFO TaskSetManager: Finished task 1.0 in stage 14.0 (TID 23) in 1589 ms on 138.4.31.14 (executor driver) (1/2)
[2025-07-11T11:46:57.434+0200] {subprocess.py:93} INFO - 25/07/11 11:46:57 INFO BlockManagerInfo: Removed broadcast_14_piece0 on 138.4.31.14:39113 in memory (size: 4.2 KiB, free: 434.3 MiB)
[2025-07-11T11:47:03.799+0200] {subprocess.py:93} INFO - 25/07/11 11:47:03 INFO Executor: Finished task 0.0 in stage 14.0 (TID 22). 1959 bytes result sent to driver
[2025-07-11T11:47:03.800+0200] {subprocess.py:93} INFO - 25/07/11 11:47:03 INFO TaskSetManager: Finished task 0.0 in stage 14.0 (TID 22) in 8309 ms on 138.4.31.14 (executor driver) (2/2)
[2025-07-11T11:47:03.800+0200] {subprocess.py:93} INFO - 25/07/11 11:47:03 INFO TaskSchedulerImpl: Removed TaskSet 14.0, whose tasks have all completed, from pool
[2025-07-11T11:47:03.800+0200] {subprocess.py:93} INFO - 25/07/11 11:47:03 INFO DAGScheduler: ShuffleMapStage 14 (collect at StringIndexer.scala:204) finished in 8,321 s
[2025-07-11T11:47:03.800+0200] {subprocess.py:93} INFO - 25/07/11 11:47:03 INFO DAGScheduler: looking for newly runnable stages
[2025-07-11T11:47:03.800+0200] {subprocess.py:93} INFO - 25/07/11 11:47:03 INFO DAGScheduler: running: Set()
[2025-07-11T11:47:03.801+0200] {subprocess.py:93} INFO - 25/07/11 11:47:03 INFO DAGScheduler: waiting: Set()
[2025-07-11T11:47:03.801+0200] {subprocess.py:93} INFO - 25/07/11 11:47:03 INFO DAGScheduler: failed: Set()
[2025-07-11T11:47:03.808+0200] {subprocess.py:93} INFO - 25/07/11 11:47:03 INFO ShufflePartitionsUtil: For shuffle(4), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576
[2025-07-11T11:47:03.838+0200] {subprocess.py:93} INFO - 25/07/11 11:47:03 INFO CodeGenerator: Code generated in 11.881601 ms
[2025-07-11T11:47:03.839+0200] {subprocess.py:93} INFO - 25/07/11 11:47:03 INFO DAGScheduler: Registering RDD 37 (collect at StringIndexer.scala:204) as input to shuffle 6
[2025-07-11T11:47:03.839+0200] {subprocess.py:93} INFO - 25/07/11 11:47:03 INFO DAGScheduler: Got map stage job 12 (collect at StringIndexer.scala:204) with 1 output partitions
[2025-07-11T11:47:03.839+0200] {subprocess.py:93} INFO - 25/07/11 11:47:03 INFO DAGScheduler: Final stage: ShuffleMapStage 19 (collect at StringIndexer.scala:204)
[2025-07-11T11:47:03.839+0200] {subprocess.py:93} INFO - 25/07/11 11:47:03 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 18)
[2025-07-11T11:47:03.839+0200] {subprocess.py:93} INFO - 25/07/11 11:47:03 INFO DAGScheduler: Missing parents: List()
[2025-07-11T11:47:03.840+0200] {subprocess.py:93} INFO - 25/07/11 11:47:03 INFO DAGScheduler: Submitting ShuffleMapStage 19 (MapPartitionsRDD[37] at collect at StringIndexer.scala:204), which has no missing parents
[2025-07-11T11:47:03.842+0200] {subprocess.py:93} INFO - 25/07/11 11:47:03 INFO MemoryStore: Block broadcast_16 stored as values in memory (estimated size 49.5 KiB, free 429.7 MiB)
[2025-07-11T11:47:03.843+0200] {subprocess.py:93} INFO - 25/07/11 11:47:03 INFO MemoryStore: Block broadcast_16_piece0 stored as bytes in memory (estimated size 23.6 KiB, free 429.7 MiB)
[2025-07-11T11:47:03.845+0200] {subprocess.py:93} INFO - 25/07/11 11:47:03 INFO BlockManagerInfo: Added broadcast_16_piece0 in memory on 138.4.31.14:39113 (size: 23.6 KiB, free: 434.2 MiB)
[2025-07-11T11:47:03.845+0200] {subprocess.py:93} INFO - 25/07/11 11:47:03 INFO SparkContext: Created broadcast 16 from broadcast at DAGScheduler.scala:1585
[2025-07-11T11:47:03.845+0200] {subprocess.py:93} INFO - 25/07/11 11:47:03 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 19 (MapPartitionsRDD[37] at collect at StringIndexer.scala:204) (first 15 tasks are for partitions Vector(0))
[2025-07-11T11:47:03.845+0200] {subprocess.py:93} INFO - 25/07/11 11:47:03 INFO TaskSchedulerImpl: Adding task set 19.0 with 1 tasks resource profile 0
[2025-07-11T11:47:03.855+0200] {subprocess.py:93} INFO - 25/07/11 11:47:03 INFO TaskSetManager: Starting task 0.0 in stage 19.0 (TID 38) (138.4.31.14, executor driver, partition 0, NODE_LOCAL, 12957 bytes)
[2025-07-11T11:47:03.855+0200] {subprocess.py:93} INFO - 25/07/11 11:47:03 INFO Executor: Running task 0.0 in stage 19.0 (TID 38)
[2025-07-11T11:47:03.862+0200] {subprocess.py:93} INFO - 25/07/11 11:47:03 INFO ShuffleBlockFetcherIterator: Getting 2 (887.5 KiB) non-empty blocks including 2 (887.5 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2025-07-11T11:47:03.862+0200] {subprocess.py:93} INFO - 25/07/11 11:47:03 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2025-07-11T11:47:03.872+0200] {subprocess.py:93} INFO - 25/07/11 11:47:03 INFO CodeGenerator: Code generated in 9.986862 ms
[2025-07-11T11:47:04.141+0200] {subprocess.py:93} INFO - 25/07/11 11:47:04 INFO Executor: Finished task 0.0 in stage 19.0 (TID 38). 6444 bytes result sent to driver
[2025-07-11T11:47:04.142+0200] {subprocess.py:93} INFO - 25/07/11 11:47:04 INFO TaskSetManager: Finished task 0.0 in stage 19.0 (TID 38) in 298 ms on 138.4.31.14 (executor driver) (1/1)
[2025-07-11T11:47:04.142+0200] {subprocess.py:93} INFO - 25/07/11 11:47:04 INFO TaskSchedulerImpl: Removed TaskSet 19.0, whose tasks have all completed, from pool
[2025-07-11T11:47:04.143+0200] {subprocess.py:93} INFO - 25/07/11 11:47:04 INFO DAGScheduler: ShuffleMapStage 19 (collect at StringIndexer.scala:204) finished in 0,304 s
[2025-07-11T11:47:04.143+0200] {subprocess.py:93} INFO - 25/07/11 11:47:04 INFO DAGScheduler: looking for newly runnable stages
[2025-07-11T11:47:04.143+0200] {subprocess.py:93} INFO - 25/07/11 11:47:04 INFO DAGScheduler: running: Set()
[2025-07-11T11:47:04.144+0200] {subprocess.py:93} INFO - 25/07/11 11:47:04 INFO DAGScheduler: waiting: Set()
[2025-07-11T11:47:04.144+0200] {subprocess.py:93} INFO - 25/07/11 11:47:04 INFO DAGScheduler: failed: Set()
[2025-07-11T11:47:04.159+0200] {subprocess.py:93} INFO - 25/07/11 11:47:04 INFO SparkContext: Starting job: collect at StringIndexer.scala:204
[2025-07-11T11:47:04.161+0200] {subprocess.py:93} INFO - 25/07/11 11:47:04 INFO DAGScheduler: Got job 13 (collect at StringIndexer.scala:204) with 1 output partitions
[2025-07-11T11:47:04.161+0200] {subprocess.py:93} INFO - 25/07/11 11:47:04 INFO DAGScheduler: Final stage: ResultStage 22 (collect at StringIndexer.scala:204)
[2025-07-11T11:47:04.161+0200] {subprocess.py:93} INFO - 25/07/11 11:47:04 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 21)
[2025-07-11T11:47:04.161+0200] {subprocess.py:93} INFO - 25/07/11 11:47:04 INFO DAGScheduler: Missing parents: List()
[2025-07-11T11:47:04.161+0200] {subprocess.py:93} INFO - 25/07/11 11:47:04 INFO DAGScheduler: Submitting ResultStage 22 (MapPartitionsRDD[40] at collect at StringIndexer.scala:204), which has no missing parents
[2025-07-11T11:47:04.167+0200] {subprocess.py:93} INFO - 25/07/11 11:47:04 INFO MemoryStore: Block broadcast_17 stored as values in memory (estimated size 48.7 KiB, free 429.7 MiB)
[2025-07-11T11:47:04.176+0200] {subprocess.py:93} INFO - 25/07/11 11:47:04 INFO MemoryStore: Block broadcast_17_piece0 stored as bytes in memory (estimated size 23.3 KiB, free 429.6 MiB)
[2025-07-11T11:47:04.177+0200] {subprocess.py:93} INFO - 25/07/11 11:47:04 INFO BlockManagerInfo: Added broadcast_17_piece0 in memory on 138.4.31.14:39113 (size: 23.3 KiB, free: 434.2 MiB)
[2025-07-11T11:47:04.178+0200] {subprocess.py:93} INFO - 25/07/11 11:47:04 INFO SparkContext: Created broadcast 17 from broadcast at DAGScheduler.scala:1585
[2025-07-11T11:47:04.178+0200] {subprocess.py:93} INFO - 25/07/11 11:47:04 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 22 (MapPartitionsRDD[40] at collect at StringIndexer.scala:204) (first 15 tasks are for partitions Vector(0))
[2025-07-11T11:47:04.178+0200] {subprocess.py:93} INFO - 25/07/11 11:47:04 INFO TaskSchedulerImpl: Adding task set 22.0 with 1 tasks resource profile 0
[2025-07-11T11:47:04.180+0200] {subprocess.py:93} INFO - 25/07/11 11:47:04 INFO TaskSetManager: Starting task 0.0 in stage 22.0 (TID 39) (138.4.31.14, executor driver, partition 0, NODE_LOCAL, 12952 bytes)
[2025-07-11T11:47:04.180+0200] {subprocess.py:93} INFO - 25/07/11 11:47:04 INFO Executor: Running task 0.0 in stage 22.0 (TID 39)
[2025-07-11T11:47:04.184+0200] {subprocess.py:93} INFO - 25/07/11 11:47:04 INFO BlockManagerInfo: Removed broadcast_16_piece0 on 138.4.31.14:39113 in memory (size: 23.6 KiB, free: 434.2 MiB)
[2025-07-11T11:47:04.186+0200] {subprocess.py:93} INFO - 25/07/11 11:47:04 INFO ShuffleBlockFetcherIterator: Getting 1 (3.2 KiB) non-empty blocks including 1 (3.2 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2025-07-11T11:47:04.187+0200] {subprocess.py:93} INFO - 25/07/11 11:47:04 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2025-07-11T11:47:04.222+0200] {subprocess.py:93} INFO - 25/07/11 11:47:04 INFO Executor: Finished task 0.0 in stage 22.0 (TID 39). 10966 bytes result sent to driver
[2025-07-11T11:47:04.226+0200] {subprocess.py:93} INFO - 25/07/11 11:47:04 INFO TaskSetManager: Finished task 0.0 in stage 22.0 (TID 39) in 44 ms on 138.4.31.14 (executor driver) (1/1)
[2025-07-11T11:47:04.226+0200] {subprocess.py:93} INFO - 25/07/11 11:47:04 INFO TaskSchedulerImpl: Removed TaskSet 22.0, whose tasks have all completed, from pool
[2025-07-11T11:47:04.226+0200] {subprocess.py:93} INFO - 25/07/11 11:47:04 INFO DAGScheduler: ResultStage 22 (collect at StringIndexer.scala:204) finished in 0,060 s
[2025-07-11T11:47:04.235+0200] {subprocess.py:93} INFO - 25/07/11 11:47:04 INFO DAGScheduler: Job 13 is finished. Cancelling potential speculative or zombie tasks for this job
[2025-07-11T11:47:04.236+0200] {subprocess.py:93} INFO - 25/07/11 11:47:04 INFO TaskSchedulerImpl: Killing all running tasks in stage 22: Stage finished
[2025-07-11T11:47:04.236+0200] {subprocess.py:93} INFO - 25/07/11 11:47:04 INFO DAGScheduler: Job 13 finished: collect at StringIndexer.scala:204, took 0,069370 s
[2025-07-11T11:47:04.367+0200] {subprocess.py:93} INFO - 25/07/11 11:47:04 INFO FileSystemOverwrite: Path /home/monica.fernandez/practica_creativa/models/string_indexer_model_Origin.bin already exists. It will be overwritten.
[2025-07-11T11:47:04.414+0200] {subprocess.py:93} INFO - 25/07/11 11:47:04 INFO HadoopMapRedCommitProtocol: Using output committer class org.apache.hadoop.mapred.FileOutputCommitter
[2025-07-11T11:47:04.414+0200] {subprocess.py:93} INFO - 25/07/11 11:47:04 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2025-07-11T11:47:04.414+0200] {subprocess.py:93} INFO - 25/07/11 11:47:04 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2025-07-11T11:47:04.468+0200] {subprocess.py:93} INFO - 25/07/11 11:47:04 INFO SparkContext: Starting job: runJob at SparkHadoopWriter.scala:83
[2025-07-11T11:47:04.473+0200] {subprocess.py:93} INFO - 25/07/11 11:47:04 INFO DAGScheduler: Got job 14 (runJob at SparkHadoopWriter.scala:83) with 1 output partitions
[2025-07-11T11:47:04.473+0200] {subprocess.py:93} INFO - 25/07/11 11:47:04 INFO DAGScheduler: Final stage: ResultStage 23 (runJob at SparkHadoopWriter.scala:83)
[2025-07-11T11:47:04.473+0200] {subprocess.py:93} INFO - 25/07/11 11:47:04 INFO DAGScheduler: Parents of final stage: List()
[2025-07-11T11:47:04.474+0200] {subprocess.py:93} INFO - 25/07/11 11:47:04 INFO DAGScheduler: Missing parents: List()
[2025-07-11T11:47:04.474+0200] {subprocess.py:93} INFO - 25/07/11 11:47:04 INFO DAGScheduler: Submitting ResultStage 23 (MapPartitionsRDD[42] at saveAsTextFile at ReadWrite.scala:413), which has no missing parents
[2025-07-11T11:47:04.482+0200] {subprocess.py:93} INFO - 25/07/11 11:47:04 INFO MemoryStore: Block broadcast_18 stored as values in memory (estimated size 101.3 KiB, free 429.6 MiB)
[2025-07-11T11:47:04.483+0200] {subprocess.py:93} INFO - 25/07/11 11:47:04 INFO MemoryStore: Block broadcast_18_piece0 stored as bytes in memory (estimated size 36.5 KiB, free 429.6 MiB)
[2025-07-11T11:47:04.483+0200] {subprocess.py:93} INFO - 25/07/11 11:47:04 INFO BlockManagerInfo: Added broadcast_18_piece0 in memory on 138.4.31.14:39113 (size: 36.5 KiB, free: 434.2 MiB)
[2025-07-11T11:47:04.484+0200] {subprocess.py:93} INFO - 25/07/11 11:47:04 INFO SparkContext: Created broadcast 18 from broadcast at DAGScheduler.scala:1585
[2025-07-11T11:47:04.484+0200] {subprocess.py:93} INFO - 25/07/11 11:47:04 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 23 (MapPartitionsRDD[42] at saveAsTextFile at ReadWrite.scala:413) (first 15 tasks are for partitions Vector(0))
[2025-07-11T11:47:04.484+0200] {subprocess.py:93} INFO - 25/07/11 11:47:04 INFO TaskSchedulerImpl: Adding task set 23.0 with 1 tasks resource profile 0
[2025-07-11T11:47:04.489+0200] {subprocess.py:93} INFO - 25/07/11 11:47:04 INFO TaskSetManager: Starting task 0.0 in stage 23.0 (TID 40) (138.4.31.14, executor driver, partition 0, PROCESS_LOCAL, 13280 bytes)
[2025-07-11T11:47:04.499+0200] {subprocess.py:93} INFO - 25/07/11 11:47:04 INFO Executor: Running task 0.0 in stage 23.0 (TID 40)
[2025-07-11T11:47:04.501+0200] {subprocess.py:93} INFO - 25/07/11 11:47:04 INFO HadoopMapRedCommitProtocol: Using output committer class org.apache.hadoop.mapred.FileOutputCommitter
[2025-07-11T11:47:04.501+0200] {subprocess.py:93} INFO - 25/07/11 11:47:04 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2025-07-11T11:47:04.501+0200] {subprocess.py:93} INFO - 25/07/11 11:47:04 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2025-07-11T11:47:04.543+0200] {subprocess.py:93} INFO - 25/07/11 11:47:04 INFO FileOutputCommitter: Saved output of task 'attempt_202507111147043644007612732849055_0042_m_000000_0' to file:/home/monica.fernandez/practica_creativa/models/string_indexer_model_Origin.bin/metadata/_temporary/0/task_202507111147043644007612732849055_0042_m_000000
[2025-07-11T11:47:04.543+0200] {subprocess.py:93} INFO - 25/07/11 11:47:04 INFO SparkHadoopMapRedUtil: attempt_202507111147043644007612732849055_0042_m_000000_0: Committed. Elapsed time: 1 ms.
[2025-07-11T11:47:04.544+0200] {subprocess.py:93} INFO - 25/07/11 11:47:04 INFO Executor: Finished task 0.0 in stage 23.0 (TID 40). 1170 bytes result sent to driver
[2025-07-11T11:47:04.544+0200] {subprocess.py:93} INFO - 25/07/11 11:47:04 INFO TaskSetManager: Finished task 0.0 in stage 23.0 (TID 40) in 52 ms on 138.4.31.14 (executor driver) (1/1)
[2025-07-11T11:47:04.544+0200] {subprocess.py:93} INFO - 25/07/11 11:47:04 INFO TaskSchedulerImpl: Removed TaskSet 23.0, whose tasks have all completed, from pool
[2025-07-11T11:47:04.544+0200] {subprocess.py:93} INFO - 25/07/11 11:47:04 INFO DAGScheduler: ResultStage 23 (runJob at SparkHadoopWriter.scala:83) finished in 0,070 s
[2025-07-11T11:47:04.544+0200] {subprocess.py:93} INFO - 25/07/11 11:47:04 INFO DAGScheduler: Job 14 is finished. Cancelling potential speculative or zombie tasks for this job
[2025-07-11T11:47:04.544+0200] {subprocess.py:93} INFO - 25/07/11 11:47:04 INFO TaskSchedulerImpl: Killing all running tasks in stage 23: Stage finished
[2025-07-11T11:47:04.544+0200] {subprocess.py:93} INFO - 25/07/11 11:47:04 INFO DAGScheduler: Job 14 finished: runJob at SparkHadoopWriter.scala:83, took 0,072542 s
[2025-07-11T11:47:04.544+0200] {subprocess.py:93} INFO - 25/07/11 11:47:04 INFO SparkHadoopWriter: Start to commit write Job job_202507111147043644007612732849055_0042.
[2025-07-11T11:47:04.576+0200] {subprocess.py:93} INFO - 25/07/11 11:47:04 INFO SparkHadoopWriter: Write Job job_202507111147043644007612732849055_0042 committed. Elapsed time: 33 ms.
[2025-07-11T11:47:04.641+0200] {subprocess.py:93} INFO - 25/07/11 11:47:04 INFO DAGScheduler: Registering RDD 45 (parquet at StringIndexer.scala:499) as input to shuffle 7
[2025-07-11T11:47:04.641+0200] {subprocess.py:93} INFO - 25/07/11 11:47:04 INFO DAGScheduler: Got map stage job 15 (parquet at StringIndexer.scala:499) with 1 output partitions
[2025-07-11T11:47:04.641+0200] {subprocess.py:93} INFO - 25/07/11 11:47:04 INFO DAGScheduler: Final stage: ShuffleMapStage 24 (parquet at StringIndexer.scala:499)
[2025-07-11T11:47:04.642+0200] {subprocess.py:93} INFO - 25/07/11 11:47:04 INFO DAGScheduler: Parents of final stage: List()
[2025-07-11T11:47:04.642+0200] {subprocess.py:93} INFO - 25/07/11 11:47:04 INFO DAGScheduler: Missing parents: List()
[2025-07-11T11:47:04.642+0200] {subprocess.py:93} INFO - 25/07/11 11:47:04 INFO DAGScheduler: Submitting ShuffleMapStage 24 (MapPartitionsRDD[45] at parquet at StringIndexer.scala:499), which has no missing parents
[2025-07-11T11:47:04.642+0200] {subprocess.py:93} INFO - 25/07/11 11:47:04 INFO MemoryStore: Block broadcast_19 stored as values in memory (estimated size 8.1 KiB, free 429.6 MiB)
[2025-07-11T11:47:04.643+0200] {subprocess.py:93} INFO - 25/07/11 11:47:04 INFO MemoryStore: Block broadcast_19_piece0 stored as bytes in memory (estimated size 4.4 KiB, free 429.6 MiB)
[2025-07-11T11:47:04.643+0200] {subprocess.py:93} INFO - 25/07/11 11:47:04 INFO BlockManagerInfo: Added broadcast_19_piece0 in memory on 138.4.31.14:39113 (size: 4.4 KiB, free: 434.2 MiB)
[2025-07-11T11:47:04.644+0200] {subprocess.py:93} INFO - 25/07/11 11:47:04 INFO SparkContext: Created broadcast 19 from broadcast at DAGScheduler.scala:1585
[2025-07-11T11:47:04.644+0200] {subprocess.py:93} INFO - 25/07/11 11:47:04 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 24 (MapPartitionsRDD[45] at parquet at StringIndexer.scala:499) (first 15 tasks are for partitions Vector(0))
[2025-07-11T11:47:04.644+0200] {subprocess.py:93} INFO - 25/07/11 11:47:04 INFO TaskSchedulerImpl: Adding task set 24.0 with 1 tasks resource profile 0
[2025-07-11T11:47:04.647+0200] {subprocess.py:93} INFO - 25/07/11 11:47:04 INFO TaskSetManager: Starting task 0.0 in stage 24.0 (TID 41) (138.4.31.14, executor driver, partition 0, PROCESS_LOCAL, 18200 bytes)
[2025-07-11T11:47:04.648+0200] {subprocess.py:93} INFO - 25/07/11 11:47:04 INFO Executor: Running task 0.0 in stage 24.0 (TID 41)
[2025-07-11T11:47:04.651+0200] {subprocess.py:93} INFO - 25/07/11 11:47:04 INFO Executor: Finished task 0.0 in stage 24.0 (TID 41). 1628 bytes result sent to driver
[2025-07-11T11:47:04.652+0200] {subprocess.py:93} INFO - 25/07/11 11:47:04 INFO TaskSetManager: Finished task 0.0 in stage 24.0 (TID 41) in 6 ms on 138.4.31.14 (executor driver) (1/1)
[2025-07-11T11:47:04.652+0200] {subprocess.py:93} INFO - 25/07/11 11:47:04 INFO TaskSchedulerImpl: Removed TaskSet 24.0, whose tasks have all completed, from pool
[2025-07-11T11:47:04.660+0200] {subprocess.py:93} INFO - 25/07/11 11:47:04 INFO DAGScheduler: ShuffleMapStage 24 (parquet at StringIndexer.scala:499) finished in 0,011 s
[2025-07-11T11:47:04.660+0200] {subprocess.py:93} INFO - 25/07/11 11:47:04 INFO DAGScheduler: looking for newly runnable stages
[2025-07-11T11:47:04.660+0200] {subprocess.py:93} INFO - 25/07/11 11:47:04 INFO DAGScheduler: running: Set()
[2025-07-11T11:47:04.660+0200] {subprocess.py:93} INFO - 25/07/11 11:47:04 INFO DAGScheduler: waiting: Set()
[2025-07-11T11:47:04.660+0200] {subprocess.py:93} INFO - 25/07/11 11:47:04 INFO DAGScheduler: failed: Set()
[2025-07-11T11:47:04.660+0200] {subprocess.py:93} INFO - 25/07/11 11:47:04 INFO ParquetUtils: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
[2025-07-11T11:47:04.661+0200] {subprocess.py:93} INFO - 25/07/11 11:47:04 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2025-07-11T11:47:04.661+0200] {subprocess.py:93} INFO - 25/07/11 11:47:04 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2025-07-11T11:47:04.661+0200] {subprocess.py:93} INFO - 25/07/11 11:47:04 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[2025-07-11T11:47:04.662+0200] {subprocess.py:93} INFO - 25/07/11 11:47:04 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2025-07-11T11:47:04.662+0200] {subprocess.py:93} INFO - 25/07/11 11:47:04 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2025-07-11T11:47:04.662+0200] {subprocess.py:93} INFO - 25/07/11 11:47:04 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[2025-07-11T11:47:04.679+0200] {subprocess.py:93} INFO - 25/07/11 11:47:04 INFO SparkContext: Starting job: parquet at StringIndexer.scala:499
[2025-07-11T11:47:04.681+0200] {subprocess.py:93} INFO - 25/07/11 11:47:04 INFO DAGScheduler: Got job 16 (parquet at StringIndexer.scala:499) with 1 output partitions
[2025-07-11T11:47:04.681+0200] {subprocess.py:93} INFO - 25/07/11 11:47:04 INFO DAGScheduler: Final stage: ResultStage 26 (parquet at StringIndexer.scala:499)
[2025-07-11T11:47:04.681+0200] {subprocess.py:93} INFO - 25/07/11 11:47:04 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 25)
[2025-07-11T11:47:04.681+0200] {subprocess.py:93} INFO - 25/07/11 11:47:04 INFO DAGScheduler: Missing parents: List()
[2025-07-11T11:47:04.681+0200] {subprocess.py:93} INFO - 25/07/11 11:47:04 INFO DAGScheduler: Submitting ResultStage 26 (MapPartitionsRDD[47] at parquet at StringIndexer.scala:499), which has no missing parents
[2025-07-11T11:47:04.697+0200] {subprocess.py:93} INFO - 25/07/11 11:47:04 INFO MemoryStore: Block broadcast_20 stored as values in memory (estimated size 230.2 KiB, free 429.3 MiB)
[2025-07-11T11:47:04.699+0200] {subprocess.py:93} INFO - 25/07/11 11:47:04 INFO MemoryStore: Block broadcast_20_piece0 stored as bytes in memory (estimated size 81.0 KiB, free 429.3 MiB)
[2025-07-11T11:47:04.699+0200] {subprocess.py:93} INFO - 25/07/11 11:47:04 INFO BlockManagerInfo: Added broadcast_20_piece0 in memory on 138.4.31.14:39113 (size: 81.0 KiB, free: 434.1 MiB)
[2025-07-11T11:47:04.700+0200] {subprocess.py:93} INFO - 25/07/11 11:47:04 INFO SparkContext: Created broadcast 20 from broadcast at DAGScheduler.scala:1585
[2025-07-11T11:47:04.700+0200] {subprocess.py:93} INFO - 25/07/11 11:47:04 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 26 (MapPartitionsRDD[47] at parquet at StringIndexer.scala:499) (first 15 tasks are for partitions Vector(0))
[2025-07-11T11:47:04.702+0200] {subprocess.py:93} INFO - 25/07/11 11:47:04 INFO TaskSchedulerImpl: Adding task set 26.0 with 1 tasks resource profile 0
[2025-07-11T11:47:04.703+0200] {subprocess.py:93} INFO - 25/07/11 11:47:04 INFO TaskSetManager: Starting task 0.0 in stage 26.0 (TID 42) (138.4.31.14, executor driver, partition 0, NODE_LOCAL, 12952 bytes)
[2025-07-11T11:47:04.703+0200] {subprocess.py:93} INFO - 25/07/11 11:47:04 INFO Executor: Running task 0.0 in stage 26.0 (TID 42)
[2025-07-11T11:47:04.715+0200] {subprocess.py:93} INFO - 25/07/11 11:47:04 INFO ShuffleBlockFetcherIterator: Getting 1 (2.9 KiB) non-empty blocks including 1 (2.9 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2025-07-11T11:47:04.715+0200] {subprocess.py:93} INFO - 25/07/11 11:47:04 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2025-07-11T11:47:04.716+0200] {subprocess.py:93} INFO - 25/07/11 11:47:04 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2025-07-11T11:47:04.716+0200] {subprocess.py:93} INFO - 25/07/11 11:47:04 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2025-07-11T11:47:04.717+0200] {subprocess.py:93} INFO - 25/07/11 11:47:04 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[2025-07-11T11:47:04.717+0200] {subprocess.py:93} INFO - 25/07/11 11:47:04 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2025-07-11T11:47:04.717+0200] {subprocess.py:93} INFO - 25/07/11 11:47:04 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2025-07-11T11:47:04.717+0200] {subprocess.py:93} INFO - 25/07/11 11:47:04 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[2025-07-11T11:47:04.717+0200] {subprocess.py:93} INFO - 25/07/11 11:47:04 INFO CodecConfig: Compression: SNAPPY
[2025-07-11T11:47:04.718+0200] {subprocess.py:93} INFO - 25/07/11 11:47:04 INFO CodecConfig: Compression: SNAPPY
[2025-07-11T11:47:04.719+0200] {subprocess.py:93} INFO - 25/07/11 11:47:04 INFO ParquetOutputFormat: ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]
[2025-07-11T11:47:04.720+0200] {subprocess.py:93} INFO - 25/07/11 11:47:04 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
[2025-07-11T11:47:04.720+0200] {subprocess.py:93} INFO - {
[2025-07-11T11:47:04.720+0200] {subprocess.py:93} INFO -   "type" : "struct",
[2025-07-11T11:47:04.720+0200] {subprocess.py:93} INFO -   "fields" : [ {
[2025-07-11T11:47:04.720+0200] {subprocess.py:93} INFO -     "name" : "labelsArray",
[2025-07-11T11:47:04.720+0200] {subprocess.py:93} INFO -     "type" : {
[2025-07-11T11:47:04.720+0200] {subprocess.py:93} INFO -       "type" : "array",
[2025-07-11T11:47:04.720+0200] {subprocess.py:93} INFO -       "elementType" : {
[2025-07-11T11:47:04.720+0200] {subprocess.py:93} INFO -         "type" : "array",
[2025-07-11T11:47:04.721+0200] {subprocess.py:93} INFO -         "elementType" : "string",
[2025-07-11T11:47:04.721+0200] {subprocess.py:93} INFO -         "containsNull" : true
[2025-07-11T11:47:04.721+0200] {subprocess.py:93} INFO -       },
[2025-07-11T11:47:04.721+0200] {subprocess.py:93} INFO -       "containsNull" : true
[2025-07-11T11:47:04.721+0200] {subprocess.py:93} INFO -     },
[2025-07-11T11:47:04.721+0200] {subprocess.py:93} INFO -     "nullable" : true,
[2025-07-11T11:47:04.721+0200] {subprocess.py:93} INFO -     "metadata" : { }
[2025-07-11T11:47:04.721+0200] {subprocess.py:93} INFO -   } ]
[2025-07-11T11:47:04.721+0200] {subprocess.py:93} INFO - }
[2025-07-11T11:47:04.721+0200] {subprocess.py:93} INFO - and corresponding Parquet message type:
[2025-07-11T11:47:04.721+0200] {subprocess.py:93} INFO - message spark_schema {
[2025-07-11T11:47:04.721+0200] {subprocess.py:93} INFO -   optional group labelsArray (LIST) {
[2025-07-11T11:47:04.721+0200] {subprocess.py:93} INFO -     repeated group list {
[2025-07-11T11:47:04.721+0200] {subprocess.py:93} INFO -       optional group element (LIST) {
[2025-07-11T11:47:04.721+0200] {subprocess.py:93} INFO -         repeated group list {
[2025-07-11T11:47:04.721+0200] {subprocess.py:93} INFO -           optional binary element (STRING);
[2025-07-11T11:47:04.721+0200] {subprocess.py:93} INFO -         }
[2025-07-11T11:47:04.721+0200] {subprocess.py:93} INFO -       }
[2025-07-11T11:47:04.722+0200] {subprocess.py:93} INFO -     }
[2025-07-11T11:47:04.722+0200] {subprocess.py:93} INFO -   }
[2025-07-11T11:47:04.722+0200] {subprocess.py:93} INFO - }
[2025-07-11T11:47:04.722+0200] {subprocess.py:93} INFO - 
[2025-07-11T11:47:04.722+0200] {subprocess.py:93} INFO - 
[2025-07-11T11:47:04.752+0200] {subprocess.py:93} INFO - 25/07/11 11:47:04 INFO FileOutputCommitter: Saved output of task 'attempt_202507111147047002661363997543923_0026_m_000000_42' to file:/home/monica.fernandez/practica_creativa/models/string_indexer_model_Origin.bin/data/_temporary/0/task_202507111147047002661363997543923_0026_m_000000
[2025-07-11T11:47:04.752+0200] {subprocess.py:93} INFO - 25/07/11 11:47:04 INFO SparkHadoopMapRedUtil: attempt_202507111147047002661363997543923_0026_m_000000_42: Committed. Elapsed time: 1 ms.
[2025-07-11T11:47:04.753+0200] {subprocess.py:93} INFO - 25/07/11 11:47:04 INFO Executor: Finished task 0.0 in stage 26.0 (TID 42). 4740 bytes result sent to driver
[2025-07-11T11:47:04.754+0200] {subprocess.py:93} INFO - 25/07/11 11:47:04 INFO TaskSetManager: Finished task 0.0 in stage 26.0 (TID 42) in 51 ms on 138.4.31.14 (executor driver) (1/1)
[2025-07-11T11:47:04.755+0200] {subprocess.py:93} INFO - 25/07/11 11:47:04 INFO TaskSchedulerImpl: Removed TaskSet 26.0, whose tasks have all completed, from pool
[2025-07-11T11:47:04.755+0200] {subprocess.py:93} INFO - 25/07/11 11:47:04 INFO DAGScheduler: ResultStage 26 (parquet at StringIndexer.scala:499) finished in 0,073 s
[2025-07-11T11:47:04.755+0200] {subprocess.py:93} INFO - 25/07/11 11:47:04 INFO DAGScheduler: Job 16 is finished. Cancelling potential speculative or zombie tasks for this job
[2025-07-11T11:47:04.755+0200] {subprocess.py:93} INFO - 25/07/11 11:47:04 INFO TaskSchedulerImpl: Killing all running tasks in stage 26: Stage finished
[2025-07-11T11:47:04.755+0200] {subprocess.py:93} INFO - 25/07/11 11:47:04 INFO DAGScheduler: Job 16 finished: parquet at StringIndexer.scala:499, took 0,074915 s
[2025-07-11T11:47:04.756+0200] {subprocess.py:93} INFO - 25/07/11 11:47:04 INFO FileFormatWriter: Start to commit write Job ac5ffd73-a96a-4aab-ab2f-c5b30d1932c0.
[2025-07-11T11:47:04.777+0200] {subprocess.py:93} INFO - 25/07/11 11:47:04 INFO FileFormatWriter: Write Job ac5ffd73-a96a-4aab-ab2f-c5b30d1932c0 committed. Elapsed time: 20 ms.
[2025-07-11T11:47:04.777+0200] {subprocess.py:93} INFO - 25/07/11 11:47:04 INFO FileFormatWriter: Finished processing stats for write job ac5ffd73-a96a-4aab-ab2f-c5b30d1932c0.
[2025-07-11T11:47:04.837+0200] {subprocess.py:93} INFO - 25/07/11 11:47:04 INFO V2ScanRelationPushDown:
[2025-07-11T11:47:04.837+0200] {subprocess.py:93} INFO - Pushing operators to origin_dest_distances
[2025-07-11T11:47:04.837+0200] {subprocess.py:93} INFO - Pushed Filters:
[2025-07-11T11:47:04.837+0200] {subprocess.py:93} INFO - Post-Scan Filters:
[2025-07-11T11:47:04.837+0200] {subprocess.py:93} INFO - 
[2025-07-11T11:47:04.845+0200] {subprocess.py:93} INFO - 25/07/11 11:47:04 INFO V2ScanRelationPushDown:
[2025-07-11T11:47:04.845+0200] {subprocess.py:93} INFO - Output: origin#42, dest#43
[2025-07-11T11:47:04.846+0200] {subprocess.py:93} INFO - 
[2025-07-11T11:47:04.868+0200] {subprocess.py:93} INFO - 25/07/11 11:47:04 INFO FileSourceStrategy: Pushed Filters:
[2025-07-11T11:47:04.868+0200] {subprocess.py:93} INFO - 25/07/11 11:47:04 INFO FileSourceStrategy: Post-Scan Filters:
[2025-07-11T11:47:04.896+0200] {subprocess.py:93} INFO - 25/07/11 11:47:04 INFO MemoryStore: Block broadcast_21 stored as values in memory (estimated size 209.9 KiB, free 429.0 MiB)
[2025-07-11T11:47:04.904+0200] {subprocess.py:93} INFO - 25/07/11 11:47:04 INFO BlockManagerInfo: Removed broadcast_17_piece0 on 138.4.31.14:39113 in memory (size: 23.3 KiB, free: 434.1 MiB)
[2025-07-11T11:47:04.911+0200] {subprocess.py:93} INFO - 25/07/11 11:47:04 INFO MemoryStore: Block broadcast_21_piece0 stored as bytes in memory (estimated size 37.7 KiB, free 429.1 MiB)
[2025-07-11T11:47:04.912+0200] {subprocess.py:93} INFO - 25/07/11 11:47:04 INFO BlockManagerInfo: Added broadcast_21_piece0 in memory on 138.4.31.14:39113 (size: 37.7 KiB, free: 434.1 MiB)
[2025-07-11T11:47:04.914+0200] {subprocess.py:93} INFO - 25/07/11 11:47:04 INFO BlockManagerInfo: Removed broadcast_18_piece0 on 138.4.31.14:39113 in memory (size: 36.5 KiB, free: 434.1 MiB)
[2025-07-11T11:47:04.915+0200] {subprocess.py:93} INFO - 25/07/11 11:47:04 INFO SparkContext: Created broadcast 21 from collect at StringIndexer.scala:204
[2025-07-11T11:47:04.916+0200] {subprocess.py:93} INFO - 25/07/11 11:47:04 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
[2025-07-11T11:47:04.920+0200] {subprocess.py:93} INFO - 25/07/11 11:47:04 INFO DAGScheduler: Registering RDD 50 (collect at StringIndexer.scala:204) as input to shuffle 8
[2025-07-11T11:47:04.920+0200] {subprocess.py:93} INFO - 25/07/11 11:47:04 INFO DAGScheduler: Got map stage job 17 (collect at StringIndexer.scala:204) with 2 output partitions
[2025-07-11T11:47:04.920+0200] {subprocess.py:93} INFO - 25/07/11 11:47:04 INFO DAGScheduler: Final stage: ShuffleMapStage 27 (collect at StringIndexer.scala:204)
[2025-07-11T11:47:04.920+0200] {subprocess.py:93} INFO - 25/07/11 11:47:04 INFO DAGScheduler: Parents of final stage: List()
[2025-07-11T11:47:04.927+0200] {subprocess.py:93} INFO - 25/07/11 11:47:04 INFO DAGScheduler: Missing parents: List()
[2025-07-11T11:47:04.927+0200] {subprocess.py:93} INFO - 25/07/11 11:47:04 INFO DAGScheduler: Submitting ShuffleMapStage 27 (MapPartitionsRDD[50] at collect at StringIndexer.scala:204), which has no missing parents
[2025-07-11T11:47:04.927+0200] {subprocess.py:93} INFO - 25/07/11 11:47:04 INFO MemoryStore: Block broadcast_22 stored as values in memory (estimated size 13.7 KiB, free 429.2 MiB)
[2025-07-11T11:47:04.927+0200] {subprocess.py:93} INFO - 25/07/11 11:47:04 INFO MemoryStore: Block broadcast_22_piece0 stored as bytes in memory (estimated size 7.0 KiB, free 429.2 MiB)
[2025-07-11T11:47:04.927+0200] {subprocess.py:93} INFO - 25/07/11 11:47:04 INFO BlockManagerInfo: Added broadcast_22_piece0 in memory on 138.4.31.14:39113 (size: 7.0 KiB, free: 434.1 MiB)
[2025-07-11T11:47:04.930+0200] {subprocess.py:93} INFO - 25/07/11 11:47:04 INFO SparkContext: Created broadcast 22 from broadcast at DAGScheduler.scala:1585
[2025-07-11T11:47:04.930+0200] {subprocess.py:93} INFO - 25/07/11 11:47:04 INFO DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 27 (MapPartitionsRDD[50] at collect at StringIndexer.scala:204) (first 15 tasks are for partitions Vector(0, 1))
[2025-07-11T11:47:04.930+0200] {subprocess.py:93} INFO - 25/07/11 11:47:04 INFO TaskSchedulerImpl: Adding task set 27.0 with 2 tasks resource profile 0
[2025-07-11T11:47:04.931+0200] {subprocess.py:93} INFO - 25/07/11 11:47:04 INFO DAGScheduler: Registering RDD 54 (collect at StringIndexer.scala:204) as input to shuffle 9
[2025-07-11T11:47:04.931+0200] {subprocess.py:93} INFO - 25/07/11 11:47:04 INFO DAGScheduler: Got map stage job 18 (collect at StringIndexer.scala:204) with 13 output partitions
[2025-07-11T11:47:04.931+0200] {subprocess.py:93} INFO - 25/07/11 11:47:04 INFO DAGScheduler: Final stage: ShuffleMapStage 28 (collect at StringIndexer.scala:204)
[2025-07-11T11:47:04.931+0200] {subprocess.py:93} INFO - 25/07/11 11:47:04 INFO DAGScheduler: Parents of final stage: List()
[2025-07-11T11:47:04.931+0200] {subprocess.py:93} INFO - 25/07/11 11:47:04 INFO DAGScheduler: Missing parents: List()
[2025-07-11T11:47:04.931+0200] {subprocess.py:93} INFO - 25/07/11 11:47:04 INFO TaskSetManager: Starting task 0.0 in stage 27.0 (TID 43) (138.4.31.14, executor driver, partition 0, PROCESS_LOCAL, 13595 bytes)
[2025-07-11T11:47:04.931+0200] {subprocess.py:93} INFO - 25/07/11 11:47:04 INFO TaskSetManager: Starting task 1.0 in stage 27.0 (TID 44) (138.4.31.14, executor driver, partition 1, PROCESS_LOCAL, 13595 bytes)
[2025-07-11T11:47:04.931+0200] {subprocess.py:93} INFO - 25/07/11 11:47:04 INFO Executor: Running task 1.0 in stage 27.0 (TID 44)
[2025-07-11T11:47:04.931+0200] {subprocess.py:93} INFO - 25/07/11 11:47:04 INFO Executor: Running task 0.0 in stage 27.0 (TID 43)
[2025-07-11T11:47:04.934+0200] {subprocess.py:93} INFO - 25/07/11 11:47:04 INFO DAGScheduler: Submitting ShuffleMapStage 28 (MapPartitionsRDD[54] at collect at StringIndexer.scala:204), which has no missing parents
[2025-07-11T11:47:04.938+0200] {subprocess.py:93} INFO - 25/07/11 11:47:04 INFO MemoryStore: Block broadcast_23 stored as values in memory (estimated size 18.2 KiB, free 429.2 MiB)
[2025-07-11T11:47:04.938+0200] {subprocess.py:93} INFO - 25/07/11 11:47:04 INFO MemoryStore: Block broadcast_23_piece0 stored as bytes in memory (estimated size 9.0 KiB, free 429.2 MiB)
[2025-07-11T11:47:04.938+0200] {subprocess.py:93} INFO - 25/07/11 11:47:04 INFO FileScanRDD: Reading File path: file:///home/monica.fernandez/practica_creativa/data/simple_flight_delay_features.jsonl.bz2, range: 4194304-4676447, partition values: [empty row]
[2025-07-11T11:47:04.938+0200] {subprocess.py:93} INFO - 25/07/11 11:47:04 INFO BlockManagerInfo: Added broadcast_23_piece0 in memory on 138.4.31.14:39113 (size: 9.0 KiB, free: 434.1 MiB)
[2025-07-11T11:47:04.938+0200] {subprocess.py:93} INFO - 25/07/11 11:47:04 INFO FileScanRDD: Reading File path: file:///home/monica.fernandez/practica_creativa/data/simple_flight_delay_features.jsonl.bz2, range: 0-4194304, partition values: [empty row]
[2025-07-11T11:47:04.939+0200] {subprocess.py:93} INFO - 25/07/11 11:47:04 INFO SparkContext: Created broadcast 23 from broadcast at DAGScheduler.scala:1585
[2025-07-11T11:47:04.939+0200] {subprocess.py:93} INFO - 25/07/11 11:47:04 INFO DAGScheduler: Submitting 13 missing tasks from ShuffleMapStage 28 (MapPartitionsRDD[54] at collect at StringIndexer.scala:204) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12))
[2025-07-11T11:47:04.940+0200] {subprocess.py:93} INFO - 25/07/11 11:47:04 INFO TaskSchedulerImpl: Adding task set 28.0 with 13 tasks resource profile 0
[2025-07-11T11:47:04.941+0200] {subprocess.py:93} INFO - 25/07/11 11:47:04 INFO BlockManagerInfo: Removed broadcast_20_piece0 on 138.4.31.14:39113 in memory (size: 81.0 KiB, free: 434.2 MiB)
[2025-07-11T11:47:04.943+0200] {subprocess.py:93} INFO - 25/07/11 11:47:04 INFO TaskSetManager: Starting task 0.0 in stage 28.0 (TID 45) (138.4.31.14, executor driver, partition 0, ANY, 14843 bytes)
[2025-07-11T11:47:04.943+0200] {subprocess.py:93} INFO - 25/07/11 11:47:04 INFO TaskSetManager: Starting task 1.0 in stage 28.0 (TID 46) (138.4.31.14, executor driver, partition 1, ANY, 14843 bytes)
[2025-07-11T11:47:04.943+0200] {subprocess.py:93} INFO - 25/07/11 11:47:04 INFO Executor: Running task 0.0 in stage 28.0 (TID 45)
[2025-07-11T11:47:04.943+0200] {subprocess.py:93} INFO - 25/07/11 11:47:04 INFO Executor: Running task 1.0 in stage 28.0 (TID 46)
[2025-07-11T11:47:04.989+0200] {subprocess.py:93} INFO - 25/07/11 11:47:04 INFO BlockManagerInfo: Removed broadcast_19_piece0 on 138.4.31.14:39113 in memory (size: 4.4 KiB, free: 434.2 MiB)
[2025-07-11T11:47:05.028+0200] {subprocess.py:93} INFO - 25/07/11 11:47:05 INFO Executor: Finished task 1.0 in stage 28.0 (TID 46). 2031 bytes result sent to driver
[2025-07-11T11:47:05.030+0200] {subprocess.py:93} INFO - 25/07/11 11:47:05 INFO TaskSetManager: Starting task 2.0 in stage 28.0 (TID 47) (138.4.31.14, executor driver, partition 2, ANY, 14843 bytes)
[2025-07-11T11:47:05.031+0200] {subprocess.py:93} INFO - 25/07/11 11:47:05 INFO Executor: Running task 2.0 in stage 28.0 (TID 47)
[2025-07-11T11:47:05.031+0200] {subprocess.py:93} INFO - 25/07/11 11:47:05 INFO TaskSetManager: Finished task 1.0 in stage 28.0 (TID 46) in 88 ms on 138.4.31.14 (executor driver) (1/13)
[2025-07-11T11:47:05.049+0200] {subprocess.py:93} INFO - 25/07/11 11:47:05 INFO Executor: Finished task 0.0 in stage 28.0 (TID 45). 1988 bytes result sent to driver
[2025-07-11T11:47:05.050+0200] {subprocess.py:93} INFO - 25/07/11 11:47:05 INFO TaskSetManager: Starting task 3.0 in stage 28.0 (TID 48) (138.4.31.14, executor driver, partition 3, ANY, 14843 bytes)
[2025-07-11T11:47:05.051+0200] {subprocess.py:93} INFO - 25/07/11 11:47:05 INFO TaskSetManager: Finished task 0.0 in stage 28.0 (TID 45) in 108 ms on 138.4.31.14 (executor driver) (2/13)
[2025-07-11T11:47:05.052+0200] {subprocess.py:93} INFO - 25/07/11 11:47:05 INFO Executor: Running task 3.0 in stage 28.0 (TID 48)
[2025-07-11T11:47:05.121+0200] {subprocess.py:93} INFO - 25/07/11 11:47:05 INFO Executor: Finished task 3.0 in stage 28.0 (TID 48). 1945 bytes result sent to driver
[2025-07-11T11:47:05.123+0200] {subprocess.py:93} INFO - 25/07/11 11:47:05 INFO TaskSetManager: Starting task 4.0 in stage 28.0 (TID 49) (138.4.31.14, executor driver, partition 4, ANY, 14843 bytes)
[2025-07-11T11:47:05.123+0200] {subprocess.py:93} INFO - 25/07/11 11:47:05 INFO Executor: Running task 4.0 in stage 28.0 (TID 49)
[2025-07-11T11:47:05.123+0200] {subprocess.py:93} INFO - 25/07/11 11:47:05 INFO TaskSetManager: Finished task 3.0 in stage 28.0 (TID 48) in 74 ms on 138.4.31.14 (executor driver) (3/13)
[2025-07-11T11:47:05.153+0200] {subprocess.py:93} INFO - 25/07/11 11:47:05 INFO Executor: Finished task 2.0 in stage 28.0 (TID 47). 1945 bytes result sent to driver
[2025-07-11T11:47:05.157+0200] {subprocess.py:93} INFO - 25/07/11 11:47:05 INFO TaskSetManager: Starting task 5.0 in stage 28.0 (TID 50) (138.4.31.14, executor driver, partition 5, ANY, 14843 bytes)
[2025-07-11T11:47:05.157+0200] {subprocess.py:93} INFO - 25/07/11 11:47:05 INFO Executor: Running task 5.0 in stage 28.0 (TID 50)
[2025-07-11T11:47:05.157+0200] {subprocess.py:93} INFO - 25/07/11 11:47:05 INFO TaskSetManager: Finished task 2.0 in stage 28.0 (TID 47) in 128 ms on 138.4.31.14 (executor driver) (4/13)
[2025-07-11T11:47:05.239+0200] {subprocess.py:93} INFO - 25/07/11 11:47:05 INFO Executor: Finished task 4.0 in stage 28.0 (TID 49). 1945 bytes result sent to driver
[2025-07-11T11:47:05.240+0200] {subprocess.py:93} INFO - 25/07/11 11:47:05 INFO TaskSetManager: Starting task 6.0 in stage 28.0 (TID 51) (138.4.31.14, executor driver, partition 6, ANY, 14959 bytes)
[2025-07-11T11:47:05.242+0200] {subprocess.py:93} INFO - 25/07/11 11:47:05 INFO TaskSetManager: Finished task 4.0 in stage 28.0 (TID 49) in 119 ms on 138.4.31.14 (executor driver) (5/13)
[2025-07-11T11:47:05.261+0200] {subprocess.py:93} INFO - 25/07/11 11:47:05 INFO Executor: Running task 6.0 in stage 28.0 (TID 51)
[2025-07-11T11:47:05.264+0200] {subprocess.py:93} INFO - 25/07/11 11:47:05 INFO Executor: Finished task 5.0 in stage 28.0 (TID 50). 1945 bytes result sent to driver
[2025-07-11T11:47:05.264+0200] {subprocess.py:93} INFO - 25/07/11 11:47:05 INFO TaskSetManager: Starting task 7.0 in stage 28.0 (TID 52) (138.4.31.14, executor driver, partition 7, ANY, 14843 bytes)
[2025-07-11T11:47:05.264+0200] {subprocess.py:93} INFO - 25/07/11 11:47:05 INFO TaskSetManager: Finished task 5.0 in stage 28.0 (TID 50) in 106 ms on 138.4.31.14 (executor driver) (6/13)
[2025-07-11T11:47:05.264+0200] {subprocess.py:93} INFO - 25/07/11 11:47:05 INFO Executor: Running task 7.0 in stage 28.0 (TID 52)
[2025-07-11T11:47:05.343+0200] {subprocess.py:93} INFO - 25/07/11 11:47:05 INFO BlockManagerInfo: Removed broadcast_12_piece0 on 138.4.31.14:39113 in memory (size: 7.0 KiB, free: 434.2 MiB)
[2025-07-11T11:47:05.452+0200] {subprocess.py:93} INFO - 25/07/11 11:47:05 INFO BlockManagerInfo: Removed broadcast_13_piece0 on 138.4.31.14:39113 in memory (size: 9.0 KiB, free: 434.2 MiB)
[2025-07-11T11:47:05.462+0200] {subprocess.py:93} INFO - 25/07/11 11:47:05 INFO Executor: Finished task 7.0 in stage 28.0 (TID 52). 1945 bytes result sent to driver
[2025-07-11T11:47:05.464+0200] {subprocess.py:93} INFO - 25/07/11 11:47:05 INFO TaskSetManager: Starting task 8.0 in stage 28.0 (TID 53) (138.4.31.14, executor driver, partition 8, ANY, 14963 bytes)
[2025-07-11T11:47:05.464+0200] {subprocess.py:93} INFO - 25/07/11 11:47:05 INFO TaskSetManager: Finished task 7.0 in stage 28.0 (TID 52) in 202 ms on 138.4.31.14 (executor driver) (7/13)
[2025-07-11T11:47:05.464+0200] {subprocess.py:93} INFO - 25/07/11 11:47:05 INFO Executor: Running task 8.0 in stage 28.0 (TID 53)
[2025-07-11T11:47:05.565+0200] {subprocess.py:93} INFO - 25/07/11 11:47:05 INFO Executor: Finished task 6.0 in stage 28.0 (TID 51). 1988 bytes result sent to driver
[2025-07-11T11:47:05.574+0200] {subprocess.py:93} INFO - 25/07/11 11:47:05 INFO TaskSetManager: Starting task 9.0 in stage 28.0 (TID 54) (138.4.31.14, executor driver, partition 9, ANY, 14843 bytes)
[2025-07-11T11:47:05.575+0200] {subprocess.py:93} INFO - 25/07/11 11:47:05 INFO TaskSetManager: Finished task 6.0 in stage 28.0 (TID 51) in 334 ms on 138.4.31.14 (executor driver) (8/13)
[2025-07-11T11:47:05.578+0200] {subprocess.py:93} INFO - 25/07/11 11:47:05 INFO Executor: Running task 9.0 in stage 28.0 (TID 54)
[2025-07-11T11:47:05.646+0200] {subprocess.py:93} INFO - 25/07/11 11:47:05 INFO Executor: Finished task 9.0 in stage 28.0 (TID 54). 1988 bytes result sent to driver
[2025-07-11T11:47:05.647+0200] {subprocess.py:93} INFO - 25/07/11 11:47:05 INFO TaskSetManager: Starting task 10.0 in stage 28.0 (TID 55) (138.4.31.14, executor driver, partition 10, ANY, 14843 bytes)
[2025-07-11T11:47:05.661+0200] {subprocess.py:93} INFO - 25/07/11 11:47:05 INFO Executor: Running task 10.0 in stage 28.0 (TID 55)
[2025-07-11T11:47:05.662+0200] {subprocess.py:93} INFO - 25/07/11 11:47:05 INFO Executor: Finished task 8.0 in stage 28.0 (TID 53). 1988 bytes result sent to driver
[2025-07-11T11:47:05.662+0200] {subprocess.py:93} INFO - 25/07/11 11:47:05 INFO TaskSetManager: Finished task 9.0 in stage 28.0 (TID 54) in 91 ms on 138.4.31.14 (executor driver) (9/13)
[2025-07-11T11:47:05.667+0200] {subprocess.py:93} INFO - 25/07/11 11:47:05 INFO TaskSetManager: Starting task 11.0 in stage 28.0 (TID 56) (138.4.31.14, executor driver, partition 11, ANY, 14963 bytes)
[2025-07-11T11:47:05.668+0200] {subprocess.py:93} INFO - 25/07/11 11:47:05 INFO TaskSetManager: Finished task 8.0 in stage 28.0 (TID 53) in 201 ms on 138.4.31.14 (executor driver) (10/13)
[2025-07-11T11:47:05.672+0200] {subprocess.py:93} INFO - 25/07/11 11:47:05 INFO Executor: Running task 11.0 in stage 28.0 (TID 56)
[2025-07-11T11:47:05.763+0200] {subprocess.py:93} INFO - 25/07/11 11:47:05 INFO Executor: Finished task 10.0 in stage 28.0 (TID 55). 1945 bytes result sent to driver
[2025-07-11T11:47:05.764+0200] {subprocess.py:93} INFO - 25/07/11 11:47:05 INFO TaskSetManager: Starting task 12.0 in stage 28.0 (TID 57) (138.4.31.14, executor driver, partition 12, ANY, 14843 bytes)
[2025-07-11T11:47:05.765+0200] {subprocess.py:93} INFO - 25/07/11 11:47:05 INFO Executor: Running task 12.0 in stage 28.0 (TID 57)
[2025-07-11T11:47:05.765+0200] {subprocess.py:93} INFO - 25/07/11 11:47:05 INFO TaskSetManager: Finished task 10.0 in stage 28.0 (TID 55) in 118 ms on 138.4.31.14 (executor driver) (11/13)
[2025-07-11T11:47:05.794+0200] {subprocess.py:93} INFO - 25/07/11 11:47:05 INFO Executor: Finished task 11.0 in stage 28.0 (TID 56). 1945 bytes result sent to driver
[2025-07-11T11:47:05.798+0200] {subprocess.py:93} INFO - 25/07/11 11:47:05 INFO TaskSetManager: Finished task 11.0 in stage 28.0 (TID 56) in 135 ms on 138.4.31.14 (executor driver) (12/13)
[2025-07-11T11:47:05.889+0200] {subprocess.py:93} INFO - 25/07/11 11:47:05 INFO Executor: Finished task 12.0 in stage 28.0 (TID 57). 1988 bytes result sent to driver
[2025-07-11T11:47:05.891+0200] {subprocess.py:93} INFO - 25/07/11 11:47:05 INFO TaskSetManager: Finished task 12.0 in stage 28.0 (TID 57) in 127 ms on 138.4.31.14 (executor driver) (13/13)
[2025-07-11T11:47:05.891+0200] {subprocess.py:93} INFO - 25/07/11 11:47:05 INFO TaskSchedulerImpl: Removed TaskSet 28.0, whose tasks have all completed, from pool
[2025-07-11T11:47:05.892+0200] {subprocess.py:93} INFO - 25/07/11 11:47:05 INFO DAGScheduler: ShuffleMapStage 28 (collect at StringIndexer.scala:204) finished in 0,954 s
[2025-07-11T11:47:05.892+0200] {subprocess.py:93} INFO - 25/07/11 11:47:05 INFO DAGScheduler: looking for newly runnable stages
[2025-07-11T11:47:05.892+0200] {subprocess.py:93} INFO - 25/07/11 11:47:05 INFO DAGScheduler: running: Set(ShuffleMapStage 27)
[2025-07-11T11:47:05.892+0200] {subprocess.py:93} INFO - 25/07/11 11:47:05 INFO DAGScheduler: waiting: Set()
[2025-07-11T11:47:05.892+0200] {subprocess.py:93} INFO - 25/07/11 11:47:05 INFO DAGScheduler: failed: Set()
[2025-07-11T11:47:05.927+0200] {subprocess.py:93} INFO - 25/07/11 11:47:05 INFO ShufflePartitionsUtil: For shuffle(9), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576
[2025-07-11T11:47:05.971+0200] {subprocess.py:93} INFO - 25/07/11 11:47:05 INFO SparkContext: Starting job: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264
[2025-07-11T11:47:05.976+0200] {subprocess.py:93} INFO - 25/07/11 11:47:05 INFO DAGScheduler: Got job 19 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) with 1 output partitions
[2025-07-11T11:47:05.976+0200] {subprocess.py:93} INFO - 25/07/11 11:47:05 INFO DAGScheduler: Final stage: ResultStage 30 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264)
[2025-07-11T11:47:05.976+0200] {subprocess.py:93} INFO - 25/07/11 11:47:05 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 29)
[2025-07-11T11:47:05.976+0200] {subprocess.py:93} INFO - 25/07/11 11:47:05 INFO DAGScheduler: Missing parents: List()
[2025-07-11T11:47:05.976+0200] {subprocess.py:93} INFO - 25/07/11 11:47:05 INFO DAGScheduler: Submitting ResultStage 30 (MapPartitionsRDD[56] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264), which has no missing parents
[2025-07-11T11:47:05.977+0200] {subprocess.py:93} INFO - 25/07/11 11:47:05 INFO MemoryStore: Block broadcast_24 stored as values in memory (estimated size 8.2 KiB, free 429.5 MiB)
[2025-07-11T11:47:05.977+0200] {subprocess.py:93} INFO - 25/07/11 11:47:05 INFO MemoryStore: Block broadcast_24_piece0 stored as bytes in memory (estimated size 4.2 KiB, free 429.5 MiB)
[2025-07-11T11:47:05.977+0200] {subprocess.py:93} INFO - 25/07/11 11:47:05 INFO BlockManagerInfo: Added broadcast_24_piece0 in memory on 138.4.31.14:39113 (size: 4.2 KiB, free: 434.2 MiB)
[2025-07-11T11:47:05.984+0200] {subprocess.py:93} INFO - 25/07/11 11:47:05 INFO SparkContext: Created broadcast 24 from broadcast at DAGScheduler.scala:1585
[2025-07-11T11:47:05.984+0200] {subprocess.py:93} INFO - 25/07/11 11:47:05 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 30 (MapPartitionsRDD[56] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) (first 15 tasks are for partitions Vector(0))
[2025-07-11T11:47:05.984+0200] {subprocess.py:93} INFO - 25/07/11 11:47:05 INFO TaskSchedulerImpl: Adding task set 30.0 with 1 tasks resource profile 0
[2025-07-11T11:47:05.984+0200] {subprocess.py:93} INFO - 25/07/11 11:47:05 INFO TaskSetManager: Starting task 0.0 in stage 30.0 (TID 58) (138.4.31.14, executor driver, partition 0, NODE_LOCAL, 12968 bytes)
[2025-07-11T11:47:05.984+0200] {subprocess.py:93} INFO - 25/07/11 11:47:05 INFO Executor: Running task 0.0 in stage 30.0 (TID 58)
[2025-07-11T11:47:05.988+0200] {subprocess.py:93} INFO - 25/07/11 11:47:05 INFO ShuffleBlockFetcherIterator: Getting 13 (196.8 KiB) non-empty blocks including 13 (196.8 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2025-07-11T11:47:05.990+0200] {subprocess.py:93} INFO - 25/07/11 11:47:05 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 5 ms
[2025-07-11T11:47:06.019+0200] {subprocess.py:93} INFO - 25/07/11 11:47:06 INFO Executor: Finished task 0.0 in stage 30.0 (TID 58). 40633 bytes result sent to driver
[2025-07-11T11:47:06.021+0200] {subprocess.py:93} INFO - 25/07/11 11:47:06 INFO TaskSetManager: Finished task 0.0 in stage 30.0 (TID 58) in 42 ms on 138.4.31.14 (executor driver) (1/1)
[2025-07-11T11:47:06.021+0200] {subprocess.py:93} INFO - 25/07/11 11:47:06 INFO TaskSchedulerImpl: Removed TaskSet 30.0, whose tasks have all completed, from pool
[2025-07-11T11:47:06.028+0200] {subprocess.py:93} INFO - 25/07/11 11:47:06 INFO DAGScheduler: ResultStage 30 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) finished in 0,047 s
[2025-07-11T11:47:06.028+0200] {subprocess.py:93} INFO - 25/07/11 11:47:06 INFO DAGScheduler: Job 19 is finished. Cancelling potential speculative or zombie tasks for this job
[2025-07-11T11:47:06.028+0200] {subprocess.py:93} INFO - 25/07/11 11:47:06 INFO TaskSchedulerImpl: Killing all running tasks in stage 30: Stage finished
[2025-07-11T11:47:06.028+0200] {subprocess.py:93} INFO - 25/07/11 11:47:06 INFO DAGScheduler: Job 19 finished: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264, took 0,050171 s
[2025-07-11T11:47:06.042+0200] {subprocess.py:93} INFO - 25/07/11 11:47:06 INFO MemoryStore: Block broadcast_25 stored as values in memory (estimated size 4.3 MiB, free 425.3 MiB)
[2025-07-11T11:47:06.045+0200] {subprocess.py:93} INFO - 25/07/11 11:47:06 INFO MemoryStore: Block broadcast_25_piece0 stored as bytes in memory (estimated size 89.8 KiB, free 425.2 MiB)
[2025-07-11T11:47:06.046+0200] {subprocess.py:93} INFO - 25/07/11 11:47:06 INFO BlockManagerInfo: Added broadcast_25_piece0 in memory on 138.4.31.14:39113 (size: 89.8 KiB, free: 434.1 MiB)
[2025-07-11T11:47:06.047+0200] {subprocess.py:93} INFO - 25/07/11 11:47:06 INFO SparkContext: Created broadcast 25 from $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264
[2025-07-11T11:47:06.813+0200] {subprocess.py:93} INFO - 25/07/11 11:47:06 INFO Executor: Finished task 1.0 in stage 27.0 (TID 44). 1959 bytes result sent to driver
[2025-07-11T11:47:06.813+0200] {subprocess.py:93} INFO - 25/07/11 11:47:06 INFO TaskSetManager: Finished task 1.0 in stage 27.0 (TID 44) in 1878 ms on 138.4.31.14 (executor driver) (1/2)
[2025-07-11T11:47:06.928+0200] {subprocess.py:93} INFO - 25/07/11 11:47:06 INFO BlockManagerInfo: Removed broadcast_24_piece0 on 138.4.31.14:39113 in memory (size: 4.2 KiB, free: 434.1 MiB)
[2025-07-11T11:47:12.631+0200] {subprocess.py:93} INFO - 25/07/11 11:47:12 INFO Executor: Finished task 0.0 in stage 27.0 (TID 43). 1959 bytes result sent to driver
[2025-07-11T11:47:12.632+0200] {subprocess.py:93} INFO - 25/07/11 11:47:12 INFO TaskSetManager: Finished task 0.0 in stage 27.0 (TID 43) in 7702 ms on 138.4.31.14 (executor driver) (2/2)
[2025-07-11T11:47:12.632+0200] {subprocess.py:93} INFO - 25/07/11 11:47:12 INFO TaskSchedulerImpl: Removed TaskSet 27.0, whose tasks have all completed, from pool
[2025-07-11T11:47:12.634+0200] {subprocess.py:93} INFO - 25/07/11 11:47:12 INFO DAGScheduler: ShuffleMapStage 27 (collect at StringIndexer.scala:204) finished in 7,708 s
[2025-07-11T11:47:12.634+0200] {subprocess.py:93} INFO - 25/07/11 11:47:12 INFO DAGScheduler: looking for newly runnable stages
[2025-07-11T11:47:12.634+0200] {subprocess.py:93} INFO - 25/07/11 11:47:12 INFO DAGScheduler: running: Set()
[2025-07-11T11:47:12.634+0200] {subprocess.py:93} INFO - 25/07/11 11:47:12 INFO DAGScheduler: waiting: Set()
[2025-07-11T11:47:12.634+0200] {subprocess.py:93} INFO - 25/07/11 11:47:12 INFO DAGScheduler: failed: Set()
[2025-07-11T11:47:12.651+0200] {subprocess.py:93} INFO - 25/07/11 11:47:12 INFO ShufflePartitionsUtil: For shuffle(8), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576
[2025-07-11T11:47:12.672+0200] {subprocess.py:93} INFO - 25/07/11 11:47:12 INFO CodeGenerator: Code generated in 10.502712 ms
[2025-07-11T11:47:12.677+0200] {subprocess.py:93} INFO - 25/07/11 11:47:12 INFO DAGScheduler: Registering RDD 60 (collect at StringIndexer.scala:204) as input to shuffle 10
[2025-07-11T11:47:12.677+0200] {subprocess.py:93} INFO - 25/07/11 11:47:12 INFO DAGScheduler: Got map stage job 20 (collect at StringIndexer.scala:204) with 1 output partitions
[2025-07-11T11:47:12.677+0200] {subprocess.py:93} INFO - 25/07/11 11:47:12 INFO DAGScheduler: Final stage: ShuffleMapStage 32 (collect at StringIndexer.scala:204)
[2025-07-11T11:47:12.677+0200] {subprocess.py:93} INFO - 25/07/11 11:47:12 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 31)
[2025-07-11T11:47:12.677+0200] {subprocess.py:93} INFO - 25/07/11 11:47:12 INFO DAGScheduler: Missing parents: List()
[2025-07-11T11:47:12.677+0200] {subprocess.py:93} INFO - 25/07/11 11:47:12 INFO DAGScheduler: Submitting ShuffleMapStage 32 (MapPartitionsRDD[60] at collect at StringIndexer.scala:204), which has no missing parents
[2025-07-11T11:47:12.682+0200] {subprocess.py:93} INFO - 25/07/11 11:47:12 INFO MemoryStore: Block broadcast_26 stored as values in memory (estimated size 49.4 KiB, free 425.1 MiB)
[2025-07-11T11:47:12.682+0200] {subprocess.py:93} INFO - 25/07/11 11:47:12 INFO MemoryStore: Block broadcast_26_piece0 stored as bytes in memory (estimated size 23.6 KiB, free 425.1 MiB)
[2025-07-11T11:47:12.682+0200] {subprocess.py:93} INFO - 25/07/11 11:47:12 INFO BlockManagerInfo: Added broadcast_26_piece0 in memory on 138.4.31.14:39113 (size: 23.6 KiB, free: 434.1 MiB)
[2025-07-11T11:47:12.682+0200] {subprocess.py:93} INFO - 25/07/11 11:47:12 INFO SparkContext: Created broadcast 26 from broadcast at DAGScheduler.scala:1585
[2025-07-11T11:47:12.684+0200] {subprocess.py:93} INFO - 25/07/11 11:47:12 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 32 (MapPartitionsRDD[60] at collect at StringIndexer.scala:204) (first 15 tasks are for partitions Vector(0))
[2025-07-11T11:47:12.684+0200] {subprocess.py:93} INFO - 25/07/11 11:47:12 INFO TaskSchedulerImpl: Adding task set 32.0 with 1 tasks resource profile 0
[2025-07-11T11:47:12.685+0200] {subprocess.py:93} INFO - 25/07/11 11:47:12 INFO TaskSetManager: Starting task 0.0 in stage 32.0 (TID 59) (138.4.31.14, executor driver, partition 0, NODE_LOCAL, 12957 bytes)
[2025-07-11T11:47:12.685+0200] {subprocess.py:93} INFO - 25/07/11 11:47:12 INFO Executor: Running task 0.0 in stage 32.0 (TID 59)
[2025-07-11T11:47:12.698+0200] {subprocess.py:93} INFO - 25/07/11 11:47:12 INFO ShuffleBlockFetcherIterator: Getting 2 (887.5 KiB) non-empty blocks including 2 (887.5 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2025-07-11T11:47:12.698+0200] {subprocess.py:93} INFO - 25/07/11 11:47:12 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2025-07-11T11:47:12.699+0200] {subprocess.py:93} INFO - 25/07/11 11:47:12 INFO CodeGenerator: Code generated in 7.88272 ms
[2025-07-11T11:47:13.065+0200] {subprocess.py:93} INFO - 25/07/11 11:47:13 INFO Executor: Finished task 0.0 in stage 32.0 (TID 59). 6487 bytes result sent to driver
[2025-07-11T11:47:13.065+0200] {subprocess.py:93} INFO - 25/07/11 11:47:13 INFO TaskSetManager: Finished task 0.0 in stage 32.0 (TID 59) in 381 ms on 138.4.31.14 (executor driver) (1/1)
[2025-07-11T11:47:13.065+0200] {subprocess.py:93} INFO - 25/07/11 11:47:13 INFO TaskSchedulerImpl: Removed TaskSet 32.0, whose tasks have all completed, from pool
[2025-07-11T11:47:13.067+0200] {subprocess.py:93} INFO - 25/07/11 11:47:13 INFO DAGScheduler: ShuffleMapStage 32 (collect at StringIndexer.scala:204) finished in 0,389 s
[2025-07-11T11:47:13.068+0200] {subprocess.py:93} INFO - 25/07/11 11:47:13 INFO DAGScheduler: looking for newly runnable stages
[2025-07-11T11:47:13.068+0200] {subprocess.py:93} INFO - 25/07/11 11:47:13 INFO DAGScheduler: running: Set()
[2025-07-11T11:47:13.068+0200] {subprocess.py:93} INFO - 25/07/11 11:47:13 INFO DAGScheduler: waiting: Set()
[2025-07-11T11:47:13.068+0200] {subprocess.py:93} INFO - 25/07/11 11:47:13 INFO DAGScheduler: failed: Set()
[2025-07-11T11:47:13.097+0200] {subprocess.py:93} INFO - 25/07/11 11:47:13 INFO SparkContext: Starting job: collect at StringIndexer.scala:204
[2025-07-11T11:47:13.099+0200] {subprocess.py:93} INFO - 25/07/11 11:47:13 INFO DAGScheduler: Got job 21 (collect at StringIndexer.scala:204) with 1 output partitions
[2025-07-11T11:47:13.099+0200] {subprocess.py:93} INFO - 25/07/11 11:47:13 INFO DAGScheduler: Final stage: ResultStage 35 (collect at StringIndexer.scala:204)
[2025-07-11T11:47:13.099+0200] {subprocess.py:93} INFO - 25/07/11 11:47:13 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 34)
[2025-07-11T11:47:13.099+0200] {subprocess.py:93} INFO - 25/07/11 11:47:13 INFO DAGScheduler: Missing parents: List()
[2025-07-11T11:47:13.099+0200] {subprocess.py:93} INFO - 25/07/11 11:47:13 INFO DAGScheduler: Submitting ResultStage 35 (MapPartitionsRDD[63] at collect at StringIndexer.scala:204), which has no missing parents
[2025-07-11T11:47:13.103+0200] {subprocess.py:93} INFO - 25/07/11 11:47:13 INFO MemoryStore: Block broadcast_27 stored as values in memory (estimated size 48.7 KiB, free 425.1 MiB)
[2025-07-11T11:47:13.105+0200] {subprocess.py:93} INFO - 25/07/11 11:47:13 INFO MemoryStore: Block broadcast_27_piece0 stored as bytes in memory (estimated size 23.3 KiB, free 425.1 MiB)
[2025-07-11T11:47:13.105+0200] {subprocess.py:93} INFO - 25/07/11 11:47:13 INFO BlockManagerInfo: Added broadcast_27_piece0 in memory on 138.4.31.14:39113 (size: 23.3 KiB, free: 434.1 MiB)
[2025-07-11T11:47:13.106+0200] {subprocess.py:93} INFO - 25/07/11 11:47:13 INFO SparkContext: Created broadcast 27 from broadcast at DAGScheduler.scala:1585
[2025-07-11T11:47:13.106+0200] {subprocess.py:93} INFO - 25/07/11 11:47:13 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 35 (MapPartitionsRDD[63] at collect at StringIndexer.scala:204) (first 15 tasks are for partitions Vector(0))
[2025-07-11T11:47:13.106+0200] {subprocess.py:93} INFO - 25/07/11 11:47:13 INFO TaskSchedulerImpl: Adding task set 35.0 with 1 tasks resource profile 0
[2025-07-11T11:47:13.112+0200] {subprocess.py:93} INFO - 25/07/11 11:47:13 INFO TaskSetManager: Starting task 0.0 in stage 35.0 (TID 60) (138.4.31.14, executor driver, partition 0, NODE_LOCAL, 12952 bytes)
[2025-07-11T11:47:13.113+0200] {subprocess.py:93} INFO - 25/07/11 11:47:13 INFO Executor: Running task 0.0 in stage 35.0 (TID 60)
[2025-07-11T11:47:13.124+0200] {subprocess.py:93} INFO - 25/07/11 11:47:13 INFO ShuffleBlockFetcherIterator: Getting 1 (3.2 KiB) non-empty blocks including 1 (3.2 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2025-07-11T11:47:13.124+0200] {subprocess.py:93} INFO - 25/07/11 11:47:13 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2025-07-11T11:47:13.175+0200] {subprocess.py:93} INFO - 25/07/11 11:47:13 INFO Executor: Finished task 0.0 in stage 35.0 (TID 60). 10907 bytes result sent to driver
[2025-07-11T11:47:13.175+0200] {subprocess.py:93} INFO - 25/07/11 11:47:13 INFO TaskSetManager: Finished task 0.0 in stage 35.0 (TID 60) in 59 ms on 138.4.31.14 (executor driver) (1/1)
[2025-07-11T11:47:13.175+0200] {subprocess.py:93} INFO - 25/07/11 11:47:13 INFO TaskSchedulerImpl: Removed TaskSet 35.0, whose tasks have all completed, from pool
[2025-07-11T11:47:13.175+0200] {subprocess.py:93} INFO - 25/07/11 11:47:13 INFO DAGScheduler: ResultStage 35 (collect at StringIndexer.scala:204) finished in 0,070 s
[2025-07-11T11:47:13.175+0200] {subprocess.py:93} INFO - 25/07/11 11:47:13 INFO DAGScheduler: Job 21 is finished. Cancelling potential speculative or zombie tasks for this job
[2025-07-11T11:47:13.175+0200] {subprocess.py:93} INFO - 25/07/11 11:47:13 INFO TaskSchedulerImpl: Killing all running tasks in stage 35: Stage finished
[2025-07-11T11:47:13.176+0200] {subprocess.py:93} INFO - 25/07/11 11:47:13 INFO DAGScheduler: Job 21 finished: collect at StringIndexer.scala:204, took 0,074052 s
[2025-07-11T11:47:13.246+0200] {subprocess.py:93} INFO - 25/07/11 11:47:13 INFO FileSystemOverwrite: Path /home/monica.fernandez/practica_creativa/models/string_indexer_model_Dest.bin already exists. It will be overwritten.
[2025-07-11T11:47:13.261+0200] {subprocess.py:93} INFO - 25/07/11 11:47:13 INFO HadoopMapRedCommitProtocol: Using output committer class org.apache.hadoop.mapred.FileOutputCommitter
[2025-07-11T11:47:13.261+0200] {subprocess.py:93} INFO - 25/07/11 11:47:13 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2025-07-11T11:47:13.261+0200] {subprocess.py:93} INFO - 25/07/11 11:47:13 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2025-07-11T11:47:13.327+0200] {subprocess.py:93} INFO - 25/07/11 11:47:13 INFO SparkContext: Starting job: runJob at SparkHadoopWriter.scala:83
[2025-07-11T11:47:13.329+0200] {subprocess.py:93} INFO - 25/07/11 11:47:13 INFO DAGScheduler: Got job 22 (runJob at SparkHadoopWriter.scala:83) with 1 output partitions
[2025-07-11T11:47:13.329+0200] {subprocess.py:93} INFO - 25/07/11 11:47:13 INFO DAGScheduler: Final stage: ResultStage 36 (runJob at SparkHadoopWriter.scala:83)
[2025-07-11T11:47:13.329+0200] {subprocess.py:93} INFO - 25/07/11 11:47:13 INFO DAGScheduler: Parents of final stage: List()
[2025-07-11T11:47:13.329+0200] {subprocess.py:93} INFO - 25/07/11 11:47:13 INFO DAGScheduler: Missing parents: List()
[2025-07-11T11:47:13.329+0200] {subprocess.py:93} INFO - 25/07/11 11:47:13 INFO DAGScheduler: Submitting ResultStage 36 (MapPartitionsRDD[65] at saveAsTextFile at ReadWrite.scala:413), which has no missing parents
[2025-07-11T11:47:13.338+0200] {subprocess.py:93} INFO - 25/07/11 11:47:13 INFO MemoryStore: Block broadcast_28 stored as values in memory (estimated size 101.3 KiB, free 425.0 MiB)
[2025-07-11T11:47:13.339+0200] {subprocess.py:93} INFO - 25/07/11 11:47:13 INFO MemoryStore: Block broadcast_28_piece0 stored as bytes in memory (estimated size 36.6 KiB, free 424.9 MiB)
[2025-07-11T11:47:13.339+0200] {subprocess.py:93} INFO - 25/07/11 11:47:13 INFO BlockManagerInfo: Added broadcast_28_piece0 in memory on 138.4.31.14:39113 (size: 36.6 KiB, free: 434.1 MiB)
[2025-07-11T11:47:13.340+0200] {subprocess.py:93} INFO - 25/07/11 11:47:13 INFO SparkContext: Created broadcast 28 from broadcast at DAGScheduler.scala:1585
[2025-07-11T11:47:13.342+0200] {subprocess.py:93} INFO - 25/07/11 11:47:13 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 36 (MapPartitionsRDD[65] at saveAsTextFile at ReadWrite.scala:413) (first 15 tasks are for partitions Vector(0))
[2025-07-11T11:47:13.342+0200] {subprocess.py:93} INFO - 25/07/11 11:47:13 INFO TaskSchedulerImpl: Adding task set 36.0 with 1 tasks resource profile 0
[2025-07-11T11:47:13.343+0200] {subprocess.py:93} INFO - 25/07/11 11:47:13 INFO TaskSetManager: Starting task 0.0 in stage 36.0 (TID 61) (138.4.31.14, executor driver, partition 0, PROCESS_LOCAL, 13276 bytes)
[2025-07-11T11:47:13.344+0200] {subprocess.py:93} INFO - 25/07/11 11:47:13 INFO Executor: Running task 0.0 in stage 36.0 (TID 61)
[2025-07-11T11:47:13.355+0200] {subprocess.py:93} INFO - 25/07/11 11:47:13 INFO HadoopMapRedCommitProtocol: Using output committer class org.apache.hadoop.mapred.FileOutputCommitter
[2025-07-11T11:47:13.357+0200] {subprocess.py:93} INFO - 25/07/11 11:47:13 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2025-07-11T11:47:13.357+0200] {subprocess.py:93} INFO - 25/07/11 11:47:13 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2025-07-11T11:47:13.408+0200] {subprocess.py:93} INFO - 25/07/11 11:47:13 INFO FileOutputCommitter: Saved output of task 'attempt_202507111147137100275901713796089_0065_m_000000_0' to file:/home/monica.fernandez/practica_creativa/models/string_indexer_model_Dest.bin/metadata/_temporary/0/task_202507111147137100275901713796089_0065_m_000000
[2025-07-11T11:47:13.408+0200] {subprocess.py:93} INFO - 25/07/11 11:47:13 INFO SparkHadoopMapRedUtil: attempt_202507111147137100275901713796089_0065_m_000000_0: Committed. Elapsed time: 1 ms.
[2025-07-11T11:47:13.412+0200] {subprocess.py:93} INFO - 25/07/11 11:47:13 INFO Executor: Finished task 0.0 in stage 36.0 (TID 61). 1170 bytes result sent to driver
[2025-07-11T11:47:13.414+0200] {subprocess.py:93} INFO - 25/07/11 11:47:13 INFO TaskSetManager: Finished task 0.0 in stage 36.0 (TID 61) in 69 ms on 138.4.31.14 (executor driver) (1/1)
[2025-07-11T11:47:13.414+0200] {subprocess.py:93} INFO - 25/07/11 11:47:13 INFO TaskSchedulerImpl: Removed TaskSet 36.0, whose tasks have all completed, from pool
[2025-07-11T11:47:13.414+0200] {subprocess.py:93} INFO - 25/07/11 11:47:13 INFO DAGScheduler: ResultStage 36 (runJob at SparkHadoopWriter.scala:83) finished in 0,085 s
[2025-07-11T11:47:13.414+0200] {subprocess.py:93} INFO - 25/07/11 11:47:13 INFO DAGScheduler: Job 22 is finished. Cancelling potential speculative or zombie tasks for this job
[2025-07-11T11:47:13.414+0200] {subprocess.py:93} INFO - 25/07/11 11:47:13 INFO TaskSchedulerImpl: Killing all running tasks in stage 36: Stage finished
[2025-07-11T11:47:13.414+0200] {subprocess.py:93} INFO - 25/07/11 11:47:13 INFO DAGScheduler: Job 22 finished: runJob at SparkHadoopWriter.scala:83, took 0,086002 s
[2025-07-11T11:47:13.414+0200] {subprocess.py:93} INFO - 25/07/11 11:47:13 INFO SparkHadoopWriter: Start to commit write Job job_202507111147137100275901713796089_0065.
[2025-07-11T11:47:13.437+0200] {subprocess.py:93} INFO - 25/07/11 11:47:13 INFO SparkHadoopWriter: Write Job job_202507111147137100275901713796089_0065 committed. Elapsed time: 22 ms.
[2025-07-11T11:47:13.502+0200] {subprocess.py:93} INFO - 25/07/11 11:47:13 INFO DAGScheduler: Registering RDD 68 (parquet at StringIndexer.scala:499) as input to shuffle 11
[2025-07-11T11:47:13.503+0200] {subprocess.py:93} INFO - 25/07/11 11:47:13 INFO DAGScheduler: Got map stage job 23 (parquet at StringIndexer.scala:499) with 1 output partitions
[2025-07-11T11:47:13.503+0200] {subprocess.py:93} INFO - 25/07/11 11:47:13 INFO DAGScheduler: Final stage: ShuffleMapStage 37 (parquet at StringIndexer.scala:499)
[2025-07-11T11:47:13.503+0200] {subprocess.py:93} INFO - 25/07/11 11:47:13 INFO DAGScheduler: Parents of final stage: List()
[2025-07-11T11:47:13.503+0200] {subprocess.py:93} INFO - 25/07/11 11:47:13 INFO DAGScheduler: Missing parents: List()
[2025-07-11T11:47:13.503+0200] {subprocess.py:93} INFO - 25/07/11 11:47:13 INFO DAGScheduler: Submitting ShuffleMapStage 37 (MapPartitionsRDD[68] at parquet at StringIndexer.scala:499), which has no missing parents
[2025-07-11T11:47:13.503+0200] {subprocess.py:93} INFO - 25/07/11 11:47:13 INFO MemoryStore: Block broadcast_29 stored as values in memory (estimated size 8.1 KiB, free 424.9 MiB)
[2025-07-11T11:47:13.503+0200] {subprocess.py:93} INFO - 25/07/11 11:47:13 INFO MemoryStore: Block broadcast_29_piece0 stored as bytes in memory (estimated size 4.4 KiB, free 424.9 MiB)
[2025-07-11T11:47:13.503+0200] {subprocess.py:93} INFO - 25/07/11 11:47:13 INFO BlockManagerInfo: Added broadcast_29_piece0 in memory on 138.4.31.14:39113 (size: 4.4 KiB, free: 434.0 MiB)
[2025-07-11T11:47:13.504+0200] {subprocess.py:93} INFO - 25/07/11 11:47:13 INFO SparkContext: Created broadcast 29 from broadcast at DAGScheduler.scala:1585
[2025-07-11T11:47:13.505+0200] {subprocess.py:93} INFO - 25/07/11 11:47:13 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 37 (MapPartitionsRDD[68] at parquet at StringIndexer.scala:499) (first 15 tasks are for partitions Vector(0))
[2025-07-11T11:47:13.505+0200] {subprocess.py:93} INFO - 25/07/11 11:47:13 INFO TaskSchedulerImpl: Adding task set 37.0 with 1 tasks resource profile 0
[2025-07-11T11:47:13.505+0200] {subprocess.py:93} INFO - 25/07/11 11:47:13 INFO TaskSetManager: Starting task 0.0 in stage 37.0 (TID 62) (138.4.31.14, executor driver, partition 0, PROCESS_LOCAL, 18200 bytes)
[2025-07-11T11:47:13.506+0200] {subprocess.py:93} INFO - 25/07/11 11:47:13 INFO Executor: Running task 0.0 in stage 37.0 (TID 62)
[2025-07-11T11:47:13.509+0200] {subprocess.py:93} INFO - 25/07/11 11:47:13 INFO Executor: Finished task 0.0 in stage 37.0 (TID 62). 1628 bytes result sent to driver
[2025-07-11T11:47:13.510+0200] {subprocess.py:93} INFO - 25/07/11 11:47:13 INFO TaskSetManager: Finished task 0.0 in stage 37.0 (TID 62) in 6 ms on 138.4.31.14 (executor driver) (1/1)
[2025-07-11T11:47:13.510+0200] {subprocess.py:93} INFO - 25/07/11 11:47:13 INFO TaskSchedulerImpl: Removed TaskSet 37.0, whose tasks have all completed, from pool
[2025-07-11T11:47:13.511+0200] {subprocess.py:93} INFO - 25/07/11 11:47:13 INFO DAGScheduler: ShuffleMapStage 37 (parquet at StringIndexer.scala:499) finished in 0,009 s
[2025-07-11T11:47:13.511+0200] {subprocess.py:93} INFO - 25/07/11 11:47:13 INFO DAGScheduler: looking for newly runnable stages
[2025-07-11T11:47:13.511+0200] {subprocess.py:93} INFO - 25/07/11 11:47:13 INFO DAGScheduler: running: Set()
[2025-07-11T11:47:13.511+0200] {subprocess.py:93} INFO - 25/07/11 11:47:13 INFO DAGScheduler: waiting: Set()
[2025-07-11T11:47:13.511+0200] {subprocess.py:93} INFO - 25/07/11 11:47:13 INFO DAGScheduler: failed: Set()
[2025-07-11T11:47:13.517+0200] {subprocess.py:93} INFO - 25/07/11 11:47:13 INFO ParquetUtils: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
[2025-07-11T11:47:13.518+0200] {subprocess.py:93} INFO - 25/07/11 11:47:13 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2025-07-11T11:47:13.519+0200] {subprocess.py:93} INFO - 25/07/11 11:47:13 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2025-07-11T11:47:13.520+0200] {subprocess.py:93} INFO - 25/07/11 11:47:13 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[2025-07-11T11:47:13.520+0200] {subprocess.py:93} INFO - 25/07/11 11:47:13 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2025-07-11T11:47:13.520+0200] {subprocess.py:93} INFO - 25/07/11 11:47:13 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2025-07-11T11:47:13.520+0200] {subprocess.py:93} INFO - 25/07/11 11:47:13 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[2025-07-11T11:47:13.545+0200] {subprocess.py:93} INFO - 25/07/11 11:47:13 INFO SparkContext: Starting job: parquet at StringIndexer.scala:499
[2025-07-11T11:47:13.546+0200] {subprocess.py:93} INFO - 25/07/11 11:47:13 INFO DAGScheduler: Got job 24 (parquet at StringIndexer.scala:499) with 1 output partitions
[2025-07-11T11:47:13.546+0200] {subprocess.py:93} INFO - 25/07/11 11:47:13 INFO DAGScheduler: Final stage: ResultStage 39 (parquet at StringIndexer.scala:499)
[2025-07-11T11:47:13.546+0200] {subprocess.py:93} INFO - 25/07/11 11:47:13 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 38)
[2025-07-11T11:47:13.546+0200] {subprocess.py:93} INFO - 25/07/11 11:47:13 INFO DAGScheduler: Missing parents: List()
[2025-07-11T11:47:13.547+0200] {subprocess.py:93} INFO - 25/07/11 11:47:13 INFO DAGScheduler: Submitting ResultStage 39 (MapPartitionsRDD[70] at parquet at StringIndexer.scala:499), which has no missing parents
[2025-07-11T11:47:13.574+0200] {subprocess.py:93} INFO - 25/07/11 11:47:13 INFO MemoryStore: Block broadcast_30 stored as values in memory (estimated size 230.2 KiB, free 424.7 MiB)
[2025-07-11T11:47:13.575+0200] {subprocess.py:93} INFO - 25/07/11 11:47:13 INFO MemoryStore: Block broadcast_30_piece0 stored as bytes in memory (estimated size 81.0 KiB, free 424.6 MiB)
[2025-07-11T11:47:13.575+0200] {subprocess.py:93} INFO - 25/07/11 11:47:13 INFO BlockManagerInfo: Added broadcast_30_piece0 in memory on 138.4.31.14:39113 (size: 81.0 KiB, free: 434.0 MiB)
[2025-07-11T11:47:13.575+0200] {subprocess.py:93} INFO - 25/07/11 11:47:13 INFO SparkContext: Created broadcast 30 from broadcast at DAGScheduler.scala:1585
[2025-07-11T11:47:13.576+0200] {subprocess.py:93} INFO - 25/07/11 11:47:13 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 39 (MapPartitionsRDD[70] at parquet at StringIndexer.scala:499) (first 15 tasks are for partitions Vector(0))
[2025-07-11T11:47:13.576+0200] {subprocess.py:93} INFO - 25/07/11 11:47:13 INFO TaskSchedulerImpl: Adding task set 39.0 with 1 tasks resource profile 0
[2025-07-11T11:47:13.577+0200] {subprocess.py:93} INFO - 25/07/11 11:47:13 INFO TaskSetManager: Starting task 0.0 in stage 39.0 (TID 63) (138.4.31.14, executor driver, partition 0, NODE_LOCAL, 12952 bytes)
[2025-07-11T11:47:13.583+0200] {subprocess.py:93} INFO - 25/07/11 11:47:13 INFO Executor: Running task 0.0 in stage 39.0 (TID 63)
[2025-07-11T11:47:13.589+0200] {subprocess.py:93} INFO - 25/07/11 11:47:13 INFO ShuffleBlockFetcherIterator: Getting 1 (2.9 KiB) non-empty blocks including 1 (2.9 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2025-07-11T11:47:13.589+0200] {subprocess.py:93} INFO - 25/07/11 11:47:13 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2025-07-11T11:47:13.591+0200] {subprocess.py:93} INFO - 25/07/11 11:47:13 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2025-07-11T11:47:13.591+0200] {subprocess.py:93} INFO - 25/07/11 11:47:13 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2025-07-11T11:47:13.591+0200] {subprocess.py:93} INFO - 25/07/11 11:47:13 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[2025-07-11T11:47:13.591+0200] {subprocess.py:93} INFO - 25/07/11 11:47:13 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2025-07-11T11:47:13.591+0200] {subprocess.py:93} INFO - 25/07/11 11:47:13 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2025-07-11T11:47:13.591+0200] {subprocess.py:93} INFO - 25/07/11 11:47:13 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[2025-07-11T11:47:13.591+0200] {subprocess.py:93} INFO - 25/07/11 11:47:13 INFO CodecConfig: Compression: SNAPPY
[2025-07-11T11:47:13.591+0200] {subprocess.py:93} INFO - 25/07/11 11:47:13 INFO CodecConfig: Compression: SNAPPY
[2025-07-11T11:47:13.594+0200] {subprocess.py:93} INFO - 25/07/11 11:47:13 INFO ParquetOutputFormat: ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]
[2025-07-11T11:47:13.594+0200] {subprocess.py:93} INFO - 25/07/11 11:47:13 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
[2025-07-11T11:47:13.595+0200] {subprocess.py:93} INFO - {
[2025-07-11T11:47:13.596+0200] {subprocess.py:93} INFO -   "type" : "struct",
[2025-07-11T11:47:13.596+0200] {subprocess.py:93} INFO -   "fields" : [ {
[2025-07-11T11:47:13.597+0200] {subprocess.py:93} INFO -     "name" : "labelsArray",
[2025-07-11T11:47:13.597+0200] {subprocess.py:93} INFO -     "type" : {
[2025-07-11T11:47:13.597+0200] {subprocess.py:93} INFO -       "type" : "array",
[2025-07-11T11:47:13.597+0200] {subprocess.py:93} INFO -       "elementType" : {
[2025-07-11T11:47:13.597+0200] {subprocess.py:93} INFO -         "type" : "array",
[2025-07-11T11:47:13.597+0200] {subprocess.py:93} INFO -         "elementType" : "string",
[2025-07-11T11:47:13.597+0200] {subprocess.py:93} INFO -         "containsNull" : true
[2025-07-11T11:47:13.597+0200] {subprocess.py:93} INFO -       },
[2025-07-11T11:47:13.597+0200] {subprocess.py:93} INFO -       "containsNull" : true
[2025-07-11T11:47:13.597+0200] {subprocess.py:93} INFO -     },
[2025-07-11T11:47:13.598+0200] {subprocess.py:93} INFO -     "nullable" : true,
[2025-07-11T11:47:13.598+0200] {subprocess.py:93} INFO -     "metadata" : { }
[2025-07-11T11:47:13.598+0200] {subprocess.py:93} INFO -   } ]
[2025-07-11T11:47:13.598+0200] {subprocess.py:93} INFO - }
[2025-07-11T11:47:13.598+0200] {subprocess.py:93} INFO - and corresponding Parquet message type:
[2025-07-11T11:47:13.598+0200] {subprocess.py:93} INFO - message spark_schema {
[2025-07-11T11:47:13.598+0200] {subprocess.py:93} INFO -   optional group labelsArray (LIST) {
[2025-07-11T11:47:13.598+0200] {subprocess.py:93} INFO -     repeated group list {
[2025-07-11T11:47:13.598+0200] {subprocess.py:93} INFO -       optional group element (LIST) {
[2025-07-11T11:47:13.598+0200] {subprocess.py:93} INFO -         repeated group list {
[2025-07-11T11:47:13.598+0200] {subprocess.py:93} INFO -           optional binary element (STRING);
[2025-07-11T11:47:13.598+0200] {subprocess.py:93} INFO -         }
[2025-07-11T11:47:13.598+0200] {subprocess.py:93} INFO -       }
[2025-07-11T11:47:13.599+0200] {subprocess.py:93} INFO -     }
[2025-07-11T11:47:13.599+0200] {subprocess.py:93} INFO -   }
[2025-07-11T11:47:13.599+0200] {subprocess.py:93} INFO - }
[2025-07-11T11:47:13.599+0200] {subprocess.py:93} INFO - 
[2025-07-11T11:47:13.599+0200] {subprocess.py:93} INFO - 
[2025-07-11T11:47:13.648+0200] {subprocess.py:93} INFO - 25/07/11 11:47:13 INFO FileOutputCommitter: Saved output of task 'attempt_202507111147133154083124007013478_0039_m_000000_63' to file:/home/monica.fernandez/practica_creativa/models/string_indexer_model_Dest.bin/data/_temporary/0/task_202507111147133154083124007013478_0039_m_000000
[2025-07-11T11:47:13.648+0200] {subprocess.py:93} INFO - 25/07/11 11:47:13 INFO SparkHadoopMapRedUtil: attempt_202507111147133154083124007013478_0039_m_000000_63: Committed. Elapsed time: 1 ms.
[2025-07-11T11:47:13.657+0200] {subprocess.py:93} INFO - 25/07/11 11:47:13 INFO Executor: Finished task 0.0 in stage 39.0 (TID 63). 4826 bytes result sent to driver
[2025-07-11T11:47:13.659+0200] {subprocess.py:93} INFO - 25/07/11 11:47:13 INFO TaskSetManager: Finished task 0.0 in stage 39.0 (TID 63) in 82 ms on 138.4.31.14 (executor driver) (1/1)
[2025-07-11T11:47:13.659+0200] {subprocess.py:93} INFO - 25/07/11 11:47:13 INFO TaskSchedulerImpl: Removed TaskSet 39.0, whose tasks have all completed, from pool
[2025-07-11T11:47:13.660+0200] {subprocess.py:93} INFO - 25/07/11 11:47:13 INFO DAGScheduler: ResultStage 39 (parquet at StringIndexer.scala:499) finished in 0,113 s
[2025-07-11T11:47:13.660+0200] {subprocess.py:93} INFO - 25/07/11 11:47:13 INFO DAGScheduler: Job 24 is finished. Cancelling potential speculative or zombie tasks for this job
[2025-07-11T11:47:13.660+0200] {subprocess.py:93} INFO - 25/07/11 11:47:13 INFO TaskSchedulerImpl: Killing all running tasks in stage 39: Stage finished
[2025-07-11T11:47:13.661+0200] {subprocess.py:93} INFO - 25/07/11 11:47:13 INFO DAGScheduler: Job 24 finished: parquet at StringIndexer.scala:499, took 0,115840 s
[2025-07-11T11:47:13.661+0200] {subprocess.py:93} INFO - 25/07/11 11:47:13 INFO BlockManagerInfo: Removed broadcast_29_piece0 on 138.4.31.14:39113 in memory (size: 4.4 KiB, free: 434.0 MiB)
[2025-07-11T11:47:13.661+0200] {subprocess.py:93} INFO - 25/07/11 11:47:13 INFO FileFormatWriter: Start to commit write Job d7414a15-53a4-4567-90ad-734c2b4262e4.
[2025-07-11T11:47:13.678+0200] {subprocess.py:93} INFO - 25/07/11 11:47:13 INFO BlockManagerInfo: Removed broadcast_28_piece0 on 138.4.31.14:39113 in memory (size: 36.6 KiB, free: 434.0 MiB)
[2025-07-11T11:47:13.682+0200] {subprocess.py:93} INFO - 25/07/11 11:47:13 INFO BlockManagerInfo: Removed broadcast_27_piece0 on 138.4.31.14:39113 in memory (size: 23.3 KiB, free: 434.0 MiB)
[2025-07-11T11:47:13.688+0200] {subprocess.py:93} INFO - 25/07/11 11:47:13 INFO BlockManagerInfo: Removed broadcast_26_piece0 on 138.4.31.14:39113 in memory (size: 23.6 KiB, free: 434.1 MiB)
[2025-07-11T11:47:13.699+0200] {subprocess.py:93} INFO - 25/07/11 11:47:13 INFO FileFormatWriter: Write Job d7414a15-53a4-4567-90ad-734c2b4262e4 committed. Elapsed time: 37 ms.
[2025-07-11T11:47:13.700+0200] {subprocess.py:93} INFO - 25/07/11 11:47:13 INFO FileFormatWriter: Finished processing stats for write job d7414a15-53a4-4567-90ad-734c2b4262e4.
[2025-07-11T11:47:13.785+0200] {subprocess.py:93} INFO - 25/07/11 11:47:13 INFO V2ScanRelationPushDown:
[2025-07-11T11:47:13.785+0200] {subprocess.py:93} INFO - Pushing operators to origin_dest_distances
[2025-07-11T11:47:13.785+0200] {subprocess.py:93} INFO - Pushed Filters:
[2025-07-11T11:47:13.785+0200] {subprocess.py:93} INFO - Post-Scan Filters:
[2025-07-11T11:47:13.786+0200] {subprocess.py:93} INFO - 
[2025-07-11T11:47:13.814+0200] {subprocess.py:93} INFO - 25/07/11 11:47:13 INFO V2ScanRelationPushDown:
[2025-07-11T11:47:13.814+0200] {subprocess.py:93} INFO - Output: origin#42, dest#43
[2025-07-11T11:47:13.814+0200] {subprocess.py:93} INFO - 
[2025-07-11T11:47:13.830+0200] {subprocess.py:93} INFO - 25/07/11 11:47:13 INFO FileSourceStrategy: Pushed Filters:
[2025-07-11T11:47:13.830+0200] {subprocess.py:93} INFO - 25/07/11 11:47:13 INFO FileSourceStrategy: Post-Scan Filters:
[2025-07-11T11:47:13.868+0200] {subprocess.py:93} INFO - 25/07/11 11:47:13 INFO CodeGenerator: Code generated in 11.895535 ms
[2025-07-11T11:47:13.870+0200] {subprocess.py:93} INFO - 25/07/11 11:47:13 INFO MemoryStore: Block broadcast_31 stored as values in memory (estimated size 209.9 KiB, free 424.7 MiB)
[2025-07-11T11:47:13.878+0200] {subprocess.py:93} INFO - 25/07/11 11:47:13 INFO MemoryStore: Block broadcast_31_piece0 stored as bytes in memory (estimated size 37.7 KiB, free 424.6 MiB)
[2025-07-11T11:47:13.878+0200] {subprocess.py:93} INFO - 25/07/11 11:47:13 INFO BlockManagerInfo: Added broadcast_31_piece0 in memory on 138.4.31.14:39113 (size: 37.7 KiB, free: 434.0 MiB)
[2025-07-11T11:47:13.879+0200] {subprocess.py:93} INFO - 25/07/11 11:47:13 INFO SparkContext: Created broadcast 31 from collect at StringIndexer.scala:204
[2025-07-11T11:47:13.880+0200] {subprocess.py:93} INFO - 25/07/11 11:47:13 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
[2025-07-11T11:47:13.888+0200] {subprocess.py:93} INFO - 25/07/11 11:47:13 INFO DAGScheduler: Registering RDD 74 (collect at StringIndexer.scala:204) as input to shuffle 12
[2025-07-11T11:47:13.888+0200] {subprocess.py:93} INFO - 25/07/11 11:47:13 INFO DAGScheduler: Got map stage job 25 (collect at StringIndexer.scala:204) with 2 output partitions
[2025-07-11T11:47:13.889+0200] {subprocess.py:93} INFO - 25/07/11 11:47:13 INFO DAGScheduler: Final stage: ShuffleMapStage 40 (collect at StringIndexer.scala:204)
[2025-07-11T11:47:13.889+0200] {subprocess.py:93} INFO - 25/07/11 11:47:13 INFO DAGScheduler: Parents of final stage: List()
[2025-07-11T11:47:13.889+0200] {subprocess.py:93} INFO - 25/07/11 11:47:13 INFO DAGScheduler: Missing parents: List()
[2025-07-11T11:47:13.889+0200] {subprocess.py:93} INFO - 25/07/11 11:47:13 INFO DAGScheduler: Submitting ShuffleMapStage 40 (MapPartitionsRDD[74] at collect at StringIndexer.scala:204), which has no missing parents
[2025-07-11T11:47:13.891+0200] {subprocess.py:93} INFO - 25/07/11 11:47:13 INFO MemoryStore: Block broadcast_32 stored as values in memory (estimated size 18.0 KiB, free 424.6 MiB)
[2025-07-11T11:47:13.891+0200] {subprocess.py:93} INFO - 25/07/11 11:47:13 INFO MemoryStore: Block broadcast_32_piece0 stored as bytes in memory (estimated size 8.6 KiB, free 424.6 MiB)
[2025-07-11T11:47:13.892+0200] {subprocess.py:93} INFO - 25/07/11 11:47:13 INFO BlockManagerInfo: Added broadcast_32_piece0 in memory on 138.4.31.14:39113 (size: 8.6 KiB, free: 434.0 MiB)
[2025-07-11T11:47:13.893+0200] {subprocess.py:93} INFO - 25/07/11 11:47:13 INFO SparkContext: Created broadcast 32 from broadcast at DAGScheduler.scala:1585
[2025-07-11T11:47:13.893+0200] {subprocess.py:93} INFO - 25/07/11 11:47:13 INFO DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 40 (MapPartitionsRDD[74] at collect at StringIndexer.scala:204) (first 15 tasks are for partitions Vector(0, 1))
[2025-07-11T11:47:13.893+0200] {subprocess.py:93} INFO - 25/07/11 11:47:13 INFO TaskSchedulerImpl: Adding task set 40.0 with 2 tasks resource profile 0
[2025-07-11T11:47:13.894+0200] {subprocess.py:93} INFO - 25/07/11 11:47:13 INFO TaskSetManager: Starting task 0.0 in stage 40.0 (TID 64) (138.4.31.14, executor driver, partition 0, PROCESS_LOCAL, 13595 bytes)
[2025-07-11T11:47:13.894+0200] {subprocess.py:93} INFO - 25/07/11 11:47:13 INFO TaskSetManager: Starting task 1.0 in stage 40.0 (TID 65) (138.4.31.14, executor driver, partition 1, PROCESS_LOCAL, 13595 bytes)
[2025-07-11T11:47:13.895+0200] {subprocess.py:93} INFO - 25/07/11 11:47:13 INFO Executor: Running task 0.0 in stage 40.0 (TID 64)
[2025-07-11T11:47:13.900+0200] {subprocess.py:93} INFO - 25/07/11 11:47:13 INFO CodeGenerator: Code generated in 6.130996 ms
[2025-07-11T11:47:13.904+0200] {subprocess.py:93} INFO - 25/07/11 11:47:13 INFO Executor: Running task 1.0 in stage 40.0 (TID 65)
[2025-07-11T11:47:13.909+0200] {subprocess.py:93} INFO - 25/07/11 11:47:13 INFO DAGScheduler: Registering RDD 78 (collect at StringIndexer.scala:204) as input to shuffle 13
[2025-07-11T11:47:13.910+0200] {subprocess.py:93} INFO - 25/07/11 11:47:13 INFO DAGScheduler: Got map stage job 26 (collect at StringIndexer.scala:204) with 13 output partitions
[2025-07-11T11:47:13.910+0200] {subprocess.py:93} INFO - 25/07/11 11:47:13 INFO DAGScheduler: Final stage: ShuffleMapStage 41 (collect at StringIndexer.scala:204)
[2025-07-11T11:47:13.910+0200] {subprocess.py:93} INFO - 25/07/11 11:47:13 INFO DAGScheduler: Parents of final stage: List()
[2025-07-11T11:47:13.911+0200] {subprocess.py:93} INFO - 25/07/11 11:47:13 INFO DAGScheduler: Missing parents: List()
[2025-07-11T11:47:13.913+0200] {subprocess.py:93} INFO - 25/07/11 11:47:13 INFO DAGScheduler: Submitting ShuffleMapStage 41 (MapPartitionsRDD[78] at collect at StringIndexer.scala:204), which has no missing parents
[2025-07-11T11:47:13.916+0200] {subprocess.py:93} INFO - 25/07/11 11:47:13 INFO MemoryStore: Block broadcast_33 stored as values in memory (estimated size 18.2 KiB, free 424.6 MiB)
[2025-07-11T11:47:13.916+0200] {subprocess.py:93} INFO - 25/07/11 11:47:13 INFO MemoryStore: Block broadcast_33_piece0 stored as bytes in memory (estimated size 9.0 KiB, free 424.6 MiB)
[2025-07-11T11:47:13.916+0200] {subprocess.py:93} INFO - 25/07/11 11:47:13 INFO BlockManagerInfo: Added broadcast_33_piece0 in memory on 138.4.31.14:39113 (size: 9.0 KiB, free: 434.0 MiB)
[2025-07-11T11:47:13.916+0200] {subprocess.py:93} INFO - 25/07/11 11:47:13 INFO SparkContext: Created broadcast 33 from broadcast at DAGScheduler.scala:1585
[2025-07-11T11:47:13.917+0200] {subprocess.py:93} INFO - 25/07/11 11:47:13 INFO DAGScheduler: Submitting 13 missing tasks from ShuffleMapStage 41 (MapPartitionsRDD[78] at collect at StringIndexer.scala:204) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12))
[2025-07-11T11:47:13.917+0200] {subprocess.py:93} INFO - 25/07/11 11:47:13 INFO TaskSchedulerImpl: Adding task set 41.0 with 13 tasks resource profile 0
[2025-07-11T11:47:13.919+0200] {subprocess.py:93} INFO - 25/07/11 11:47:13 INFO CodeGenerator: Code generated in 19.579956 ms
[2025-07-11T11:47:13.919+0200] {subprocess.py:93} INFO - 25/07/11 11:47:13 INFO TaskSetManager: Starting task 0.0 in stage 41.0 (TID 66) (138.4.31.14, executor driver, partition 0, ANY, 14963 bytes)
[2025-07-11T11:47:13.919+0200] {subprocess.py:93} INFO - 25/07/11 11:47:13 INFO TaskSetManager: Starting task 1.0 in stage 41.0 (TID 67) (138.4.31.14, executor driver, partition 1, ANY, 14843 bytes)
[2025-07-11T11:47:13.920+0200] {subprocess.py:93} INFO - 25/07/11 11:47:13 INFO Executor: Running task 1.0 in stage 41.0 (TID 67)
[2025-07-11T11:47:13.922+0200] {subprocess.py:93} INFO - 25/07/11 11:47:13 INFO FileScanRDD: Reading File path: file:///home/monica.fernandez/practica_creativa/data/simple_flight_delay_features.jsonl.bz2, range: 0-4194304, partition values: [empty row]
[2025-07-11T11:47:13.923+0200] {subprocess.py:93} INFO - 25/07/11 11:47:13 INFO FileScanRDD: Reading File path: file:///home/monica.fernandez/practica_creativa/data/simple_flight_delay_features.jsonl.bz2, range: 4194304-4676447, partition values: [empty row]
[2025-07-11T11:47:13.929+0200] {subprocess.py:93} INFO - 25/07/11 11:47:13 INFO Executor: Running task 0.0 in stage 41.0 (TID 66)
[2025-07-11T11:47:13.940+0200] {subprocess.py:93} INFO - 25/07/11 11:47:13 INFO CodeGenerator: Code generated in 13.668333 ms
[2025-07-11T11:47:14.038+0200] {subprocess.py:93} INFO - 25/07/11 11:47:14 INFO Executor: Finished task 1.0 in stage 41.0 (TID 67). 2031 bytes result sent to driver
[2025-07-11T11:47:14.043+0200] {subprocess.py:93} INFO - 25/07/11 11:47:14 INFO TaskSetManager: Starting task 2.0 in stage 41.0 (TID 68) (138.4.31.14, executor driver, partition 2, ANY, 14843 bytes)
[2025-07-11T11:47:14.046+0200] {subprocess.py:93} INFO - 25/07/11 11:47:14 INFO Executor: Running task 2.0 in stage 41.0 (TID 68)
[2025-07-11T11:47:14.047+0200] {subprocess.py:93} INFO - 25/07/11 11:47:14 INFO BlockManagerInfo: Removed broadcast_30_piece0 on 138.4.31.14:39113 in memory (size: 81.0 KiB, free: 434.1 MiB)
[2025-07-11T11:47:14.053+0200] {subprocess.py:93} INFO - 25/07/11 11:47:14 INFO TaskSetManager: Finished task 1.0 in stage 41.0 (TID 67) in 134 ms on 138.4.31.14 (executor driver) (1/13)
[2025-07-11T11:47:14.105+0200] {subprocess.py:93} INFO - 25/07/11 11:47:14 INFO Executor: Finished task 0.0 in stage 41.0 (TID 66). 1988 bytes result sent to driver
[2025-07-11T11:47:14.115+0200] {subprocess.py:93} INFO - 25/07/11 11:47:14 INFO TaskSetManager: Starting task 3.0 in stage 41.0 (TID 69) (138.4.31.14, executor driver, partition 3, ANY, 14843 bytes)
[2025-07-11T11:47:14.119+0200] {subprocess.py:93} INFO - 25/07/11 11:47:14 INFO TaskSetManager: Finished task 0.0 in stage 41.0 (TID 66) in 198 ms on 138.4.31.14 (executor driver) (2/13)
[2025-07-11T11:47:14.121+0200] {subprocess.py:93} INFO - 25/07/11 11:47:14 INFO Executor: Running task 3.0 in stage 41.0 (TID 69)
[2025-07-11T11:47:14.168+0200] {subprocess.py:93} INFO - 25/07/11 11:47:14 INFO Executor: Finished task 2.0 in stage 41.0 (TID 68). 1945 bytes result sent to driver
[2025-07-11T11:47:14.168+0200] {subprocess.py:93} INFO - 25/07/11 11:47:14 INFO TaskSetManager: Starting task 4.0 in stage 41.0 (TID 70) (138.4.31.14, executor driver, partition 4, ANY, 14843 bytes)
[2025-07-11T11:47:14.169+0200] {subprocess.py:93} INFO - 25/07/11 11:47:14 INFO Executor: Running task 4.0 in stage 41.0 (TID 70)
[2025-07-11T11:47:14.171+0200] {subprocess.py:93} INFO - 25/07/11 11:47:14 INFO TaskSetManager: Finished task 2.0 in stage 41.0 (TID 68) in 127 ms on 138.4.31.14 (executor driver) (3/13)
[2025-07-11T11:47:14.236+0200] {subprocess.py:93} INFO - 25/07/11 11:47:14 INFO Executor: Finished task 3.0 in stage 41.0 (TID 69). 1945 bytes result sent to driver
[2025-07-11T11:47:14.243+0200] {subprocess.py:93} INFO - 25/07/11 11:47:14 INFO TaskSetManager: Starting task 5.0 in stage 41.0 (TID 71) (138.4.31.14, executor driver, partition 5, ANY, 14843 bytes)
[2025-07-11T11:47:14.245+0200] {subprocess.py:93} INFO - 25/07/11 11:47:14 INFO TaskSetManager: Finished task 3.0 in stage 41.0 (TID 69) in 128 ms on 138.4.31.14 (executor driver) (4/13)
[2025-07-11T11:47:14.245+0200] {subprocess.py:93} INFO - 25/07/11 11:47:14 INFO Executor: Running task 5.0 in stage 41.0 (TID 71)
[2025-07-11T11:47:14.322+0200] {subprocess.py:93} INFO - 25/07/11 11:47:14 INFO Executor: Finished task 4.0 in stage 41.0 (TID 70). 1988 bytes result sent to driver
[2025-07-11T11:47:14.323+0200] {subprocess.py:93} INFO - 25/07/11 11:47:14 INFO TaskSetManager: Starting task 6.0 in stage 41.0 (TID 72) (138.4.31.14, executor driver, partition 6, ANY, 14963 bytes)
[2025-07-11T11:47:14.324+0200] {subprocess.py:93} INFO - 25/07/11 11:47:14 INFO TaskSetManager: Finished task 4.0 in stage 41.0 (TID 70) in 155 ms on 138.4.31.14 (executor driver) (5/13)
[2025-07-11T11:47:14.324+0200] {subprocess.py:93} INFO - 25/07/11 11:47:14 INFO Executor: Running task 6.0 in stage 41.0 (TID 72)
[2025-07-11T11:47:14.379+0200] {subprocess.py:93} INFO - 25/07/11 11:47:14 INFO Executor: Finished task 5.0 in stage 41.0 (TID 71). 1945 bytes result sent to driver
[2025-07-11T11:47:14.380+0200] {subprocess.py:93} INFO - 25/07/11 11:47:14 INFO TaskSetManager: Starting task 7.0 in stage 41.0 (TID 73) (138.4.31.14, executor driver, partition 7, ANY, 14843 bytes)
[2025-07-11T11:47:14.385+0200] {subprocess.py:93} INFO - 25/07/11 11:47:14 INFO Executor: Running task 7.0 in stage 41.0 (TID 73)
[2025-07-11T11:47:14.386+0200] {subprocess.py:93} INFO - 25/07/11 11:47:14 INFO TaskSetManager: Finished task 5.0 in stage 41.0 (TID 71) in 143 ms on 138.4.31.14 (executor driver) (6/13)
[2025-07-11T11:47:14.444+0200] {subprocess.py:93} INFO - 25/07/11 11:47:14 INFO Executor: Finished task 6.0 in stage 41.0 (TID 72). 1945 bytes result sent to driver
[2025-07-11T11:47:14.445+0200] {subprocess.py:93} INFO - 25/07/11 11:47:14 INFO TaskSetManager: Starting task 8.0 in stage 41.0 (TID 74) (138.4.31.14, executor driver, partition 8, ANY, 14843 bytes)
[2025-07-11T11:47:14.445+0200] {subprocess.py:93} INFO - 25/07/11 11:47:14 INFO Executor: Running task 8.0 in stage 41.0 (TID 74)
[2025-07-11T11:47:14.445+0200] {subprocess.py:93} INFO - 25/07/11 11:47:14 INFO TaskSetManager: Finished task 6.0 in stage 41.0 (TID 72) in 123 ms on 138.4.31.14 (executor driver) (7/13)
[2025-07-11T11:47:14.538+0200] {subprocess.py:93} INFO - 25/07/11 11:47:14 INFO Executor: Finished task 7.0 in stage 41.0 (TID 73). 1988 bytes result sent to driver
[2025-07-11T11:47:14.540+0200] {subprocess.py:93} INFO - 25/07/11 11:47:14 INFO TaskSetManager: Starting task 9.0 in stage 41.0 (TID 75) (138.4.31.14, executor driver, partition 9, ANY, 14843 bytes)
[2025-07-11T11:47:14.541+0200] {subprocess.py:93} INFO - 25/07/11 11:47:14 INFO TaskSetManager: Finished task 7.0 in stage 41.0 (TID 73) in 159 ms on 138.4.31.14 (executor driver) (8/13)
[2025-07-11T11:47:14.541+0200] {subprocess.py:93} INFO - 25/07/11 11:47:14 INFO Executor: Running task 9.0 in stage 41.0 (TID 75)
[2025-07-11T11:47:14.609+0200] {subprocess.py:93} INFO - 25/07/11 11:47:14 INFO Executor: Finished task 8.0 in stage 41.0 (TID 74). 1988 bytes result sent to driver
[2025-07-11T11:47:14.613+0200] {subprocess.py:93} INFO - 25/07/11 11:47:14 INFO TaskSetManager: Starting task 10.0 in stage 41.0 (TID 76) (138.4.31.14, executor driver, partition 10, ANY, 14843 bytes)
[2025-07-11T11:47:14.613+0200] {subprocess.py:93} INFO - 25/07/11 11:47:14 INFO TaskSetManager: Finished task 8.0 in stage 41.0 (TID 74) in 169 ms on 138.4.31.14 (executor driver) (9/13)
[2025-07-11T11:47:14.614+0200] {subprocess.py:93} INFO - 25/07/11 11:47:14 INFO Executor: Running task 10.0 in stage 41.0 (TID 76)
[2025-07-11T11:47:14.672+0200] {subprocess.py:93} INFO - 25/07/11 11:47:14 INFO Executor: Finished task 9.0 in stage 41.0 (TID 75). 1945 bytes result sent to driver
[2025-07-11T11:47:14.673+0200] {subprocess.py:93} INFO - 25/07/11 11:47:14 INFO TaskSetManager: Starting task 11.0 in stage 41.0 (TID 77) (138.4.31.14, executor driver, partition 11, ANY, 14959 bytes)
[2025-07-11T11:47:14.674+0200] {subprocess.py:93} INFO - 25/07/11 11:47:14 INFO Executor: Running task 11.0 in stage 41.0 (TID 77)
[2025-07-11T11:47:14.676+0200] {subprocess.py:93} INFO - 25/07/11 11:47:14 INFO TaskSetManager: Finished task 9.0 in stage 41.0 (TID 75) in 138 ms on 138.4.31.14 (executor driver) (10/13)
[2025-07-11T11:47:14.725+0200] {subprocess.py:93} INFO - 25/07/11 11:47:14 INFO Executor: Finished task 10.0 in stage 41.0 (TID 76). 1945 bytes result sent to driver
[2025-07-11T11:47:14.726+0200] {subprocess.py:93} INFO - 25/07/11 11:47:14 INFO TaskSetManager: Starting task 12.0 in stage 41.0 (TID 78) (138.4.31.14, executor driver, partition 12, ANY, 14843 bytes)
[2025-07-11T11:47:14.726+0200] {subprocess.py:93} INFO - 25/07/11 11:47:14 INFO TaskSetManager: Finished task 10.0 in stage 41.0 (TID 76) in 105 ms on 138.4.31.14 (executor driver) (11/13)
[2025-07-11T11:47:14.730+0200] {subprocess.py:93} INFO - 25/07/11 11:47:14 INFO Executor: Running task 12.0 in stage 41.0 (TID 78)
[2025-07-11T11:47:14.851+0200] {subprocess.py:93} INFO - 25/07/11 11:47:14 INFO Executor: Finished task 11.0 in stage 41.0 (TID 77). 1945 bytes result sent to driver
[2025-07-11T11:47:14.851+0200] {subprocess.py:93} INFO - 25/07/11 11:47:14 INFO TaskSetManager: Finished task 11.0 in stage 41.0 (TID 77) in 170 ms on 138.4.31.14 (executor driver) (12/13)
[2025-07-11T11:47:14.865+0200] {subprocess.py:93} INFO - 25/07/11 11:47:14 INFO Executor: Finished task 12.0 in stage 41.0 (TID 78). 1945 bytes result sent to driver
[2025-07-11T11:47:14.865+0200] {subprocess.py:93} INFO - 25/07/11 11:47:14 INFO TaskSetManager: Finished task 12.0 in stage 41.0 (TID 78) in 149 ms on 138.4.31.14 (executor driver) (13/13)
[2025-07-11T11:47:14.865+0200] {subprocess.py:93} INFO - 25/07/11 11:47:14 INFO TaskSchedulerImpl: Removed TaskSet 41.0, whose tasks have all completed, from pool
[2025-07-11T11:47:14.881+0200] {subprocess.py:93} INFO - 25/07/11 11:47:14 INFO DAGScheduler: ShuffleMapStage 41 (collect at StringIndexer.scala:204) finished in 0,953 s
[2025-07-11T11:47:14.881+0200] {subprocess.py:93} INFO - 25/07/11 11:47:14 INFO DAGScheduler: looking for newly runnable stages
[2025-07-11T11:47:14.881+0200] {subprocess.py:93} INFO - 25/07/11 11:47:14 INFO DAGScheduler: running: Set(ShuffleMapStage 40)
[2025-07-11T11:47:14.884+0200] {subprocess.py:93} INFO - 25/07/11 11:47:14 INFO DAGScheduler: waiting: Set()
[2025-07-11T11:47:14.887+0200] {subprocess.py:93} INFO - 25/07/11 11:47:14 INFO DAGScheduler: failed: Set()
[2025-07-11T11:47:14.888+0200] {subprocess.py:93} INFO - 25/07/11 11:47:14 INFO ShufflePartitionsUtil: For shuffle(13), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576
[2025-07-11T11:47:14.928+0200] {subprocess.py:93} INFO - 25/07/11 11:47:14 INFO SparkContext: Starting job: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264
[2025-07-11T11:47:14.947+0200] {subprocess.py:93} INFO - 25/07/11 11:47:14 INFO DAGScheduler: Got job 27 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) with 1 output partitions
[2025-07-11T11:47:14.947+0200] {subprocess.py:93} INFO - 25/07/11 11:47:14 INFO DAGScheduler: Final stage: ResultStage 43 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264)
[2025-07-11T11:47:14.955+0200] {subprocess.py:93} INFO - 25/07/11 11:47:14 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 42)
[2025-07-11T11:47:14.956+0200] {subprocess.py:93} INFO - 25/07/11 11:47:14 INFO DAGScheduler: Missing parents: List()
[2025-07-11T11:47:14.956+0200] {subprocess.py:93} INFO - 25/07/11 11:47:14 INFO DAGScheduler: Submitting ResultStage 43 (MapPartitionsRDD[80] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264), which has no missing parents
[2025-07-11T11:47:14.957+0200] {subprocess.py:93} INFO - 25/07/11 11:47:14 INFO MemoryStore: Block broadcast_34 stored as values in memory (estimated size 8.2 KiB, free 424.9 MiB)
[2025-07-11T11:47:14.958+0200] {subprocess.py:93} INFO - 25/07/11 11:47:14 INFO MemoryStore: Block broadcast_34_piece0 stored as bytes in memory (estimated size 4.2 KiB, free 424.9 MiB)
[2025-07-11T11:47:14.958+0200] {subprocess.py:93} INFO - 25/07/11 11:47:14 INFO BlockManagerInfo: Added broadcast_34_piece0 in memory on 138.4.31.14:39113 (size: 4.2 KiB, free: 434.1 MiB)
[2025-07-11T11:47:14.959+0200] {subprocess.py:93} INFO - 25/07/11 11:47:14 INFO SparkContext: Created broadcast 34 from broadcast at DAGScheduler.scala:1585
[2025-07-11T11:47:14.959+0200] {subprocess.py:93} INFO - 25/07/11 11:47:14 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 43 (MapPartitionsRDD[80] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) (first 15 tasks are for partitions Vector(0))
[2025-07-11T11:47:14.960+0200] {subprocess.py:93} INFO - 25/07/11 11:47:14 INFO TaskSchedulerImpl: Adding task set 43.0 with 1 tasks resource profile 0
[2025-07-11T11:47:14.961+0200] {subprocess.py:93} INFO - 25/07/11 11:47:14 INFO TaskSetManager: Starting task 0.0 in stage 43.0 (TID 79) (138.4.31.14, executor driver, partition 0, NODE_LOCAL, 12968 bytes)
[2025-07-11T11:47:14.961+0200] {subprocess.py:93} INFO - 25/07/11 11:47:14 INFO Executor: Running task 0.0 in stage 43.0 (TID 79)
[2025-07-11T11:47:14.968+0200] {subprocess.py:93} INFO - 25/07/11 11:47:14 INFO ShuffleBlockFetcherIterator: Getting 13 (196.8 KiB) non-empty blocks including 13 (196.8 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2025-07-11T11:47:14.969+0200] {subprocess.py:93} INFO - 25/07/11 11:47:14 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 3 ms
[2025-07-11T11:47:15.038+0200] {subprocess.py:93} INFO - 25/07/11 11:47:15 INFO Executor: Finished task 0.0 in stage 43.0 (TID 79). 40443 bytes result sent to driver
[2025-07-11T11:47:15.038+0200] {subprocess.py:93} INFO - 25/07/11 11:47:15 INFO TaskSetManager: Finished task 0.0 in stage 43.0 (TID 79) in 78 ms on 138.4.31.14 (executor driver) (1/1)
[2025-07-11T11:47:15.038+0200] {subprocess.py:93} INFO - 25/07/11 11:47:15 INFO TaskSchedulerImpl: Removed TaskSet 43.0, whose tasks have all completed, from pool
[2025-07-11T11:47:15.043+0200] {subprocess.py:93} INFO - 25/07/11 11:47:15 INFO DAGScheduler: ResultStage 43 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) finished in 0,086 s
[2025-07-11T11:47:15.043+0200] {subprocess.py:93} INFO - 25/07/11 11:47:15 INFO DAGScheduler: Job 27 is finished. Cancelling potential speculative or zombie tasks for this job
[2025-07-11T11:47:15.044+0200] {subprocess.py:93} INFO - 25/07/11 11:47:15 INFO TaskSchedulerImpl: Killing all running tasks in stage 43: Stage finished
[2025-07-11T11:47:15.044+0200] {subprocess.py:93} INFO - 25/07/11 11:47:15 INFO DAGScheduler: Job 27 finished: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264, took 0,114571 s
[2025-07-11T11:47:15.051+0200] {subprocess.py:93} INFO - 25/07/11 11:47:15 INFO MemoryStore: Block broadcast_35 stored as values in memory (estimated size 4.3 MiB, free 420.6 MiB)
[2025-07-11T11:47:15.057+0200] {subprocess.py:93} INFO - 25/07/11 11:47:15 INFO MemoryStore: Block broadcast_35_piece0 stored as bytes in memory (estimated size 89.4 KiB, free 420.6 MiB)
[2025-07-11T11:47:15.057+0200] {subprocess.py:93} INFO - 25/07/11 11:47:15 INFO BlockManagerInfo: Added broadcast_35_piece0 in memory on 138.4.31.14:39113 (size: 89.4 KiB, free: 434.0 MiB)
[2025-07-11T11:47:15.058+0200] {subprocess.py:93} INFO - 25/07/11 11:47:15 INFO SparkContext: Created broadcast 35 from $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264
[2025-07-11T11:47:15.387+0200] {subprocess.py:93} INFO - 25/07/11 11:47:15 INFO BlockManagerInfo: Removed broadcast_34_piece0 on 138.4.31.14:39113 in memory (size: 4.2 KiB, free: 434.0 MiB)
[2025-07-11T11:47:15.736+0200] {subprocess.py:93} INFO - 25/07/11 11:47:15 INFO Executor: Finished task 1.0 in stage 40.0 (TID 65). 2031 bytes result sent to driver
[2025-07-11T11:47:15.737+0200] {subprocess.py:93} INFO - 25/07/11 11:47:15 INFO TaskSetManager: Finished task 1.0 in stage 40.0 (TID 65) in 1842 ms on 138.4.31.14 (executor driver) (1/2)
[2025-07-11T11:47:22.569+0200] {subprocess.py:93} INFO - 25/07/11 11:47:22 INFO Executor: Finished task 0.0 in stage 40.0 (TID 64). 2031 bytes result sent to driver
[2025-07-11T11:47:22.570+0200] {subprocess.py:93} INFO - 25/07/11 11:47:22 INFO TaskSetManager: Finished task 0.0 in stage 40.0 (TID 64) in 8676 ms on 138.4.31.14 (executor driver) (2/2)
[2025-07-11T11:47:22.570+0200] {subprocess.py:93} INFO - 25/07/11 11:47:22 INFO TaskSchedulerImpl: Removed TaskSet 40.0, whose tasks have all completed, from pool
[2025-07-11T11:47:22.571+0200] {subprocess.py:93} INFO - 25/07/11 11:47:22 INFO DAGScheduler: ShuffleMapStage 40 (collect at StringIndexer.scala:204) finished in 8,681 s
[2025-07-11T11:47:22.571+0200] {subprocess.py:93} INFO - 25/07/11 11:47:22 INFO DAGScheduler: looking for newly runnable stages
[2025-07-11T11:47:22.571+0200] {subprocess.py:93} INFO - 25/07/11 11:47:22 INFO DAGScheduler: running: Set()
[2025-07-11T11:47:22.571+0200] {subprocess.py:93} INFO - 25/07/11 11:47:22 INFO DAGScheduler: waiting: Set()
[2025-07-11T11:47:22.574+0200] {subprocess.py:93} INFO - 25/07/11 11:47:22 INFO DAGScheduler: failed: Set()
[2025-07-11T11:47:22.577+0200] {subprocess.py:93} INFO - 25/07/11 11:47:22 INFO ShufflePartitionsUtil: For shuffle(12), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576
[2025-07-11T11:47:22.599+0200] {subprocess.py:93} INFO - 25/07/11 11:47:22 INFO CodeGenerator: Code generated in 7.659061 ms
[2025-07-11T11:47:22.603+0200] {subprocess.py:93} INFO - 25/07/11 11:47:22 INFO DAGScheduler: Registering RDD 84 (collect at StringIndexer.scala:204) as input to shuffle 14
[2025-07-11T11:47:22.604+0200] {subprocess.py:93} INFO - 25/07/11 11:47:22 INFO DAGScheduler: Got map stage job 28 (collect at StringIndexer.scala:204) with 1 output partitions
[2025-07-11T11:47:22.604+0200] {subprocess.py:93} INFO - 25/07/11 11:47:22 INFO DAGScheduler: Final stage: ShuffleMapStage 45 (collect at StringIndexer.scala:204)
[2025-07-11T11:47:22.604+0200] {subprocess.py:93} INFO - 25/07/11 11:47:22 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 44)
[2025-07-11T11:47:22.604+0200] {subprocess.py:93} INFO - 25/07/11 11:47:22 INFO DAGScheduler: Missing parents: List()
[2025-07-11T11:47:22.605+0200] {subprocess.py:93} INFO - 25/07/11 11:47:22 INFO DAGScheduler: Submitting ShuffleMapStage 45 (MapPartitionsRDD[84] at collect at StringIndexer.scala:204), which has no missing parents
[2025-07-11T11:47:22.608+0200] {subprocess.py:93} INFO - 25/07/11 11:47:22 INFO MemoryStore: Block broadcast_36 stored as values in memory (estimated size 50.8 KiB, free 420.5 MiB)
[2025-07-11T11:47:22.609+0200] {subprocess.py:93} INFO - 25/07/11 11:47:22 INFO MemoryStore: Block broadcast_36_piece0 stored as bytes in memory (estimated size 24.1 KiB, free 420.5 MiB)
[2025-07-11T11:47:22.609+0200] {subprocess.py:93} INFO - 25/07/11 11:47:22 INFO BlockManagerInfo: Added broadcast_36_piece0 in memory on 138.4.31.14:39113 (size: 24.1 KiB, free: 434.0 MiB)
[2025-07-11T11:47:22.609+0200] {subprocess.py:93} INFO - 25/07/11 11:47:22 INFO SparkContext: Created broadcast 36 from broadcast at DAGScheduler.scala:1585
[2025-07-11T11:47:22.610+0200] {subprocess.py:93} INFO - 25/07/11 11:47:22 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 45 (MapPartitionsRDD[84] at collect at StringIndexer.scala:204) (first 15 tasks are for partitions Vector(0))
[2025-07-11T11:47:22.610+0200] {subprocess.py:93} INFO - 25/07/11 11:47:22 INFO TaskSchedulerImpl: Adding task set 45.0 with 1 tasks resource profile 0
[2025-07-11T11:47:22.611+0200] {subprocess.py:93} INFO - 25/07/11 11:47:22 INFO TaskSetManager: Starting task 0.0 in stage 45.0 (TID 80) (138.4.31.14, executor driver, partition 0, NODE_LOCAL, 12957 bytes)
[2025-07-11T11:47:22.611+0200] {subprocess.py:93} INFO - 25/07/11 11:47:22 INFO Executor: Running task 0.0 in stage 45.0 (TID 80)
[2025-07-11T11:47:22.616+0200] {subprocess.py:93} INFO - 25/07/11 11:47:22 INFO ShuffleBlockFetcherIterator: Getting 2 (1220.4 KiB) non-empty blocks including 2 (1220.4 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2025-07-11T11:47:22.617+0200] {subprocess.py:93} INFO - 25/07/11 11:47:22 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2025-07-11T11:47:22.624+0200] {subprocess.py:93} INFO - 25/07/11 11:47:22 INFO CodeGenerator: Code generated in 6.865793 ms
[2025-07-11T11:47:22.905+0200] {subprocess.py:93} INFO - 25/07/11 11:47:22 INFO Executor: Finished task 0.0 in stage 45.0 (TID 80). 6543 bytes result sent to driver
[2025-07-11T11:47:22.906+0200] {subprocess.py:93} INFO - 25/07/11 11:47:22 INFO TaskSetManager: Finished task 0.0 in stage 45.0 (TID 80) in 295 ms on 138.4.31.14 (executor driver) (1/1)
[2025-07-11T11:47:22.906+0200] {subprocess.py:93} INFO - 25/07/11 11:47:22 INFO TaskSchedulerImpl: Removed TaskSet 45.0, whose tasks have all completed, from pool
[2025-07-11T11:47:22.908+0200] {subprocess.py:93} INFO - 25/07/11 11:47:22 INFO DAGScheduler: ShuffleMapStage 45 (collect at StringIndexer.scala:204) finished in 0,302 s
[2025-07-11T11:47:22.908+0200] {subprocess.py:93} INFO - 25/07/11 11:47:22 INFO DAGScheduler: looking for newly runnable stages
[2025-07-11T11:47:22.908+0200] {subprocess.py:93} INFO - 25/07/11 11:47:22 INFO DAGScheduler: running: Set()
[2025-07-11T11:47:22.908+0200] {subprocess.py:93} INFO - 25/07/11 11:47:22 INFO DAGScheduler: waiting: Set()
[2025-07-11T11:47:22.908+0200] {subprocess.py:93} INFO - 25/07/11 11:47:22 INFO DAGScheduler: failed: Set()
[2025-07-11T11:47:22.928+0200] {subprocess.py:93} INFO - 25/07/11 11:47:22 INFO SparkContext: Starting job: collect at StringIndexer.scala:204
[2025-07-11T11:47:22.932+0200] {subprocess.py:93} INFO - 25/07/11 11:47:22 INFO DAGScheduler: Got job 29 (collect at StringIndexer.scala:204) with 1 output partitions
[2025-07-11T11:47:22.932+0200] {subprocess.py:93} INFO - 25/07/11 11:47:22 INFO DAGScheduler: Final stage: ResultStage 48 (collect at StringIndexer.scala:204)
[2025-07-11T11:47:22.932+0200] {subprocess.py:93} INFO - 25/07/11 11:47:22 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 47)
[2025-07-11T11:47:22.932+0200] {subprocess.py:93} INFO - 25/07/11 11:47:22 INFO DAGScheduler: Missing parents: List()
[2025-07-11T11:47:22.932+0200] {subprocess.py:93} INFO - 25/07/11 11:47:22 INFO DAGScheduler: Submitting ResultStage 48 (MapPartitionsRDD[87] at collect at StringIndexer.scala:204), which has no missing parents
[2025-07-11T11:47:22.933+0200] {subprocess.py:93} INFO - 25/07/11 11:47:22 INFO MemoryStore: Block broadcast_37 stored as values in memory (estimated size 50.0 KiB, free 420.4 MiB)
[2025-07-11T11:47:22.933+0200] {subprocess.py:93} INFO - 25/07/11 11:47:22 INFO MemoryStore: Block broadcast_37_piece0 stored as bytes in memory (estimated size 23.8 KiB, free 420.4 MiB)
[2025-07-11T11:47:22.934+0200] {subprocess.py:93} INFO - 25/07/11 11:47:22 INFO BlockManagerInfo: Added broadcast_37_piece0 in memory on 138.4.31.14:39113 (size: 23.8 KiB, free: 433.9 MiB)
[2025-07-11T11:47:22.935+0200] {subprocess.py:93} INFO - 25/07/11 11:47:22 INFO SparkContext: Created broadcast 37 from broadcast at DAGScheduler.scala:1585
[2025-07-11T11:47:22.935+0200] {subprocess.py:93} INFO - 25/07/11 11:47:22 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 48 (MapPartitionsRDD[87] at collect at StringIndexer.scala:204) (first 15 tasks are for partitions Vector(0))
[2025-07-11T11:47:22.935+0200] {subprocess.py:93} INFO - 25/07/11 11:47:22 INFO TaskSchedulerImpl: Adding task set 48.0 with 1 tasks resource profile 0
[2025-07-11T11:47:22.936+0200] {subprocess.py:93} INFO - 25/07/11 11:47:22 INFO TaskSetManager: Starting task 0.0 in stage 48.0 (TID 81) (138.4.31.14, executor driver, partition 0, NODE_LOCAL, 12952 bytes)
[2025-07-11T11:47:22.936+0200] {subprocess.py:93} INFO - 25/07/11 11:47:22 INFO Executor: Running task 0.0 in stage 48.0 (TID 81)
[2025-07-11T11:47:22.942+0200] {subprocess.py:93} INFO - 25/07/11 11:47:22 INFO ShuffleBlockFetcherIterator: Getting 1 (46.5 KiB) non-empty blocks including 1 (46.5 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2025-07-11T11:47:22.942+0200] {subprocess.py:93} INFO - 25/07/11 11:47:22 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2025-07-11T11:47:22.992+0200] {subprocess.py:93} INFO - 25/07/11 11:47:22 INFO Executor: Finished task 0.0 in stage 48.0 (TID 81). 53379 bytes result sent to driver
[2025-07-11T11:47:22.993+0200] {subprocess.py:93} INFO - 25/07/11 11:47:22 INFO TaskSetManager: Finished task 0.0 in stage 48.0 (TID 81) in 58 ms on 138.4.31.14 (executor driver) (1/1)
[2025-07-11T11:47:22.993+0200] {subprocess.py:93} INFO - 25/07/11 11:47:22 INFO TaskSchedulerImpl: Removed TaskSet 48.0, whose tasks have all completed, from pool
[2025-07-11T11:47:22.994+0200] {subprocess.py:93} INFO - 25/07/11 11:47:22 INFO DAGScheduler: ResultStage 48 (collect at StringIndexer.scala:204) finished in 0,062 s
[2025-07-11T11:47:22.994+0200] {subprocess.py:93} INFO - 25/07/11 11:47:22 INFO DAGScheduler: Job 29 is finished. Cancelling potential speculative or zombie tasks for this job
[2025-07-11T11:47:22.994+0200] {subprocess.py:93} INFO - 25/07/11 11:47:22 INFO TaskSchedulerImpl: Killing all running tasks in stage 48: Stage finished
[2025-07-11T11:47:22.995+0200] {subprocess.py:93} INFO - 25/07/11 11:47:22 INFO DAGScheduler: Job 29 finished: collect at StringIndexer.scala:204, took 0,066361 s
[2025-07-11T11:47:23.083+0200] {subprocess.py:93} INFO - 25/07/11 11:47:23 INFO FileSystemOverwrite: Path /home/monica.fernandez/practica_creativa/models/string_indexer_model_Route.bin already exists. It will be overwritten.
[2025-07-11T11:47:23.103+0200] {subprocess.py:93} INFO - 25/07/11 11:47:23 INFO HadoopMapRedCommitProtocol: Using output committer class org.apache.hadoop.mapred.FileOutputCommitter
[2025-07-11T11:47:23.103+0200] {subprocess.py:93} INFO - 25/07/11 11:47:23 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2025-07-11T11:47:23.103+0200] {subprocess.py:93} INFO - 25/07/11 11:47:23 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2025-07-11T11:47:23.134+0200] {subprocess.py:93} INFO - 25/07/11 11:47:23 INFO SparkContext: Starting job: runJob at SparkHadoopWriter.scala:83
[2025-07-11T11:47:23.134+0200] {subprocess.py:93} INFO - 25/07/11 11:47:23 INFO DAGScheduler: Got job 30 (runJob at SparkHadoopWriter.scala:83) with 1 output partitions
[2025-07-11T11:47:23.134+0200] {subprocess.py:93} INFO - 25/07/11 11:47:23 INFO DAGScheduler: Final stage: ResultStage 49 (runJob at SparkHadoopWriter.scala:83)
[2025-07-11T11:47:23.134+0200] {subprocess.py:93} INFO - 25/07/11 11:47:23 INFO DAGScheduler: Parents of final stage: List()
[2025-07-11T11:47:23.134+0200] {subprocess.py:93} INFO - 25/07/11 11:47:23 INFO DAGScheduler: Missing parents: List()
[2025-07-11T11:47:23.134+0200] {subprocess.py:93} INFO - 25/07/11 11:47:23 INFO DAGScheduler: Submitting ResultStage 49 (MapPartitionsRDD[89] at saveAsTextFile at ReadWrite.scala:413), which has no missing parents
[2025-07-11T11:47:23.143+0200] {subprocess.py:93} INFO - 25/07/11 11:47:23 INFO MemoryStore: Block broadcast_38 stored as values in memory (estimated size 101.3 KiB, free 420.3 MiB)
[2025-07-11T11:47:23.144+0200] {subprocess.py:93} INFO - 25/07/11 11:47:23 INFO MemoryStore: Block broadcast_38_piece0 stored as bytes in memory (estimated size 36.5 KiB, free 420.3 MiB)
[2025-07-11T11:47:23.145+0200] {subprocess.py:93} INFO - 25/07/11 11:47:23 INFO BlockManagerInfo: Added broadcast_38_piece0 in memory on 138.4.31.14:39113 (size: 36.5 KiB, free: 433.9 MiB)
[2025-07-11T11:47:23.145+0200] {subprocess.py:93} INFO - 25/07/11 11:47:23 INFO SparkContext: Created broadcast 38 from broadcast at DAGScheduler.scala:1585
[2025-07-11T11:47:23.145+0200] {subprocess.py:93} INFO - 25/07/11 11:47:23 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 49 (MapPartitionsRDD[89] at saveAsTextFile at ReadWrite.scala:413) (first 15 tasks are for partitions Vector(0))
[2025-07-11T11:47:23.146+0200] {subprocess.py:93} INFO - 25/07/11 11:47:23 INFO TaskSchedulerImpl: Adding task set 49.0 with 1 tasks resource profile 0
[2025-07-11T11:47:23.146+0200] {subprocess.py:93} INFO - 25/07/11 11:47:23 INFO TaskSetManager: Starting task 0.0 in stage 49.0 (TID 82) (138.4.31.14, executor driver, partition 0, PROCESS_LOCAL, 13278 bytes)
[2025-07-11T11:47:23.147+0200] {subprocess.py:93} INFO - 25/07/11 11:47:23 INFO Executor: Running task 0.0 in stage 49.0 (TID 82)
[2025-07-11T11:47:23.152+0200] {subprocess.py:93} INFO - 25/07/11 11:47:23 INFO HadoopMapRedCommitProtocol: Using output committer class org.apache.hadoop.mapred.FileOutputCommitter
[2025-07-11T11:47:23.152+0200] {subprocess.py:93} INFO - 25/07/11 11:47:23 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2025-07-11T11:47:23.152+0200] {subprocess.py:93} INFO - 25/07/11 11:47:23 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2025-07-11T11:47:23.180+0200] {subprocess.py:93} INFO - 25/07/11 11:47:23 INFO FileOutputCommitter: Saved output of task 'attempt_202507111147238100525293016524562_0089_m_000000_0' to file:/home/monica.fernandez/practica_creativa/models/string_indexer_model_Route.bin/metadata/_temporary/0/task_202507111147238100525293016524562_0089_m_000000
[2025-07-11T11:47:23.180+0200] {subprocess.py:93} INFO - 25/07/11 11:47:23 INFO SparkHadoopMapRedUtil: attempt_202507111147238100525293016524562_0089_m_000000_0: Committed. Elapsed time: 1 ms.
[2025-07-11T11:47:23.181+0200] {subprocess.py:93} INFO - 25/07/11 11:47:23 INFO Executor: Finished task 0.0 in stage 49.0 (TID 82). 1170 bytes result sent to driver
[2025-07-11T11:47:23.182+0200] {subprocess.py:93} INFO - 25/07/11 11:47:23 INFO TaskSetManager: Finished task 0.0 in stage 49.0 (TID 82) in 36 ms on 138.4.31.14 (executor driver) (1/1)
[2025-07-11T11:47:23.182+0200] {subprocess.py:93} INFO - 25/07/11 11:47:23 INFO TaskSchedulerImpl: Removed TaskSet 49.0, whose tasks have all completed, from pool
[2025-07-11T11:47:23.182+0200] {subprocess.py:93} INFO - 25/07/11 11:47:23 INFO DAGScheduler: ResultStage 49 (runJob at SparkHadoopWriter.scala:83) finished in 0,047 s
[2025-07-11T11:47:23.182+0200] {subprocess.py:93} INFO - 25/07/11 11:47:23 INFO DAGScheduler: Job 30 is finished. Cancelling potential speculative or zombie tasks for this job
[2025-07-11T11:47:23.182+0200] {subprocess.py:93} INFO - 25/07/11 11:47:23 INFO TaskSchedulerImpl: Killing all running tasks in stage 49: Stage finished
[2025-07-11T11:47:23.183+0200] {subprocess.py:93} INFO - 25/07/11 11:47:23 INFO DAGScheduler: Job 30 finished: runJob at SparkHadoopWriter.scala:83, took 0,048877 s
[2025-07-11T11:47:23.183+0200] {subprocess.py:93} INFO - 25/07/11 11:47:23 INFO SparkHadoopWriter: Start to commit write Job job_202507111147238100525293016524562_0089.
[2025-07-11T11:47:23.204+0200] {subprocess.py:93} INFO - 25/07/11 11:47:23 INFO SparkHadoopWriter: Write Job job_202507111147238100525293016524562_0089 committed. Elapsed time: 20 ms.
[2025-07-11T11:47:23.237+0200] {subprocess.py:93} INFO - 25/07/11 11:47:23 INFO DAGScheduler: Registering RDD 92 (parquet at StringIndexer.scala:499) as input to shuffle 15
[2025-07-11T11:47:23.237+0200] {subprocess.py:93} INFO - 25/07/11 11:47:23 INFO DAGScheduler: Got map stage job 31 (parquet at StringIndexer.scala:499) with 1 output partitions
[2025-07-11T11:47:23.237+0200] {subprocess.py:93} INFO - 25/07/11 11:47:23 INFO DAGScheduler: Final stage: ShuffleMapStage 50 (parquet at StringIndexer.scala:499)
[2025-07-11T11:47:23.237+0200] {subprocess.py:93} INFO - 25/07/11 11:47:23 INFO DAGScheduler: Parents of final stage: List()
[2025-07-11T11:47:23.237+0200] {subprocess.py:93} INFO - 25/07/11 11:47:23 INFO DAGScheduler: Missing parents: List()
[2025-07-11T11:47:23.237+0200] {subprocess.py:93} INFO - 25/07/11 11:47:23 INFO DAGScheduler: Submitting ShuffleMapStage 50 (MapPartitionsRDD[92] at parquet at StringIndexer.scala:499), which has no missing parents
[2025-07-11T11:47:23.238+0200] {subprocess.py:93} INFO - 25/07/11 11:47:23 INFO MemoryStore: Block broadcast_39 stored as values in memory (estimated size 8.1 KiB, free 420.3 MiB)
[2025-07-11T11:47:23.239+0200] {subprocess.py:93} INFO - 25/07/11 11:47:23 INFO MemoryStore: Block broadcast_39_piece0 stored as bytes in memory (estimated size 4.4 KiB, free 420.3 MiB)
[2025-07-11T11:47:23.239+0200] {subprocess.py:93} INFO - 25/07/11 11:47:23 INFO BlockManagerInfo: Added broadcast_39_piece0 in memory on 138.4.31.14:39113 (size: 4.4 KiB, free: 433.9 MiB)
[2025-07-11T11:47:23.240+0200] {subprocess.py:93} INFO - 25/07/11 11:47:23 INFO SparkContext: Created broadcast 39 from broadcast at DAGScheduler.scala:1585
[2025-07-11T11:47:23.240+0200] {subprocess.py:93} INFO - 25/07/11 11:47:23 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 50 (MapPartitionsRDD[92] at parquet at StringIndexer.scala:499) (first 15 tasks are for partitions Vector(0))
[2025-07-11T11:47:23.240+0200] {subprocess.py:93} INFO - 25/07/11 11:47:23 INFO TaskSchedulerImpl: Adding task set 50.0 with 1 tasks resource profile 0
[2025-07-11T11:47:23.241+0200] {subprocess.py:93} INFO - 25/07/11 11:47:23 INFO TaskSetManager: Starting task 0.0 in stage 50.0 (TID 83) (138.4.31.14, executor driver, partition 0, PROCESS_LOCAL, 80641 bytes)
[2025-07-11T11:47:23.241+0200] {subprocess.py:93} INFO - 25/07/11 11:47:23 INFO Executor: Running task 0.0 in stage 50.0 (TID 83)
[2025-07-11T11:47:23.253+0200] {subprocess.py:93} INFO - 25/07/11 11:47:23 INFO Executor: Finished task 0.0 in stage 50.0 (TID 83). 1628 bytes result sent to driver
[2025-07-11T11:47:23.253+0200] {subprocess.py:93} INFO - 25/07/11 11:47:23 INFO TaskSetManager: Finished task 0.0 in stage 50.0 (TID 83) in 5 ms on 138.4.31.14 (executor driver) (1/1)
[2025-07-11T11:47:23.253+0200] {subprocess.py:93} INFO - 25/07/11 11:47:23 INFO TaskSchedulerImpl: Removed TaskSet 50.0, whose tasks have all completed, from pool
[2025-07-11T11:47:23.253+0200] {subprocess.py:93} INFO - 25/07/11 11:47:23 INFO DAGScheduler: ShuffleMapStage 50 (parquet at StringIndexer.scala:499) finished in 0,008 s
[2025-07-11T11:47:23.254+0200] {subprocess.py:93} INFO - 25/07/11 11:47:23 INFO DAGScheduler: looking for newly runnable stages
[2025-07-11T11:47:23.254+0200] {subprocess.py:93} INFO - 25/07/11 11:47:23 INFO DAGScheduler: running: Set()
[2025-07-11T11:47:23.254+0200] {subprocess.py:93} INFO - 25/07/11 11:47:23 INFO DAGScheduler: waiting: Set()
[2025-07-11T11:47:23.254+0200] {subprocess.py:93} INFO - 25/07/11 11:47:23 INFO DAGScheduler: failed: Set()
[2025-07-11T11:47:23.254+0200] {subprocess.py:93} INFO - 25/07/11 11:47:23 INFO ParquetUtils: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
[2025-07-11T11:47:23.255+0200] {subprocess.py:93} INFO - 25/07/11 11:47:23 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2025-07-11T11:47:23.255+0200] {subprocess.py:93} INFO - 25/07/11 11:47:23 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2025-07-11T11:47:23.256+0200] {subprocess.py:93} INFO - 25/07/11 11:47:23 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[2025-07-11T11:47:23.256+0200] {subprocess.py:93} INFO - 25/07/11 11:47:23 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2025-07-11T11:47:23.256+0200] {subprocess.py:93} INFO - 25/07/11 11:47:23 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2025-07-11T11:47:23.256+0200] {subprocess.py:93} INFO - 25/07/11 11:47:23 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[2025-07-11T11:47:23.280+0200] {subprocess.py:93} INFO - 25/07/11 11:47:23 INFO SparkContext: Starting job: parquet at StringIndexer.scala:499
[2025-07-11T11:47:23.281+0200] {subprocess.py:93} INFO - 25/07/11 11:47:23 INFO DAGScheduler: Got job 32 (parquet at StringIndexer.scala:499) with 1 output partitions
[2025-07-11T11:47:23.282+0200] {subprocess.py:93} INFO - 25/07/11 11:47:23 INFO DAGScheduler: Final stage: ResultStage 52 (parquet at StringIndexer.scala:499)
[2025-07-11T11:47:23.282+0200] {subprocess.py:93} INFO - 25/07/11 11:47:23 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 51)
[2025-07-11T11:47:23.282+0200] {subprocess.py:93} INFO - 25/07/11 11:47:23 INFO DAGScheduler: Missing parents: List()
[2025-07-11T11:47:23.282+0200] {subprocess.py:93} INFO - 25/07/11 11:47:23 INFO DAGScheduler: Submitting ResultStage 52 (MapPartitionsRDD[94] at parquet at StringIndexer.scala:499), which has no missing parents
[2025-07-11T11:47:23.299+0200] {subprocess.py:93} INFO - 25/07/11 11:47:23 INFO MemoryStore: Block broadcast_40 stored as values in memory (estimated size 230.2 KiB, free 420.0 MiB)
[2025-07-11T11:47:23.299+0200] {subprocess.py:93} INFO - 25/07/11 11:47:23 INFO MemoryStore: Block broadcast_40_piece0 stored as bytes in memory (estimated size 81.0 KiB, free 420.0 MiB)
[2025-07-11T11:47:23.299+0200] {subprocess.py:93} INFO - 25/07/11 11:47:23 INFO BlockManagerInfo: Added broadcast_40_piece0 in memory on 138.4.31.14:39113 (size: 81.0 KiB, free: 433.8 MiB)
[2025-07-11T11:47:23.299+0200] {subprocess.py:93} INFO - 25/07/11 11:47:23 INFO SparkContext: Created broadcast 40 from broadcast at DAGScheduler.scala:1585
[2025-07-11T11:47:23.299+0200] {subprocess.py:93} INFO - 25/07/11 11:47:23 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 52 (MapPartitionsRDD[94] at parquet at StringIndexer.scala:499) (first 15 tasks are for partitions Vector(0))
[2025-07-11T11:47:23.299+0200] {subprocess.py:93} INFO - 25/07/11 11:47:23 INFO TaskSchedulerImpl: Adding task set 52.0 with 1 tasks resource profile 0
[2025-07-11T11:47:23.300+0200] {subprocess.py:93} INFO - 25/07/11 11:47:23 INFO TaskSetManager: Starting task 0.0 in stage 52.0 (TID 84) (138.4.31.14, executor driver, partition 0, NODE_LOCAL, 12952 bytes)
[2025-07-11T11:47:23.301+0200] {subprocess.py:93} INFO - 25/07/11 11:47:23 INFO Executor: Running task 0.0 in stage 52.0 (TID 84)
[2025-07-11T11:47:23.317+0200] {subprocess.py:93} INFO - 25/07/11 11:47:23 INFO ShuffleBlockFetcherIterator: Getting 1 (42.2 KiB) non-empty blocks including 1 (42.2 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2025-07-11T11:47:23.317+0200] {subprocess.py:93} INFO - 25/07/11 11:47:23 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2025-07-11T11:47:23.317+0200] {subprocess.py:93} INFO - 25/07/11 11:47:23 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2025-07-11T11:47:23.317+0200] {subprocess.py:93} INFO - 25/07/11 11:47:23 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2025-07-11T11:47:23.317+0200] {subprocess.py:93} INFO - 25/07/11 11:47:23 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[2025-07-11T11:47:23.317+0200] {subprocess.py:93} INFO - 25/07/11 11:47:23 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2025-07-11T11:47:23.317+0200] {subprocess.py:93} INFO - 25/07/11 11:47:23 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2025-07-11T11:47:23.317+0200] {subprocess.py:93} INFO - 25/07/11 11:47:23 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[2025-07-11T11:47:23.317+0200] {subprocess.py:93} INFO - 25/07/11 11:47:23 INFO CodecConfig: Compression: SNAPPY
[2025-07-11T11:47:23.317+0200] {subprocess.py:93} INFO - 25/07/11 11:47:23 INFO CodecConfig: Compression: SNAPPY
[2025-07-11T11:47:23.318+0200] {subprocess.py:93} INFO - 25/07/11 11:47:23 INFO ParquetOutputFormat: ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]
[2025-07-11T11:47:23.318+0200] {subprocess.py:93} INFO - 25/07/11 11:47:23 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
[2025-07-11T11:47:23.319+0200] {subprocess.py:93} INFO - {
[2025-07-11T11:47:23.319+0200] {subprocess.py:93} INFO -   "type" : "struct",
[2025-07-11T11:47:23.319+0200] {subprocess.py:93} INFO -   "fields" : [ {
[2025-07-11T11:47:23.319+0200] {subprocess.py:93} INFO -     "name" : "labelsArray",
[2025-07-11T11:47:23.319+0200] {subprocess.py:93} INFO -     "type" : {
[2025-07-11T11:47:23.319+0200] {subprocess.py:93} INFO -       "type" : "array",
[2025-07-11T11:47:23.319+0200] {subprocess.py:93} INFO -       "elementType" : {
[2025-07-11T11:47:23.319+0200] {subprocess.py:93} INFO -         "type" : "array",
[2025-07-11T11:47:23.319+0200] {subprocess.py:93} INFO -         "elementType" : "string",
[2025-07-11T11:47:23.319+0200] {subprocess.py:93} INFO -         "containsNull" : true
[2025-07-11T11:47:23.319+0200] {subprocess.py:93} INFO -       },
[2025-07-11T11:47:23.319+0200] {subprocess.py:93} INFO -       "containsNull" : true
[2025-07-11T11:47:23.319+0200] {subprocess.py:93} INFO -     },
[2025-07-11T11:47:23.319+0200] {subprocess.py:93} INFO -     "nullable" : true,
[2025-07-11T11:47:23.319+0200] {subprocess.py:93} INFO -     "metadata" : { }
[2025-07-11T11:47:23.319+0200] {subprocess.py:93} INFO -   } ]
[2025-07-11T11:47:23.319+0200] {subprocess.py:93} INFO - }
[2025-07-11T11:47:23.319+0200] {subprocess.py:93} INFO - and corresponding Parquet message type:
[2025-07-11T11:47:23.320+0200] {subprocess.py:93} INFO - message spark_schema {
[2025-07-11T11:47:23.320+0200] {subprocess.py:93} INFO -   optional group labelsArray (LIST) {
[2025-07-11T11:47:23.320+0200] {subprocess.py:93} INFO -     repeated group list {
[2025-07-11T11:47:23.320+0200] {subprocess.py:93} INFO -       optional group element (LIST) {
[2025-07-11T11:47:23.320+0200] {subprocess.py:93} INFO -         repeated group list {
[2025-07-11T11:47:23.320+0200] {subprocess.py:93} INFO -           optional binary element (STRING);
[2025-07-11T11:47:23.320+0200] {subprocess.py:93} INFO -         }
[2025-07-11T11:47:23.320+0200] {subprocess.py:93} INFO -       }
[2025-07-11T11:47:23.320+0200] {subprocess.py:93} INFO -     }
[2025-07-11T11:47:23.320+0200] {subprocess.py:93} INFO -   }
[2025-07-11T11:47:23.320+0200] {subprocess.py:93} INFO - }
[2025-07-11T11:47:23.320+0200] {subprocess.py:93} INFO - 
[2025-07-11T11:47:23.320+0200] {subprocess.py:93} INFO - 
[2025-07-11T11:47:23.362+0200] {subprocess.py:93} INFO - 25/07/11 11:47:23 INFO BlockManagerInfo: Removed broadcast_37_piece0 on 138.4.31.14:39113 in memory (size: 23.8 KiB, free: 433.9 MiB)
[2025-07-11T11:47:23.376+0200] {subprocess.py:93} INFO - 25/07/11 11:47:23 INFO BlockManagerInfo: Removed broadcast_38_piece0 on 138.4.31.14:39113 in memory (size: 36.5 KiB, free: 433.9 MiB)
[2025-07-11T11:47:23.388+0200] {subprocess.py:93} INFO - 25/07/11 11:47:23 INFO BlockManagerInfo: Removed broadcast_39_piece0 on 138.4.31.14:39113 in memory (size: 4.4 KiB, free: 433.9 MiB)
[2025-07-11T11:47:23.392+0200] {subprocess.py:93} INFO - 25/07/11 11:47:23 INFO BlockManagerInfo: Removed broadcast_36_piece0 on 138.4.31.14:39113 in memory (size: 24.1 KiB, free: 433.9 MiB)
[2025-07-11T11:47:23.401+0200] {subprocess.py:93} INFO - 25/07/11 11:47:23 INFO FileOutputCommitter: Saved output of task 'attempt_20250711114723992365889799964857_0052_m_000000_84' to file:/home/monica.fernandez/practica_creativa/models/string_indexer_model_Route.bin/data/_temporary/0/task_20250711114723992365889799964857_0052_m_000000
[2025-07-11T11:47:23.401+0200] {subprocess.py:93} INFO - 25/07/11 11:47:23 INFO SparkHadoopMapRedUtil: attempt_20250711114723992365889799964857_0052_m_000000_84: Committed. Elapsed time: 1 ms.
[2025-07-11T11:47:23.407+0200] {subprocess.py:93} INFO - 25/07/11 11:47:23 INFO Executor: Finished task 0.0 in stage 52.0 (TID 84). 4783 bytes result sent to driver
[2025-07-11T11:47:23.408+0200] {subprocess.py:93} INFO - 25/07/11 11:47:23 INFO TaskSetManager: Finished task 0.0 in stage 52.0 (TID 84) in 109 ms on 138.4.31.14 (executor driver) (1/1)
[2025-07-11T11:47:23.408+0200] {subprocess.py:93} INFO - 25/07/11 11:47:23 INFO TaskSchedulerImpl: Removed TaskSet 52.0, whose tasks have all completed, from pool
[2025-07-11T11:47:23.413+0200] {subprocess.py:93} INFO - 25/07/11 11:47:23 INFO DAGScheduler: ResultStage 52 (parquet at StringIndexer.scala:499) finished in 0,129 s
[2025-07-11T11:47:23.413+0200] {subprocess.py:93} INFO - 25/07/11 11:47:23 INFO DAGScheduler: Job 32 is finished. Cancelling potential speculative or zombie tasks for this job
[2025-07-11T11:47:23.413+0200] {subprocess.py:93} INFO - 25/07/11 11:47:23 INFO TaskSchedulerImpl: Killing all running tasks in stage 52: Stage finished
[2025-07-11T11:47:23.414+0200] {subprocess.py:93} INFO - 25/07/11 11:47:23 INFO DAGScheduler: Job 32 finished: parquet at StringIndexer.scala:499, took 0,133326 s
[2025-07-11T11:47:23.414+0200] {subprocess.py:93} INFO - 25/07/11 11:47:23 INFO FileFormatWriter: Start to commit write Job 67b29285-b308-45b7-aa34-0e58e7406d57.
[2025-07-11T11:47:23.459+0200] {subprocess.py:93} INFO - 25/07/11 11:47:23 INFO FileFormatWriter: Write Job 67b29285-b308-45b7-aa34-0e58e7406d57 committed. Elapsed time: 40 ms.
[2025-07-11T11:47:23.460+0200] {subprocess.py:93} INFO - 25/07/11 11:47:23 INFO FileFormatWriter: Finished processing stats for write job 67b29285-b308-45b7-aa34-0e58e7406d57.
[2025-07-11T11:47:23.944+0200] {subprocess.py:93} INFO - Traceback (most recent call last):
[2025-07-11T11:47:23.944+0200] {subprocess.py:93} INFO -   File "/home/monica.fernandez/practica_creativa/resources/train_spark_mllib_model.py", line 147, in <module>
[2025-07-11T11:47:23.944+0200] {subprocess.py:93} INFO -     main(sys.argv[1] if len(sys.argv) > 1 else ".")
[2025-07-11T11:47:23.944+0200] {subprocess.py:93} INFO -   File "/home/monica.fernandez/practica_creativa/resources/train_spark_mllib_model.py", line 97, in main
[2025-07-11T11:47:23.944+0200] {subprocess.py:93} INFO -     final_vectorized_features = vector_assembler.transform(ml_bucketized_features)
[2025-07-11T11:47:23.944+0200] {subprocess.py:93} INFO -                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2025-07-11T11:47:23.944+0200] {subprocess.py:93} INFO -   File "/home/monica.fernandez/.sdkman/candidates/spark/current/python/lib/pyspark.zip/pyspark/ml/base.py", line 262, in transform
[2025-07-11T11:47:23.944+0200] {subprocess.py:93} INFO -   File "/home/monica.fernandez/.sdkman/candidates/spark/current/python/lib/pyspark.zip/pyspark/ml/wrapper.py", line 398, in _transform
[2025-07-11T11:47:23.944+0200] {subprocess.py:93} INFO -   File "/home/monica.fernandez/.sdkman/candidates/spark/current/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py", line 1322, in __call__
[2025-07-11T11:47:23.945+0200] {subprocess.py:93} INFO -   File "/home/monica.fernandez/.sdkman/candidates/spark/current/python/lib/pyspark.zip/pyspark/errors/exceptions/captured.py", line 185, in deco
[2025-07-11T11:47:23.956+0200] {subprocess.py:93} INFO - pyspark.errors.exceptions.captured.AnalysisException: [AMBIGUOUS_REFERENCE] Reference `Distance` is ambiguous, could be: [`Distance`, `Distance`].
[2025-07-11T11:47:24.563+0200] {subprocess.py:93} INFO - 25/07/11 11:47:24 INFO SparkContext: Invoking stop() from shutdown hook
[2025-07-11T11:47:24.563+0200] {subprocess.py:93} INFO - 25/07/11 11:47:24 INFO SparkContext: SparkContext is stopping with exitCode 0.
[2025-07-11T11:47:24.586+0200] {subprocess.py:93} INFO - 25/07/11 11:47:24 INFO CassandraConnector: Disconnected from Cassandra cluster.
[2025-07-11T11:47:24.586+0200] {subprocess.py:93} INFO - 25/07/11 11:47:24 INFO SerialShutdownHooks: Successfully executed shutdown hook: Clearing session cache for C* connector
[2025-07-11T11:47:24.598+0200] {subprocess.py:93} INFO - 25/07/11 11:47:24 INFO SparkUI: Stopped Spark web UI at http://138.4.31.14:4041
[2025-07-11T11:47:24.617+0200] {subprocess.py:93} INFO - 25/07/11 11:47:24 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
[2025-07-11T11:47:24.644+0200] {subprocess.py:93} INFO - 25/07/11 11:47:24 INFO MemoryStore: MemoryStore cleared
[2025-07-11T11:47:24.644+0200] {subprocess.py:93} INFO - 25/07/11 11:47:24 INFO BlockManager: BlockManager stopped
[2025-07-11T11:47:24.650+0200] {subprocess.py:93} INFO - 25/07/11 11:47:24 INFO BlockManagerMaster: BlockManagerMaster stopped
[2025-07-11T11:47:24.655+0200] {subprocess.py:93} INFO - 25/07/11 11:47:24 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
[2025-07-11T11:47:24.680+0200] {subprocess.py:93} INFO - 25/07/11 11:47:24 INFO SparkContext: Successfully stopped SparkContext
[2025-07-11T11:47:24.681+0200] {subprocess.py:93} INFO - 25/07/11 11:47:24 INFO ShutdownHookManager: Shutdown hook called
[2025-07-11T11:47:24.681+0200] {subprocess.py:93} INFO - 25/07/11 11:47:24 INFO ShutdownHookManager: Deleting directory /tmp/spark-e296bed0-a463-4855-8646-e7702eb39551
[2025-07-11T11:47:24.684+0200] {subprocess.py:93} INFO - 25/07/11 11:47:24 INFO ShutdownHookManager: Deleting directory /tmp/spark-7342345b-73e9-4615-b392-312f8b72e9b5/pyspark-1724905b-13d7-4cdc-bca0-d74223a6aeff
[2025-07-11T11:47:24.687+0200] {subprocess.py:93} INFO - 25/07/11 11:47:24 INFO ShutdownHookManager: Deleting directory /tmp/spark-7342345b-73e9-4615-b392-312f8b72e9b5
[2025-07-11T11:47:24.792+0200] {subprocess.py:97} INFO - Command exited with return code 1
[2025-07-11T11:47:24.814+0200] {taskinstance.py:1768} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/monica.fernandez/practica_creativa/venv-airflow/lib/python3.11/site-packages/airflow/operators/bash.py", line 196, in execute
    raise AirflowException(
airflow.exceptions.AirflowException: Bash command failed. The command returned a non-zero exit code 1.
[2025-07-11T11:47:24.818+0200] {taskinstance.py:1318} INFO - Marking task as FAILED. dag_id=agile_data_science_batch_prediction_model_training, task_id=pyspark_train_classifier_model, execution_date=20250710T102709, start_date=20250711T094622, end_date=20250711T094724
[2025-07-11T11:47:24.844+0200] {standard_task_runner.py:100} ERROR - Failed to execute job 138 for task pyspark_train_classifier_model (Bash command failed. The command returned a non-zero exit code 1.; 89812)
[2025-07-11T11:47:24.855+0200] {local_task_job.py:208} INFO - Task exited with return code 1
[2025-07-11T11:47:24.877+0200] {taskinstance.py:2578} INFO - 0 downstream tasks scheduled from follow-on schedule check
