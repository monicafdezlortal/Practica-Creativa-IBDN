[2025-07-10T12:13:42.141+0200] {taskinstance.py:1083} INFO - Dependencies all met for <TaskInstance: agile_data_science_batch_prediction_model_training.pyspark_train_classifier_model manual__2025-07-10T10:02:45.624160+00:00 [queued]>
[2025-07-10T12:13:42.193+0200] {taskinstance.py:1083} INFO - Dependencies all met for <TaskInstance: agile_data_science_batch_prediction_model_training.pyspark_train_classifier_model manual__2025-07-10T10:02:45.624160+00:00 [queued]>
[2025-07-10T12:13:42.193+0200] {taskinstance.py:1279} INFO - 
--------------------------------------------------------------------------------
[2025-07-10T12:13:42.193+0200] {taskinstance.py:1280} INFO - Starting attempt 3 of 4
[2025-07-10T12:13:42.193+0200] {taskinstance.py:1281} INFO - 
--------------------------------------------------------------------------------
[2025-07-10T12:13:42.225+0200] {taskinstance.py:1300} INFO - Executing <Task(BashOperator): pyspark_train_classifier_model> on 2025-07-10 10:02:45.624160+00:00
[2025-07-10T12:13:42.230+0200] {standard_task_runner.py:55} INFO - Started process 129308 to run task
[2025-07-10T12:13:42.233+0200] {standard_task_runner.py:82} INFO - Running: ['airflow', 'tasks', 'run', 'agile_data_science_batch_prediction_model_training', 'pyspark_train_classifier_model', 'manual__2025-07-10T10:02:45.624160+00:00', '--job-id', '129', '--raw', '--subdir', 'DAGS_FOLDER/setup.py', '--cfg-path', '/tmp/tmpke3dix5o']
[2025-07-10T12:13:42.234+0200] {standard_task_runner.py:83} INFO - Job 129: Subtask pyspark_train_classifier_model
[2025-07-10T12:13:42.342+0200] {task_command.py:388} INFO - Running <TaskInstance: agile_data_science_batch_prediction_model_training.pyspark_train_classifier_model manual__2025-07-10T10:02:45.624160+00:00 [running]> on host l017.lab.dit.upm.es
[2025-07-10T12:13:42.451+0200] {taskinstance.py:1507} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=airflow
AIRFLOW_CTX_DAG_ID=agile_data_science_batch_prediction_model_training
AIRFLOW_CTX_TASK_ID=pyspark_train_classifier_model
AIRFLOW_CTX_EXECUTION_DATE=2025-07-10T10:02:45.624160+00:00
AIRFLOW_CTX_TRY_NUMBER=3
AIRFLOW_CTX_DAG_RUN_ID=manual__2025-07-10T10:02:45.624160+00:00
[2025-07-10T12:13:42.454+0200] {subprocess.py:63} INFO - Tmp dir root location: 
 /tmp
[2025-07-10T12:13:42.455+0200] {subprocess.py:75} INFO - Running command: ['/usr/bin/bash', '-c', '\nspark-submit --master local[4]   --packages com.datastax.spark:spark-cassandra-connector_2.12:3.5.0   /home/monica.fernandez/practica_creativa/resources/train_spark_mllib_model.py   /home/monica.fernandez/practica_creativa']
[2025-07-10T12:13:42.468+0200] {subprocess.py:86} INFO - Output:
[2025-07-10T12:13:47.564+0200] {subprocess.py:93} INFO - 25/07/10 12:13:47 WARN Utils: Your hostname, l017 resolves to a loopback address: 127.0.1.1; using 138.4.31.17 instead (on interface enp1s0)
[2025-07-10T12:13:47.567+0200] {subprocess.py:93} INFO - 25/07/10 12:13:47 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
[2025-07-10T12:13:47.805+0200] {subprocess.py:93} INFO - :: loading settings :: url = jar:file:/home/monica.fernandez/.sdkman/candidates/spark/3.5.3/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml
[2025-07-10T12:13:47.910+0200] {subprocess.py:93} INFO - Ivy Default Cache set to: /home/monica.fernandez/.ivy2/cache
[2025-07-10T12:13:47.910+0200] {subprocess.py:93} INFO - The jars for the packages stored in: /home/monica.fernandez/.ivy2/jars
[2025-07-10T12:13:47.914+0200] {subprocess.py:93} INFO - com.datastax.spark#spark-cassandra-connector_2.12 added as a dependency
[2025-07-10T12:13:47.916+0200] {subprocess.py:93} INFO - :: resolving dependencies :: org.apache.spark#spark-submit-parent-4b040254-0226-459c-b006-c4f606664df9;1.0
[2025-07-10T12:13:47.916+0200] {subprocess.py:93} INFO - 	confs: [default]
[2025-07-10T12:13:48.088+0200] {subprocess.py:93} INFO - 	found com.datastax.spark#spark-cassandra-connector_2.12;3.5.0 in central
[2025-07-10T12:13:48.121+0200] {subprocess.py:93} INFO - 	found com.datastax.spark#spark-cassandra-connector-driver_2.12;3.5.0 in central
[2025-07-10T12:13:48.157+0200] {subprocess.py:93} INFO - 	found org.scala-lang.modules#scala-collection-compat_2.12;2.11.0 in central
[2025-07-10T12:13:48.198+0200] {subprocess.py:93} INFO - 	found com.datastax.oss#java-driver-core-shaded;4.13.0 in central
[2025-07-10T12:13:48.228+0200] {subprocess.py:93} INFO - 	found com.datastax.oss#native-protocol;1.5.0 in central
[2025-07-10T12:13:48.254+0200] {subprocess.py:93} INFO - 	found com.datastax.oss#java-driver-shaded-guava;25.1-jre-graal-sub-1 in central
[2025-07-10T12:13:48.287+0200] {subprocess.py:93} INFO - 	found com.typesafe#config;1.4.1 in central
[2025-07-10T12:13:48.313+0200] {subprocess.py:93} INFO - 	found org.slf4j#slf4j-api;1.7.26 in central
[2025-07-10T12:13:48.344+0200] {subprocess.py:93} INFO - 	found io.dropwizard.metrics#metrics-core;4.1.18 in central
[2025-07-10T12:13:48.366+0200] {subprocess.py:93} INFO - 	found org.hdrhistogram#HdrHistogram;2.1.12 in central
[2025-07-10T12:13:48.389+0200] {subprocess.py:93} INFO - 	found org.reactivestreams#reactive-streams;1.0.3 in central
[2025-07-10T12:13:48.410+0200] {subprocess.py:93} INFO - 	found com.github.stephenc.jcip#jcip-annotations;1.0-1 in central
[2025-07-10T12:13:48.436+0200] {subprocess.py:93} INFO - 	found com.github.spotbugs#spotbugs-annotations;3.1.12 in central
[2025-07-10T12:13:48.462+0200] {subprocess.py:93} INFO - 	found com.google.code.findbugs#jsr305;3.0.2 in central
[2025-07-10T12:13:48.493+0200] {subprocess.py:93} INFO - 	found com.datastax.oss#java-driver-mapper-runtime;4.13.0 in central
[2025-07-10T12:13:48.521+0200] {subprocess.py:93} INFO - 	found com.datastax.oss#java-driver-query-builder;4.13.0 in central
[2025-07-10T12:13:48.544+0200] {subprocess.py:93} INFO - 	found org.apache.commons#commons-lang3;3.10 in central
[2025-07-10T12:13:48.560+0200] {subprocess.py:93} INFO - 	found com.thoughtworks.paranamer#paranamer;2.8 in central
[2025-07-10T12:13:48.576+0200] {subprocess.py:93} INFO - 	found org.scala-lang#scala-reflect;2.12.11 in central
[2025-07-10T12:13:48.636+0200] {subprocess.py:93} INFO - :: resolution report :: resolve 678ms :: artifacts dl 42ms
[2025-07-10T12:13:48.637+0200] {subprocess.py:93} INFO - 	:: modules in use:
[2025-07-10T12:13:48.637+0200] {subprocess.py:93} INFO - 	com.datastax.oss#java-driver-core-shaded;4.13.0 from central in [default]
[2025-07-10T12:13:48.637+0200] {subprocess.py:93} INFO - 	com.datastax.oss#java-driver-mapper-runtime;4.13.0 from central in [default]
[2025-07-10T12:13:48.637+0200] {subprocess.py:93} INFO - 	com.datastax.oss#java-driver-query-builder;4.13.0 from central in [default]
[2025-07-10T12:13:48.637+0200] {subprocess.py:93} INFO - 	com.datastax.oss#java-driver-shaded-guava;25.1-jre-graal-sub-1 from central in [default]
[2025-07-10T12:13:48.638+0200] {subprocess.py:93} INFO - 	com.datastax.oss#native-protocol;1.5.0 from central in [default]
[2025-07-10T12:13:48.638+0200] {subprocess.py:93} INFO - 	com.datastax.spark#spark-cassandra-connector-driver_2.12;3.5.0 from central in [default]
[2025-07-10T12:13:48.638+0200] {subprocess.py:93} INFO - 	com.datastax.spark#spark-cassandra-connector_2.12;3.5.0 from central in [default]
[2025-07-10T12:13:48.638+0200] {subprocess.py:93} INFO - 	com.github.spotbugs#spotbugs-annotations;3.1.12 from central in [default]
[2025-07-10T12:13:48.638+0200] {subprocess.py:93} INFO - 	com.github.stephenc.jcip#jcip-annotations;1.0-1 from central in [default]
[2025-07-10T12:13:48.638+0200] {subprocess.py:93} INFO - 	com.google.code.findbugs#jsr305;3.0.2 from central in [default]
[2025-07-10T12:13:48.638+0200] {subprocess.py:93} INFO - 	com.thoughtworks.paranamer#paranamer;2.8 from central in [default]
[2025-07-10T12:13:48.639+0200] {subprocess.py:93} INFO - 	com.typesafe#config;1.4.1 from central in [default]
[2025-07-10T12:13:48.639+0200] {subprocess.py:93} INFO - 	io.dropwizard.metrics#metrics-core;4.1.18 from central in [default]
[2025-07-10T12:13:48.639+0200] {subprocess.py:93} INFO - 	org.apache.commons#commons-lang3;3.10 from central in [default]
[2025-07-10T12:13:48.639+0200] {subprocess.py:93} INFO - 	org.hdrhistogram#HdrHistogram;2.1.12 from central in [default]
[2025-07-10T12:13:48.639+0200] {subprocess.py:93} INFO - 	org.reactivestreams#reactive-streams;1.0.3 from central in [default]
[2025-07-10T12:13:48.639+0200] {subprocess.py:93} INFO - 	org.scala-lang#scala-reflect;2.12.11 from central in [default]
[2025-07-10T12:13:48.640+0200] {subprocess.py:93} INFO - 	org.scala-lang.modules#scala-collection-compat_2.12;2.11.0 from central in [default]
[2025-07-10T12:13:48.640+0200] {subprocess.py:93} INFO - 	org.slf4j#slf4j-api;1.7.26 from central in [default]
[2025-07-10T12:13:48.640+0200] {subprocess.py:93} INFO - 	---------------------------------------------------------------------
[2025-07-10T12:13:48.640+0200] {subprocess.py:93} INFO - 	|                  |            modules            ||   artifacts   |
[2025-07-10T12:13:48.640+0200] {subprocess.py:93} INFO - 	|       conf       | number| search|dwnlded|evicted|| number|dwnlded|
[2025-07-10T12:13:48.640+0200] {subprocess.py:93} INFO - 	---------------------------------------------------------------------
[2025-07-10T12:13:48.640+0200] {subprocess.py:93} INFO - 	|      default     |   19  |   0   |   0   |   0   ||   19  |   0   |
[2025-07-10T12:13:48.641+0200] {subprocess.py:93} INFO - 	---------------------------------------------------------------------
[2025-07-10T12:13:48.649+0200] {subprocess.py:93} INFO - :: retrieving :: org.apache.spark#spark-submit-parent-4b040254-0226-459c-b006-c4f606664df9
[2025-07-10T12:13:48.650+0200] {subprocess.py:93} INFO - 	confs: [default]
[2025-07-10T12:13:48.667+0200] {subprocess.py:93} INFO - 	0 artifacts copied, 19 already retrieved (0kB/17ms)
[2025-07-10T12:13:48.959+0200] {subprocess.py:93} INFO - 25/07/10 12:13:48 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2025-07-10T12:13:54.880+0200] {subprocess.py:93} INFO - 25/07/10 12:13:54 INFO SparkContext: Running Spark version 3.5.3
[2025-07-10T12:13:54.880+0200] {subprocess.py:93} INFO - 25/07/10 12:13:54 INFO SparkContext: OS info Linux, 6.1.0-35-amd64, amd64
[2025-07-10T12:13:54.880+0200] {subprocess.py:93} INFO - 25/07/10 12:13:54 INFO SparkContext: Java version 17.0.14
[2025-07-10T12:13:54.904+0200] {subprocess.py:93} INFO - 25/07/10 12:13:54 INFO ResourceUtils: ==============================================================
[2025-07-10T12:13:54.904+0200] {subprocess.py:93} INFO - 25/07/10 12:13:54 INFO ResourceUtils: No custom resources configured for spark.driver.
[2025-07-10T12:13:54.905+0200] {subprocess.py:93} INFO - 25/07/10 12:13:54 INFO ResourceUtils: ==============================================================
[2025-07-10T12:13:54.905+0200] {subprocess.py:93} INFO - 25/07/10 12:13:54 INFO SparkContext: Submitted application: train_spark_mllib_model.py
[2025-07-10T12:13:54.933+0200] {subprocess.py:93} INFO - 25/07/10 12:13:54 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
[2025-07-10T12:13:54.942+0200] {subprocess.py:93} INFO - 25/07/10 12:13:54 INFO ResourceProfile: Limiting resource is cpu
[2025-07-10T12:13:54.943+0200] {subprocess.py:93} INFO - 25/07/10 12:13:54 INFO ResourceProfileManager: Added ResourceProfile id: 0
[2025-07-10T12:13:55.031+0200] {subprocess.py:93} INFO - 25/07/10 12:13:55 INFO SecurityManager: Changing view acls to: monica.fernandez
[2025-07-10T12:13:55.032+0200] {subprocess.py:93} INFO - 25/07/10 12:13:55 INFO SecurityManager: Changing modify acls to: monica.fernandez
[2025-07-10T12:13:55.033+0200] {subprocess.py:93} INFO - 25/07/10 12:13:55 INFO SecurityManager: Changing view acls groups to:
[2025-07-10T12:13:55.033+0200] {subprocess.py:93} INFO - 25/07/10 12:13:55 INFO SecurityManager: Changing modify acls groups to:
[2025-07-10T12:13:55.034+0200] {subprocess.py:93} INFO - 25/07/10 12:13:55 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: monica.fernandez; groups with view permissions: EMPTY; users with modify permissions: monica.fernandez; groups with modify permissions: EMPTY
[2025-07-10T12:13:55.387+0200] {subprocess.py:93} INFO - 25/07/10 12:13:55 INFO Utils: Successfully started service 'sparkDriver' on port 40375.
[2025-07-10T12:13:55.468+0200] {subprocess.py:93} INFO - 25/07/10 12:13:55 INFO SparkEnv: Registering MapOutputTracker
[2025-07-10T12:13:55.513+0200] {subprocess.py:93} INFO - 25/07/10 12:13:55 INFO SparkEnv: Registering BlockManagerMaster
[2025-07-10T12:13:55.537+0200] {subprocess.py:93} INFO - 25/07/10 12:13:55 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[2025-07-10T12:13:55.537+0200] {subprocess.py:93} INFO - 25/07/10 12:13:55 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
[2025-07-10T12:13:55.542+0200] {subprocess.py:93} INFO - 25/07/10 12:13:55 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
[2025-07-10T12:13:55.577+0200] {subprocess.py:93} INFO - 25/07/10 12:13:55 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-341cd8ec-7e71-4b7c-8967-85105cca7d10
[2025-07-10T12:13:55.595+0200] {subprocess.py:93} INFO - 25/07/10 12:13:55 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB
[2025-07-10T12:13:55.614+0200] {subprocess.py:93} INFO - 25/07/10 12:13:55 INFO SparkEnv: Registering OutputCommitCoordinator
[2025-07-10T12:13:55.788+0200] {subprocess.py:93} INFO - 25/07/10 12:13:55 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI
[2025-07-10T12:13:55.878+0200] {subprocess.py:93} INFO - 25/07/10 12:13:55 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
[2025-07-10T12:13:55.890+0200] {subprocess.py:93} INFO - 25/07/10 12:13:55 INFO Utils: Successfully started service 'SparkUI' on port 4041.
[2025-07-10T12:13:55.927+0200] {subprocess.py:93} INFO - 25/07/10 12:13:55 INFO SparkContext: Added JAR file:///home/monica.fernandez/.ivy2/jars/com.datastax.spark_spark-cassandra-connector_2.12-3.5.0.jar at spark://138.4.31.17:40375/jars/com.datastax.spark_spark-cassandra-connector_2.12-3.5.0.jar with timestamp 1752142434871
[2025-07-10T12:13:55.928+0200] {subprocess.py:93} INFO - 25/07/10 12:13:55 INFO SparkContext: Added JAR file:///home/monica.fernandez/.ivy2/jars/com.datastax.spark_spark-cassandra-connector-driver_2.12-3.5.0.jar at spark://138.4.31.17:40375/jars/com.datastax.spark_spark-cassandra-connector-driver_2.12-3.5.0.jar with timestamp 1752142434871
[2025-07-10T12:13:55.928+0200] {subprocess.py:93} INFO - 25/07/10 12:13:55 INFO SparkContext: Added JAR file:///home/monica.fernandez/.ivy2/jars/org.scala-lang.modules_scala-collection-compat_2.12-2.11.0.jar at spark://138.4.31.17:40375/jars/org.scala-lang.modules_scala-collection-compat_2.12-2.11.0.jar with timestamp 1752142434871
[2025-07-10T12:13:55.929+0200] {subprocess.py:93} INFO - 25/07/10 12:13:55 INFO SparkContext: Added JAR file:///home/monica.fernandez/.ivy2/jars/com.datastax.oss_java-driver-core-shaded-4.13.0.jar at spark://138.4.31.17:40375/jars/com.datastax.oss_java-driver-core-shaded-4.13.0.jar with timestamp 1752142434871
[2025-07-10T12:13:55.930+0200] {subprocess.py:93} INFO - 25/07/10 12:13:55 INFO SparkContext: Added JAR file:///home/monica.fernandez/.ivy2/jars/com.datastax.oss_java-driver-mapper-runtime-4.13.0.jar at spark://138.4.31.17:40375/jars/com.datastax.oss_java-driver-mapper-runtime-4.13.0.jar with timestamp 1752142434871
[2025-07-10T12:13:55.930+0200] {subprocess.py:93} INFO - 25/07/10 12:13:55 INFO SparkContext: Added JAR file:///home/monica.fernandez/.ivy2/jars/org.apache.commons_commons-lang3-3.10.jar at spark://138.4.31.17:40375/jars/org.apache.commons_commons-lang3-3.10.jar with timestamp 1752142434871
[2025-07-10T12:13:55.931+0200] {subprocess.py:93} INFO - 25/07/10 12:13:55 INFO SparkContext: Added JAR file:///home/monica.fernandez/.ivy2/jars/com.thoughtworks.paranamer_paranamer-2.8.jar at spark://138.4.31.17:40375/jars/com.thoughtworks.paranamer_paranamer-2.8.jar with timestamp 1752142434871
[2025-07-10T12:13:55.931+0200] {subprocess.py:93} INFO - 25/07/10 12:13:55 INFO SparkContext: Added JAR file:///home/monica.fernandez/.ivy2/jars/org.scala-lang_scala-reflect-2.12.11.jar at spark://138.4.31.17:40375/jars/org.scala-lang_scala-reflect-2.12.11.jar with timestamp 1752142434871
[2025-07-10T12:13:55.931+0200] {subprocess.py:93} INFO - 25/07/10 12:13:55 INFO SparkContext: Added JAR file:///home/monica.fernandez/.ivy2/jars/com.datastax.oss_native-protocol-1.5.0.jar at spark://138.4.31.17:40375/jars/com.datastax.oss_native-protocol-1.5.0.jar with timestamp 1752142434871
[2025-07-10T12:13:55.932+0200] {subprocess.py:93} INFO - 25/07/10 12:13:55 INFO SparkContext: Added JAR file:///home/monica.fernandez/.ivy2/jars/com.datastax.oss_java-driver-shaded-guava-25.1-jre-graal-sub-1.jar at spark://138.4.31.17:40375/jars/com.datastax.oss_java-driver-shaded-guava-25.1-jre-graal-sub-1.jar with timestamp 1752142434871
[2025-07-10T12:13:55.933+0200] {subprocess.py:93} INFO - 25/07/10 12:13:55 INFO SparkContext: Added JAR file:///home/monica.fernandez/.ivy2/jars/com.typesafe_config-1.4.1.jar at spark://138.4.31.17:40375/jars/com.typesafe_config-1.4.1.jar with timestamp 1752142434871
[2025-07-10T12:13:55.933+0200] {subprocess.py:93} INFO - 25/07/10 12:13:55 INFO SparkContext: Added JAR file:///home/monica.fernandez/.ivy2/jars/org.slf4j_slf4j-api-1.7.26.jar at spark://138.4.31.17:40375/jars/org.slf4j_slf4j-api-1.7.26.jar with timestamp 1752142434871
[2025-07-10T12:13:55.934+0200] {subprocess.py:93} INFO - 25/07/10 12:13:55 INFO SparkContext: Added JAR file:///home/monica.fernandez/.ivy2/jars/io.dropwizard.metrics_metrics-core-4.1.18.jar at spark://138.4.31.17:40375/jars/io.dropwizard.metrics_metrics-core-4.1.18.jar with timestamp 1752142434871
[2025-07-10T12:13:55.934+0200] {subprocess.py:93} INFO - 25/07/10 12:13:55 INFO SparkContext: Added JAR file:///home/monica.fernandez/.ivy2/jars/org.hdrhistogram_HdrHistogram-2.1.12.jar at spark://138.4.31.17:40375/jars/org.hdrhistogram_HdrHistogram-2.1.12.jar with timestamp 1752142434871
[2025-07-10T12:13:55.934+0200] {subprocess.py:93} INFO - 25/07/10 12:13:55 INFO SparkContext: Added JAR file:///home/monica.fernandez/.ivy2/jars/org.reactivestreams_reactive-streams-1.0.3.jar at spark://138.4.31.17:40375/jars/org.reactivestreams_reactive-streams-1.0.3.jar with timestamp 1752142434871
[2025-07-10T12:13:55.934+0200] {subprocess.py:93} INFO - 25/07/10 12:13:55 INFO SparkContext: Added JAR file:///home/monica.fernandez/.ivy2/jars/com.github.stephenc.jcip_jcip-annotations-1.0-1.jar at spark://138.4.31.17:40375/jars/com.github.stephenc.jcip_jcip-annotations-1.0-1.jar with timestamp 1752142434871
[2025-07-10T12:13:55.935+0200] {subprocess.py:93} INFO - 25/07/10 12:13:55 INFO SparkContext: Added JAR file:///home/monica.fernandez/.ivy2/jars/com.github.spotbugs_spotbugs-annotations-3.1.12.jar at spark://138.4.31.17:40375/jars/com.github.spotbugs_spotbugs-annotations-3.1.12.jar with timestamp 1752142434871
[2025-07-10T12:13:55.936+0200] {subprocess.py:93} INFO - 25/07/10 12:13:55 INFO SparkContext: Added JAR file:///home/monica.fernandez/.ivy2/jars/com.google.code.findbugs_jsr305-3.0.2.jar at spark://138.4.31.17:40375/jars/com.google.code.findbugs_jsr305-3.0.2.jar with timestamp 1752142434871
[2025-07-10T12:13:55.936+0200] {subprocess.py:93} INFO - 25/07/10 12:13:55 INFO SparkContext: Added JAR file:///home/monica.fernandez/.ivy2/jars/com.datastax.oss_java-driver-query-builder-4.13.0.jar at spark://138.4.31.17:40375/jars/com.datastax.oss_java-driver-query-builder-4.13.0.jar with timestamp 1752142434871
[2025-07-10T12:13:55.938+0200] {subprocess.py:93} INFO - 25/07/10 12:13:55 INFO SparkContext: Added file file:///home/monica.fernandez/.ivy2/jars/com.datastax.spark_spark-cassandra-connector_2.12-3.5.0.jar at file:///home/monica.fernandez/.ivy2/jars/com.datastax.spark_spark-cassandra-connector_2.12-3.5.0.jar with timestamp 1752142434871
[2025-07-10T12:13:55.939+0200] {subprocess.py:93} INFO - 25/07/10 12:13:55 INFO Utils: Copying /home/monica.fernandez/.ivy2/jars/com.datastax.spark_spark-cassandra-connector_2.12-3.5.0.jar to /tmp/spark-64875627-1bdb-48c4-b3bf-1e4a10abda6f/userFiles-960e7b42-c0c2-4b0d-b262-4e9d8197476a/com.datastax.spark_spark-cassandra-connector_2.12-3.5.0.jar
[2025-07-10T12:13:55.970+0200] {subprocess.py:93} INFO - 25/07/10 12:13:55 INFO SparkContext: Added file file:///home/monica.fernandez/.ivy2/jars/com.datastax.spark_spark-cassandra-connector-driver_2.12-3.5.0.jar at file:///home/monica.fernandez/.ivy2/jars/com.datastax.spark_spark-cassandra-connector-driver_2.12-3.5.0.jar with timestamp 1752142434871
[2025-07-10T12:13:55.970+0200] {subprocess.py:93} INFO - 25/07/10 12:13:55 INFO Utils: Copying /home/monica.fernandez/.ivy2/jars/com.datastax.spark_spark-cassandra-connector-driver_2.12-3.5.0.jar to /tmp/spark-64875627-1bdb-48c4-b3bf-1e4a10abda6f/userFiles-960e7b42-c0c2-4b0d-b262-4e9d8197476a/com.datastax.spark_spark-cassandra-connector-driver_2.12-3.5.0.jar
[2025-07-10T12:13:55.983+0200] {subprocess.py:93} INFO - 25/07/10 12:13:55 INFO SparkContext: Added file file:///home/monica.fernandez/.ivy2/jars/org.scala-lang.modules_scala-collection-compat_2.12-2.11.0.jar at file:///home/monica.fernandez/.ivy2/jars/org.scala-lang.modules_scala-collection-compat_2.12-2.11.0.jar with timestamp 1752142434871
[2025-07-10T12:13:55.983+0200] {subprocess.py:93} INFO - 25/07/10 12:13:55 INFO Utils: Copying /home/monica.fernandez/.ivy2/jars/org.scala-lang.modules_scala-collection-compat_2.12-2.11.0.jar to /tmp/spark-64875627-1bdb-48c4-b3bf-1e4a10abda6f/userFiles-960e7b42-c0c2-4b0d-b262-4e9d8197476a/org.scala-lang.modules_scala-collection-compat_2.12-2.11.0.jar
[2025-07-10T12:13:55.990+0200] {subprocess.py:93} INFO - 25/07/10 12:13:55 INFO SparkContext: Added file file:///home/monica.fernandez/.ivy2/jars/com.datastax.oss_java-driver-core-shaded-4.13.0.jar at file:///home/monica.fernandez/.ivy2/jars/com.datastax.oss_java-driver-core-shaded-4.13.0.jar with timestamp 1752142434871
[2025-07-10T12:13:55.990+0200] {subprocess.py:93} INFO - 25/07/10 12:13:55 INFO Utils: Copying /home/monica.fernandez/.ivy2/jars/com.datastax.oss_java-driver-core-shaded-4.13.0.jar to /tmp/spark-64875627-1bdb-48c4-b3bf-1e4a10abda6f/userFiles-960e7b42-c0c2-4b0d-b262-4e9d8197476a/com.datastax.oss_java-driver-core-shaded-4.13.0.jar
[2025-07-10T12:13:56.053+0200] {subprocess.py:93} INFO - 25/07/10 12:13:56 INFO SparkContext: Added file file:///home/monica.fernandez/.ivy2/jars/com.datastax.oss_java-driver-mapper-runtime-4.13.0.jar at file:///home/monica.fernandez/.ivy2/jars/com.datastax.oss_java-driver-mapper-runtime-4.13.0.jar with timestamp 1752142434871
[2025-07-10T12:13:56.053+0200] {subprocess.py:93} INFO - 25/07/10 12:13:56 INFO Utils: Copying /home/monica.fernandez/.ivy2/jars/com.datastax.oss_java-driver-mapper-runtime-4.13.0.jar to /tmp/spark-64875627-1bdb-48c4-b3bf-1e4a10abda6f/userFiles-960e7b42-c0c2-4b0d-b262-4e9d8197476a/com.datastax.oss_java-driver-mapper-runtime-4.13.0.jar
[2025-07-10T12:13:56.058+0200] {subprocess.py:93} INFO - 25/07/10 12:13:56 INFO SparkContext: Added file file:///home/monica.fernandez/.ivy2/jars/org.apache.commons_commons-lang3-3.10.jar at file:///home/monica.fernandez/.ivy2/jars/org.apache.commons_commons-lang3-3.10.jar with timestamp 1752142434871
[2025-07-10T12:13:56.058+0200] {subprocess.py:93} INFO - 25/07/10 12:13:56 INFO Utils: Copying /home/monica.fernandez/.ivy2/jars/org.apache.commons_commons-lang3-3.10.jar to /tmp/spark-64875627-1bdb-48c4-b3bf-1e4a10abda6f/userFiles-960e7b42-c0c2-4b0d-b262-4e9d8197476a/org.apache.commons_commons-lang3-3.10.jar
[2025-07-10T12:13:56.067+0200] {subprocess.py:93} INFO - 25/07/10 12:13:56 INFO SparkContext: Added file file:///home/monica.fernandez/.ivy2/jars/com.thoughtworks.paranamer_paranamer-2.8.jar at file:///home/monica.fernandez/.ivy2/jars/com.thoughtworks.paranamer_paranamer-2.8.jar with timestamp 1752142434871
[2025-07-10T12:13:56.067+0200] {subprocess.py:93} INFO - 25/07/10 12:13:56 INFO Utils: Copying /home/monica.fernandez/.ivy2/jars/com.thoughtworks.paranamer_paranamer-2.8.jar to /tmp/spark-64875627-1bdb-48c4-b3bf-1e4a10abda6f/userFiles-960e7b42-c0c2-4b0d-b262-4e9d8197476a/com.thoughtworks.paranamer_paranamer-2.8.jar
[2025-07-10T12:13:56.072+0200] {subprocess.py:93} INFO - 25/07/10 12:13:56 INFO SparkContext: Added file file:///home/monica.fernandez/.ivy2/jars/org.scala-lang_scala-reflect-2.12.11.jar at file:///home/monica.fernandez/.ivy2/jars/org.scala-lang_scala-reflect-2.12.11.jar with timestamp 1752142434871
[2025-07-10T12:13:56.072+0200] {subprocess.py:93} INFO - 25/07/10 12:13:56 INFO Utils: Copying /home/monica.fernandez/.ivy2/jars/org.scala-lang_scala-reflect-2.12.11.jar to /tmp/spark-64875627-1bdb-48c4-b3bf-1e4a10abda6f/userFiles-960e7b42-c0c2-4b0d-b262-4e9d8197476a/org.scala-lang_scala-reflect-2.12.11.jar
[2025-07-10T12:13:56.107+0200] {subprocess.py:93} INFO - 25/07/10 12:13:56 INFO SparkContext: Added file file:///home/monica.fernandez/.ivy2/jars/com.datastax.oss_native-protocol-1.5.0.jar at file:///home/monica.fernandez/.ivy2/jars/com.datastax.oss_native-protocol-1.5.0.jar with timestamp 1752142434871
[2025-07-10T12:13:56.107+0200] {subprocess.py:93} INFO - 25/07/10 12:13:56 INFO Utils: Copying /home/monica.fernandez/.ivy2/jars/com.datastax.oss_native-protocol-1.5.0.jar to /tmp/spark-64875627-1bdb-48c4-b3bf-1e4a10abda6f/userFiles-960e7b42-c0c2-4b0d-b262-4e9d8197476a/com.datastax.oss_native-protocol-1.5.0.jar
[2025-07-10T12:13:56.114+0200] {subprocess.py:93} INFO - 25/07/10 12:13:56 INFO SparkContext: Added file file:///home/monica.fernandez/.ivy2/jars/com.datastax.oss_java-driver-shaded-guava-25.1-jre-graal-sub-1.jar at file:///home/monica.fernandez/.ivy2/jars/com.datastax.oss_java-driver-shaded-guava-25.1-jre-graal-sub-1.jar with timestamp 1752142434871
[2025-07-10T12:13:56.114+0200] {subprocess.py:93} INFO - 25/07/10 12:13:56 INFO Utils: Copying /home/monica.fernandez/.ivy2/jars/com.datastax.oss_java-driver-shaded-guava-25.1-jre-graal-sub-1.jar to /tmp/spark-64875627-1bdb-48c4-b3bf-1e4a10abda6f/userFiles-960e7b42-c0c2-4b0d-b262-4e9d8197476a/com.datastax.oss_java-driver-shaded-guava-25.1-jre-graal-sub-1.jar
[2025-07-10T12:13:56.141+0200] {subprocess.py:93} INFO - 25/07/10 12:13:56 INFO SparkContext: Added file file:///home/monica.fernandez/.ivy2/jars/com.typesafe_config-1.4.1.jar at file:///home/monica.fernandez/.ivy2/jars/com.typesafe_config-1.4.1.jar with timestamp 1752142434871
[2025-07-10T12:13:56.141+0200] {subprocess.py:93} INFO - 25/07/10 12:13:56 INFO Utils: Copying /home/monica.fernandez/.ivy2/jars/com.typesafe_config-1.4.1.jar to /tmp/spark-64875627-1bdb-48c4-b3bf-1e4a10abda6f/userFiles-960e7b42-c0c2-4b0d-b262-4e9d8197476a/com.typesafe_config-1.4.1.jar
[2025-07-10T12:13:56.148+0200] {subprocess.py:93} INFO - 25/07/10 12:13:56 INFO SparkContext: Added file file:///home/monica.fernandez/.ivy2/jars/org.slf4j_slf4j-api-1.7.26.jar at file:///home/monica.fernandez/.ivy2/jars/org.slf4j_slf4j-api-1.7.26.jar with timestamp 1752142434871
[2025-07-10T12:13:56.149+0200] {subprocess.py:93} INFO - 25/07/10 12:13:56 INFO Utils: Copying /home/monica.fernandez/.ivy2/jars/org.slf4j_slf4j-api-1.7.26.jar to /tmp/spark-64875627-1bdb-48c4-b3bf-1e4a10abda6f/userFiles-960e7b42-c0c2-4b0d-b262-4e9d8197476a/org.slf4j_slf4j-api-1.7.26.jar
[2025-07-10T12:13:56.153+0200] {subprocess.py:93} INFO - 25/07/10 12:13:56 INFO SparkContext: Added file file:///home/monica.fernandez/.ivy2/jars/io.dropwizard.metrics_metrics-core-4.1.18.jar at file:///home/monica.fernandez/.ivy2/jars/io.dropwizard.metrics_metrics-core-4.1.18.jar with timestamp 1752142434871
[2025-07-10T12:13:56.153+0200] {subprocess.py:93} INFO - 25/07/10 12:13:56 INFO Utils: Copying /home/monica.fernandez/.ivy2/jars/io.dropwizard.metrics_metrics-core-4.1.18.jar to /tmp/spark-64875627-1bdb-48c4-b3bf-1e4a10abda6f/userFiles-960e7b42-c0c2-4b0d-b262-4e9d8197476a/io.dropwizard.metrics_metrics-core-4.1.18.jar
[2025-07-10T12:13:56.163+0200] {subprocess.py:93} INFO - 25/07/10 12:13:56 INFO SparkContext: Added file file:///home/monica.fernandez/.ivy2/jars/org.hdrhistogram_HdrHistogram-2.1.12.jar at file:///home/monica.fernandez/.ivy2/jars/org.hdrhistogram_HdrHistogram-2.1.12.jar with timestamp 1752142434871
[2025-07-10T12:13:56.163+0200] {subprocess.py:93} INFO - 25/07/10 12:13:56 INFO Utils: Copying /home/monica.fernandez/.ivy2/jars/org.hdrhistogram_HdrHistogram-2.1.12.jar to /tmp/spark-64875627-1bdb-48c4-b3bf-1e4a10abda6f/userFiles-960e7b42-c0c2-4b0d-b262-4e9d8197476a/org.hdrhistogram_HdrHistogram-2.1.12.jar
[2025-07-10T12:13:56.169+0200] {subprocess.py:93} INFO - 25/07/10 12:13:56 INFO SparkContext: Added file file:///home/monica.fernandez/.ivy2/jars/org.reactivestreams_reactive-streams-1.0.3.jar at file:///home/monica.fernandez/.ivy2/jars/org.reactivestreams_reactive-streams-1.0.3.jar with timestamp 1752142434871
[2025-07-10T12:13:56.169+0200] {subprocess.py:93} INFO - 25/07/10 12:13:56 INFO Utils: Copying /home/monica.fernandez/.ivy2/jars/org.reactivestreams_reactive-streams-1.0.3.jar to /tmp/spark-64875627-1bdb-48c4-b3bf-1e4a10abda6f/userFiles-960e7b42-c0c2-4b0d-b262-4e9d8197476a/org.reactivestreams_reactive-streams-1.0.3.jar
[2025-07-10T12:13:56.172+0200] {subprocess.py:93} INFO - 25/07/10 12:13:56 INFO SparkContext: Added file file:///home/monica.fernandez/.ivy2/jars/com.github.stephenc.jcip_jcip-annotations-1.0-1.jar at file:///home/monica.fernandez/.ivy2/jars/com.github.stephenc.jcip_jcip-annotations-1.0-1.jar with timestamp 1752142434871
[2025-07-10T12:13:56.173+0200] {subprocess.py:93} INFO - 25/07/10 12:13:56 INFO Utils: Copying /home/monica.fernandez/.ivy2/jars/com.github.stephenc.jcip_jcip-annotations-1.0-1.jar to /tmp/spark-64875627-1bdb-48c4-b3bf-1e4a10abda6f/userFiles-960e7b42-c0c2-4b0d-b262-4e9d8197476a/com.github.stephenc.jcip_jcip-annotations-1.0-1.jar
[2025-07-10T12:13:56.176+0200] {subprocess.py:93} INFO - 25/07/10 12:13:56 INFO SparkContext: Added file file:///home/monica.fernandez/.ivy2/jars/com.github.spotbugs_spotbugs-annotations-3.1.12.jar at file:///home/monica.fernandez/.ivy2/jars/com.github.spotbugs_spotbugs-annotations-3.1.12.jar with timestamp 1752142434871
[2025-07-10T12:13:56.176+0200] {subprocess.py:93} INFO - 25/07/10 12:13:56 INFO Utils: Copying /home/monica.fernandez/.ivy2/jars/com.github.spotbugs_spotbugs-annotations-3.1.12.jar to /tmp/spark-64875627-1bdb-48c4-b3bf-1e4a10abda6f/userFiles-960e7b42-c0c2-4b0d-b262-4e9d8197476a/com.github.spotbugs_spotbugs-annotations-3.1.12.jar
[2025-07-10T12:13:56.180+0200] {subprocess.py:93} INFO - 25/07/10 12:13:56 INFO SparkContext: Added file file:///home/monica.fernandez/.ivy2/jars/com.google.code.findbugs_jsr305-3.0.2.jar at file:///home/monica.fernandez/.ivy2/jars/com.google.code.findbugs_jsr305-3.0.2.jar with timestamp 1752142434871
[2025-07-10T12:13:56.181+0200] {subprocess.py:93} INFO - 25/07/10 12:13:56 INFO Utils: Copying /home/monica.fernandez/.ivy2/jars/com.google.code.findbugs_jsr305-3.0.2.jar to /tmp/spark-64875627-1bdb-48c4-b3bf-1e4a10abda6f/userFiles-960e7b42-c0c2-4b0d-b262-4e9d8197476a/com.google.code.findbugs_jsr305-3.0.2.jar
[2025-07-10T12:13:56.184+0200] {subprocess.py:93} INFO - 25/07/10 12:13:56 INFO SparkContext: Added file file:///home/monica.fernandez/.ivy2/jars/com.datastax.oss_java-driver-query-builder-4.13.0.jar at file:///home/monica.fernandez/.ivy2/jars/com.datastax.oss_java-driver-query-builder-4.13.0.jar with timestamp 1752142434871
[2025-07-10T12:13:56.185+0200] {subprocess.py:93} INFO - 25/07/10 12:13:56 INFO Utils: Copying /home/monica.fernandez/.ivy2/jars/com.datastax.oss_java-driver-query-builder-4.13.0.jar to /tmp/spark-64875627-1bdb-48c4-b3bf-1e4a10abda6f/userFiles-960e7b42-c0c2-4b0d-b262-4e9d8197476a/com.datastax.oss_java-driver-query-builder-4.13.0.jar
[2025-07-10T12:13:56.256+0200] {subprocess.py:93} INFO - 25/07/10 12:13:56 INFO Executor: Starting executor ID driver on host 138.4.31.17
[2025-07-10T12:13:56.257+0200] {subprocess.py:93} INFO - 25/07/10 12:13:56 INFO Executor: OS info Linux, 6.1.0-35-amd64, amd64
[2025-07-10T12:13:56.257+0200] {subprocess.py:93} INFO - 25/07/10 12:13:56 INFO Executor: Java version 17.0.14
[2025-07-10T12:13:56.263+0200] {subprocess.py:93} INFO - 25/07/10 12:13:56 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''
[2025-07-10T12:13:56.263+0200] {subprocess.py:93} INFO - 25/07/10 12:13:56 INFO Executor: Created or updated repl class loader org.apache.spark.util.MutableURLClassLoader@52d03407 for default.
[2025-07-10T12:13:56.274+0200] {subprocess.py:93} INFO - 25/07/10 12:13:56 INFO Executor: Fetching file:///home/monica.fernandez/.ivy2/jars/com.datastax.spark_spark-cassandra-connector-driver_2.12-3.5.0.jar with timestamp 1752142434871
[2025-07-10T12:13:56.299+0200] {subprocess.py:93} INFO - 25/07/10 12:13:56 INFO Utils: /home/monica.fernandez/.ivy2/jars/com.datastax.spark_spark-cassandra-connector-driver_2.12-3.5.0.jar has been previously copied to /tmp/spark-64875627-1bdb-48c4-b3bf-1e4a10abda6f/userFiles-960e7b42-c0c2-4b0d-b262-4e9d8197476a/com.datastax.spark_spark-cassandra-connector-driver_2.12-3.5.0.jar
[2025-07-10T12:13:56.303+0200] {subprocess.py:93} INFO - 25/07/10 12:13:56 INFO Executor: Fetching file:///home/monica.fernandez/.ivy2/jars/com.google.code.findbugs_jsr305-3.0.2.jar with timestamp 1752142434871
[2025-07-10T12:13:56.303+0200] {subprocess.py:93} INFO - 25/07/10 12:13:56 INFO Utils: /home/monica.fernandez/.ivy2/jars/com.google.code.findbugs_jsr305-3.0.2.jar has been previously copied to /tmp/spark-64875627-1bdb-48c4-b3bf-1e4a10abda6f/userFiles-960e7b42-c0c2-4b0d-b262-4e9d8197476a/com.google.code.findbugs_jsr305-3.0.2.jar
[2025-07-10T12:13:56.307+0200] {subprocess.py:93} INFO - 25/07/10 12:13:56 INFO Executor: Fetching file:///home/monica.fernandez/.ivy2/jars/com.typesafe_config-1.4.1.jar with timestamp 1752142434871
[2025-07-10T12:13:56.308+0200] {subprocess.py:93} INFO - 25/07/10 12:13:56 INFO Utils: /home/monica.fernandez/.ivy2/jars/com.typesafe_config-1.4.1.jar has been previously copied to /tmp/spark-64875627-1bdb-48c4-b3bf-1e4a10abda6f/userFiles-960e7b42-c0c2-4b0d-b262-4e9d8197476a/com.typesafe_config-1.4.1.jar
[2025-07-10T12:13:56.316+0200] {subprocess.py:93} INFO - 25/07/10 12:13:56 INFO Executor: Fetching file:///home/monica.fernandez/.ivy2/jars/com.thoughtworks.paranamer_paranamer-2.8.jar with timestamp 1752142434871
[2025-07-10T12:13:56.316+0200] {subprocess.py:93} INFO - 25/07/10 12:13:56 INFO Utils: /home/monica.fernandez/.ivy2/jars/com.thoughtworks.paranamer_paranamer-2.8.jar has been previously copied to /tmp/spark-64875627-1bdb-48c4-b3bf-1e4a10abda6f/userFiles-960e7b42-c0c2-4b0d-b262-4e9d8197476a/com.thoughtworks.paranamer_paranamer-2.8.jar
[2025-07-10T12:13:56.320+0200] {subprocess.py:93} INFO - 25/07/10 12:13:56 INFO Executor: Fetching file:///home/monica.fernandez/.ivy2/jars/org.hdrhistogram_HdrHistogram-2.1.12.jar with timestamp 1752142434871
[2025-07-10T12:13:56.320+0200] {subprocess.py:93} INFO - 25/07/10 12:13:56 INFO Utils: /home/monica.fernandez/.ivy2/jars/org.hdrhistogram_HdrHistogram-2.1.12.jar has been previously copied to /tmp/spark-64875627-1bdb-48c4-b3bf-1e4a10abda6f/userFiles-960e7b42-c0c2-4b0d-b262-4e9d8197476a/org.hdrhistogram_HdrHistogram-2.1.12.jar
[2025-07-10T12:13:56.325+0200] {subprocess.py:93} INFO - 25/07/10 12:13:56 INFO Executor: Fetching file:///home/monica.fernandez/.ivy2/jars/com.github.spotbugs_spotbugs-annotations-3.1.12.jar with timestamp 1752142434871
[2025-07-10T12:13:56.325+0200] {subprocess.py:93} INFO - 25/07/10 12:13:56 INFO Utils: /home/monica.fernandez/.ivy2/jars/com.github.spotbugs_spotbugs-annotations-3.1.12.jar has been previously copied to /tmp/spark-64875627-1bdb-48c4-b3bf-1e4a10abda6f/userFiles-960e7b42-c0c2-4b0d-b262-4e9d8197476a/com.github.spotbugs_spotbugs-annotations-3.1.12.jar
[2025-07-10T12:13:56.329+0200] {subprocess.py:93} INFO - 25/07/10 12:13:56 INFO Executor: Fetching file:///home/monica.fernandez/.ivy2/jars/org.apache.commons_commons-lang3-3.10.jar with timestamp 1752142434871
[2025-07-10T12:13:56.330+0200] {subprocess.py:93} INFO - 25/07/10 12:13:56 INFO Utils: /home/monica.fernandez/.ivy2/jars/org.apache.commons_commons-lang3-3.10.jar has been previously copied to /tmp/spark-64875627-1bdb-48c4-b3bf-1e4a10abda6f/userFiles-960e7b42-c0c2-4b0d-b262-4e9d8197476a/org.apache.commons_commons-lang3-3.10.jar
[2025-07-10T12:13:56.334+0200] {subprocess.py:93} INFO - 25/07/10 12:13:56 INFO Executor: Fetching file:///home/monica.fernandez/.ivy2/jars/io.dropwizard.metrics_metrics-core-4.1.18.jar with timestamp 1752142434871
[2025-07-10T12:13:56.335+0200] {subprocess.py:93} INFO - 25/07/10 12:13:56 INFO Utils: /home/monica.fernandez/.ivy2/jars/io.dropwizard.metrics_metrics-core-4.1.18.jar has been previously copied to /tmp/spark-64875627-1bdb-48c4-b3bf-1e4a10abda6f/userFiles-960e7b42-c0c2-4b0d-b262-4e9d8197476a/io.dropwizard.metrics_metrics-core-4.1.18.jar
[2025-07-10T12:13:56.338+0200] {subprocess.py:93} INFO - 25/07/10 12:13:56 INFO Executor: Fetching file:///home/monica.fernandez/.ivy2/jars/com.datastax.oss_java-driver-shaded-guava-25.1-jre-graal-sub-1.jar with timestamp 1752142434871
[2025-07-10T12:13:56.342+0200] {subprocess.py:93} INFO - 25/07/10 12:13:56 INFO Utils: /home/monica.fernandez/.ivy2/jars/com.datastax.oss_java-driver-shaded-guava-25.1-jre-graal-sub-1.jar has been previously copied to /tmp/spark-64875627-1bdb-48c4-b3bf-1e4a10abda6f/userFiles-960e7b42-c0c2-4b0d-b262-4e9d8197476a/com.datastax.oss_java-driver-shaded-guava-25.1-jre-graal-sub-1.jar
[2025-07-10T12:13:56.346+0200] {subprocess.py:93} INFO - 25/07/10 12:13:56 INFO Executor: Fetching file:///home/monica.fernandez/.ivy2/jars/com.datastax.oss_java-driver-core-shaded-4.13.0.jar with timestamp 1752142434871
[2025-07-10T12:13:56.355+0200] {subprocess.py:93} INFO - 25/07/10 12:13:56 INFO Utils: /home/monica.fernandez/.ivy2/jars/com.datastax.oss_java-driver-core-shaded-4.13.0.jar has been previously copied to /tmp/spark-64875627-1bdb-48c4-b3bf-1e4a10abda6f/userFiles-960e7b42-c0c2-4b0d-b262-4e9d8197476a/com.datastax.oss_java-driver-core-shaded-4.13.0.jar
[2025-07-10T12:13:56.358+0200] {subprocess.py:93} INFO - 25/07/10 12:13:56 INFO Executor: Fetching file:///home/monica.fernandez/.ivy2/jars/com.datastax.oss_native-protocol-1.5.0.jar with timestamp 1752142434871
[2025-07-10T12:13:56.359+0200] {subprocess.py:93} INFO - 25/07/10 12:13:56 INFO Utils: /home/monica.fernandez/.ivy2/jars/com.datastax.oss_native-protocol-1.5.0.jar has been previously copied to /tmp/spark-64875627-1bdb-48c4-b3bf-1e4a10abda6f/userFiles-960e7b42-c0c2-4b0d-b262-4e9d8197476a/com.datastax.oss_native-protocol-1.5.0.jar
[2025-07-10T12:13:56.363+0200] {subprocess.py:93} INFO - 25/07/10 12:13:56 INFO Executor: Fetching file:///home/monica.fernandez/.ivy2/jars/org.slf4j_slf4j-api-1.7.26.jar with timestamp 1752142434871
[2025-07-10T12:13:56.363+0200] {subprocess.py:93} INFO - 25/07/10 12:13:56 INFO Utils: /home/monica.fernandez/.ivy2/jars/org.slf4j_slf4j-api-1.7.26.jar has been previously copied to /tmp/spark-64875627-1bdb-48c4-b3bf-1e4a10abda6f/userFiles-960e7b42-c0c2-4b0d-b262-4e9d8197476a/org.slf4j_slf4j-api-1.7.26.jar
[2025-07-10T12:13:56.367+0200] {subprocess.py:93} INFO - 25/07/10 12:13:56 INFO Executor: Fetching file:///home/monica.fernandez/.ivy2/jars/org.scala-lang.modules_scala-collection-compat_2.12-2.11.0.jar with timestamp 1752142434871
[2025-07-10T12:13:56.368+0200] {subprocess.py:93} INFO - 25/07/10 12:13:56 INFO Utils: /home/monica.fernandez/.ivy2/jars/org.scala-lang.modules_scala-collection-compat_2.12-2.11.0.jar has been previously copied to /tmp/spark-64875627-1bdb-48c4-b3bf-1e4a10abda6f/userFiles-960e7b42-c0c2-4b0d-b262-4e9d8197476a/org.scala-lang.modules_scala-collection-compat_2.12-2.11.0.jar
[2025-07-10T12:13:56.372+0200] {subprocess.py:93} INFO - 25/07/10 12:13:56 INFO Executor: Fetching file:///home/monica.fernandez/.ivy2/jars/com.datastax.spark_spark-cassandra-connector_2.12-3.5.0.jar with timestamp 1752142434871
[2025-07-10T12:13:56.374+0200] {subprocess.py:93} INFO - 25/07/10 12:13:56 INFO Utils: /home/monica.fernandez/.ivy2/jars/com.datastax.spark_spark-cassandra-connector_2.12-3.5.0.jar has been previously copied to /tmp/spark-64875627-1bdb-48c4-b3bf-1e4a10abda6f/userFiles-960e7b42-c0c2-4b0d-b262-4e9d8197476a/com.datastax.spark_spark-cassandra-connector_2.12-3.5.0.jar
[2025-07-10T12:13:56.378+0200] {subprocess.py:93} INFO - 25/07/10 12:13:56 INFO Executor: Fetching file:///home/monica.fernandez/.ivy2/jars/com.datastax.oss_java-driver-query-builder-4.13.0.jar with timestamp 1752142434871
[2025-07-10T12:13:56.379+0200] {subprocess.py:93} INFO - 25/07/10 12:13:56 INFO Utils: /home/monica.fernandez/.ivy2/jars/com.datastax.oss_java-driver-query-builder-4.13.0.jar has been previously copied to /tmp/spark-64875627-1bdb-48c4-b3bf-1e4a10abda6f/userFiles-960e7b42-c0c2-4b0d-b262-4e9d8197476a/com.datastax.oss_java-driver-query-builder-4.13.0.jar
[2025-07-10T12:13:56.382+0200] {subprocess.py:93} INFO - 25/07/10 12:13:56 INFO Executor: Fetching file:///home/monica.fernandez/.ivy2/jars/com.github.stephenc.jcip_jcip-annotations-1.0-1.jar with timestamp 1752142434871
[2025-07-10T12:13:56.383+0200] {subprocess.py:93} INFO - 25/07/10 12:13:56 INFO Utils: /home/monica.fernandez/.ivy2/jars/com.github.stephenc.jcip_jcip-annotations-1.0-1.jar has been previously copied to /tmp/spark-64875627-1bdb-48c4-b3bf-1e4a10abda6f/userFiles-960e7b42-c0c2-4b0d-b262-4e9d8197476a/com.github.stephenc.jcip_jcip-annotations-1.0-1.jar
[2025-07-10T12:13:56.386+0200] {subprocess.py:93} INFO - 25/07/10 12:13:56 INFO Executor: Fetching file:///home/monica.fernandez/.ivy2/jars/org.reactivestreams_reactive-streams-1.0.3.jar with timestamp 1752142434871
[2025-07-10T12:13:56.387+0200] {subprocess.py:93} INFO - 25/07/10 12:13:56 INFO Utils: /home/monica.fernandez/.ivy2/jars/org.reactivestreams_reactive-streams-1.0.3.jar has been previously copied to /tmp/spark-64875627-1bdb-48c4-b3bf-1e4a10abda6f/userFiles-960e7b42-c0c2-4b0d-b262-4e9d8197476a/org.reactivestreams_reactive-streams-1.0.3.jar
[2025-07-10T12:13:56.390+0200] {subprocess.py:93} INFO - 25/07/10 12:13:56 INFO Executor: Fetching file:///home/monica.fernandez/.ivy2/jars/com.datastax.oss_java-driver-mapper-runtime-4.13.0.jar with timestamp 1752142434871
[2025-07-10T12:13:56.391+0200] {subprocess.py:93} INFO - 25/07/10 12:13:56 INFO Utils: /home/monica.fernandez/.ivy2/jars/com.datastax.oss_java-driver-mapper-runtime-4.13.0.jar has been previously copied to /tmp/spark-64875627-1bdb-48c4-b3bf-1e4a10abda6f/userFiles-960e7b42-c0c2-4b0d-b262-4e9d8197476a/com.datastax.oss_java-driver-mapper-runtime-4.13.0.jar
[2025-07-10T12:13:56.395+0200] {subprocess.py:93} INFO - 25/07/10 12:13:56 INFO Executor: Fetching file:///home/monica.fernandez/.ivy2/jars/org.scala-lang_scala-reflect-2.12.11.jar with timestamp 1752142434871
[2025-07-10T12:13:56.400+0200] {subprocess.py:93} INFO - 25/07/10 12:13:56 INFO Utils: /home/monica.fernandez/.ivy2/jars/org.scala-lang_scala-reflect-2.12.11.jar has been previously copied to /tmp/spark-64875627-1bdb-48c4-b3bf-1e4a10abda6f/userFiles-960e7b42-c0c2-4b0d-b262-4e9d8197476a/org.scala-lang_scala-reflect-2.12.11.jar
[2025-07-10T12:13:56.405+0200] {subprocess.py:93} INFO - 25/07/10 12:13:56 INFO Executor: Fetching spark://138.4.31.17:40375/jars/com.typesafe_config-1.4.1.jar with timestamp 1752142434871
[2025-07-10T12:13:56.448+0200] {subprocess.py:93} INFO - 25/07/10 12:13:56 INFO TransportClientFactory: Successfully created connection to /138.4.31.17:40375 after 27 ms (0 ms spent in bootstraps)
[2025-07-10T12:13:56.456+0200] {subprocess.py:93} INFO - 25/07/10 12:13:56 INFO Utils: Fetching spark://138.4.31.17:40375/jars/com.typesafe_config-1.4.1.jar to /tmp/spark-64875627-1bdb-48c4-b3bf-1e4a10abda6f/userFiles-960e7b42-c0c2-4b0d-b262-4e9d8197476a/fetchFileTemp8506166409826260887.tmp
[2025-07-10T12:13:56.481+0200] {subprocess.py:93} INFO - 25/07/10 12:13:56 INFO Utils: /tmp/spark-64875627-1bdb-48c4-b3bf-1e4a10abda6f/userFiles-960e7b42-c0c2-4b0d-b262-4e9d8197476a/fetchFileTemp8506166409826260887.tmp has been previously copied to /tmp/spark-64875627-1bdb-48c4-b3bf-1e4a10abda6f/userFiles-960e7b42-c0c2-4b0d-b262-4e9d8197476a/com.typesafe_config-1.4.1.jar
[2025-07-10T12:13:56.486+0200] {subprocess.py:93} INFO - 25/07/10 12:13:56 INFO Executor: Adding file:/tmp/spark-64875627-1bdb-48c4-b3bf-1e4a10abda6f/userFiles-960e7b42-c0c2-4b0d-b262-4e9d8197476a/com.typesafe_config-1.4.1.jar to class loader default
[2025-07-10T12:13:56.486+0200] {subprocess.py:93} INFO - 25/07/10 12:13:56 INFO Executor: Fetching spark://138.4.31.17:40375/jars/com.datastax.oss_java-driver-mapper-runtime-4.13.0.jar with timestamp 1752142434871
[2025-07-10T12:13:56.487+0200] {subprocess.py:93} INFO - 25/07/10 12:13:56 INFO Utils: Fetching spark://138.4.31.17:40375/jars/com.datastax.oss_java-driver-mapper-runtime-4.13.0.jar to /tmp/spark-64875627-1bdb-48c4-b3bf-1e4a10abda6f/userFiles-960e7b42-c0c2-4b0d-b262-4e9d8197476a/fetchFileTemp823647517434617042.tmp
[2025-07-10T12:13:56.489+0200] {subprocess.py:93} INFO - 25/07/10 12:13:56 INFO Utils: /tmp/spark-64875627-1bdb-48c4-b3bf-1e4a10abda6f/userFiles-960e7b42-c0c2-4b0d-b262-4e9d8197476a/fetchFileTemp823647517434617042.tmp has been previously copied to /tmp/spark-64875627-1bdb-48c4-b3bf-1e4a10abda6f/userFiles-960e7b42-c0c2-4b0d-b262-4e9d8197476a/com.datastax.oss_java-driver-mapper-runtime-4.13.0.jar
[2025-07-10T12:13:56.492+0200] {subprocess.py:93} INFO - 25/07/10 12:13:56 INFO Executor: Adding file:/tmp/spark-64875627-1bdb-48c4-b3bf-1e4a10abda6f/userFiles-960e7b42-c0c2-4b0d-b262-4e9d8197476a/com.datastax.oss_java-driver-mapper-runtime-4.13.0.jar to class loader default
[2025-07-10T12:13:56.492+0200] {subprocess.py:93} INFO - 25/07/10 12:13:56 INFO Executor: Fetching spark://138.4.31.17:40375/jars/com.datastax.oss_java-driver-shaded-guava-25.1-jre-graal-sub-1.jar with timestamp 1752142434871
[2025-07-10T12:13:56.493+0200] {subprocess.py:93} INFO - 25/07/10 12:13:56 INFO Utils: Fetching spark://138.4.31.17:40375/jars/com.datastax.oss_java-driver-shaded-guava-25.1-jre-graal-sub-1.jar to /tmp/spark-64875627-1bdb-48c4-b3bf-1e4a10abda6f/userFiles-960e7b42-c0c2-4b0d-b262-4e9d8197476a/fetchFileTemp14008342346791848457.tmp
[2025-07-10T12:13:56.507+0200] {subprocess.py:93} INFO - 25/07/10 12:13:56 INFO Utils: /tmp/spark-64875627-1bdb-48c4-b3bf-1e4a10abda6f/userFiles-960e7b42-c0c2-4b0d-b262-4e9d8197476a/fetchFileTemp14008342346791848457.tmp has been previously copied to /tmp/spark-64875627-1bdb-48c4-b3bf-1e4a10abda6f/userFiles-960e7b42-c0c2-4b0d-b262-4e9d8197476a/com.datastax.oss_java-driver-shaded-guava-25.1-jre-graal-sub-1.jar
[2025-07-10T12:13:56.511+0200] {subprocess.py:93} INFO - 25/07/10 12:13:56 INFO Executor: Adding file:/tmp/spark-64875627-1bdb-48c4-b3bf-1e4a10abda6f/userFiles-960e7b42-c0c2-4b0d-b262-4e9d8197476a/com.datastax.oss_java-driver-shaded-guava-25.1-jre-graal-sub-1.jar to class loader default
[2025-07-10T12:13:56.511+0200] {subprocess.py:93} INFO - 25/07/10 12:13:56 INFO Executor: Fetching spark://138.4.31.17:40375/jars/com.google.code.findbugs_jsr305-3.0.2.jar with timestamp 1752142434871
[2025-07-10T12:13:56.512+0200] {subprocess.py:93} INFO - 25/07/10 12:13:56 INFO Utils: Fetching spark://138.4.31.17:40375/jars/com.google.code.findbugs_jsr305-3.0.2.jar to /tmp/spark-64875627-1bdb-48c4-b3bf-1e4a10abda6f/userFiles-960e7b42-c0c2-4b0d-b262-4e9d8197476a/fetchFileTemp15323386538514275348.tmp
[2025-07-10T12:13:56.513+0200] {subprocess.py:93} INFO - 25/07/10 12:13:56 INFO Utils: /tmp/spark-64875627-1bdb-48c4-b3bf-1e4a10abda6f/userFiles-960e7b42-c0c2-4b0d-b262-4e9d8197476a/fetchFileTemp15323386538514275348.tmp has been previously copied to /tmp/spark-64875627-1bdb-48c4-b3bf-1e4a10abda6f/userFiles-960e7b42-c0c2-4b0d-b262-4e9d8197476a/com.google.code.findbugs_jsr305-3.0.2.jar
[2025-07-10T12:13:56.517+0200] {subprocess.py:93} INFO - 25/07/10 12:13:56 INFO Executor: Adding file:/tmp/spark-64875627-1bdb-48c4-b3bf-1e4a10abda6f/userFiles-960e7b42-c0c2-4b0d-b262-4e9d8197476a/com.google.code.findbugs_jsr305-3.0.2.jar to class loader default
[2025-07-10T12:13:56.517+0200] {subprocess.py:93} INFO - 25/07/10 12:13:56 INFO Executor: Fetching spark://138.4.31.17:40375/jars/com.datastax.spark_spark-cassandra-connector_2.12-3.5.0.jar with timestamp 1752142434871
[2025-07-10T12:13:56.518+0200] {subprocess.py:93} INFO - 25/07/10 12:13:56 INFO Utils: Fetching spark://138.4.31.17:40375/jars/com.datastax.spark_spark-cassandra-connector_2.12-3.5.0.jar to /tmp/spark-64875627-1bdb-48c4-b3bf-1e4a10abda6f/userFiles-960e7b42-c0c2-4b0d-b262-4e9d8197476a/fetchFileTemp2201122303278226503.tmp
[2025-07-10T12:13:56.527+0200] {subprocess.py:93} INFO - 25/07/10 12:13:56 INFO Utils: /tmp/spark-64875627-1bdb-48c4-b3bf-1e4a10abda6f/userFiles-960e7b42-c0c2-4b0d-b262-4e9d8197476a/fetchFileTemp2201122303278226503.tmp has been previously copied to /tmp/spark-64875627-1bdb-48c4-b3bf-1e4a10abda6f/userFiles-960e7b42-c0c2-4b0d-b262-4e9d8197476a/com.datastax.spark_spark-cassandra-connector_2.12-3.5.0.jar
[2025-07-10T12:13:56.533+0200] {subprocess.py:93} INFO - 25/07/10 12:13:56 INFO Executor: Adding file:/tmp/spark-64875627-1bdb-48c4-b3bf-1e4a10abda6f/userFiles-960e7b42-c0c2-4b0d-b262-4e9d8197476a/com.datastax.spark_spark-cassandra-connector_2.12-3.5.0.jar to class loader default
[2025-07-10T12:13:56.533+0200] {subprocess.py:93} INFO - 25/07/10 12:13:56 INFO Executor: Fetching spark://138.4.31.17:40375/jars/com.github.spotbugs_spotbugs-annotations-3.1.12.jar with timestamp 1752142434871
[2025-07-10T12:13:56.534+0200] {subprocess.py:93} INFO - 25/07/10 12:13:56 INFO Utils: Fetching spark://138.4.31.17:40375/jars/com.github.spotbugs_spotbugs-annotations-3.1.12.jar to /tmp/spark-64875627-1bdb-48c4-b3bf-1e4a10abda6f/userFiles-960e7b42-c0c2-4b0d-b262-4e9d8197476a/fetchFileTemp16105210024307106011.tmp
[2025-07-10T12:13:56.535+0200] {subprocess.py:93} INFO - 25/07/10 12:13:56 INFO Utils: /tmp/spark-64875627-1bdb-48c4-b3bf-1e4a10abda6f/userFiles-960e7b42-c0c2-4b0d-b262-4e9d8197476a/fetchFileTemp16105210024307106011.tmp has been previously copied to /tmp/spark-64875627-1bdb-48c4-b3bf-1e4a10abda6f/userFiles-960e7b42-c0c2-4b0d-b262-4e9d8197476a/com.github.spotbugs_spotbugs-annotations-3.1.12.jar
[2025-07-10T12:13:56.539+0200] {subprocess.py:93} INFO - 25/07/10 12:13:56 INFO Executor: Adding file:/tmp/spark-64875627-1bdb-48c4-b3bf-1e4a10abda6f/userFiles-960e7b42-c0c2-4b0d-b262-4e9d8197476a/com.github.spotbugs_spotbugs-annotations-3.1.12.jar to class loader default
[2025-07-10T12:13:56.539+0200] {subprocess.py:93} INFO - 25/07/10 12:13:56 INFO Executor: Fetching spark://138.4.31.17:40375/jars/com.github.stephenc.jcip_jcip-annotations-1.0-1.jar with timestamp 1752142434871
[2025-07-10T12:13:56.541+0200] {subprocess.py:93} INFO - 25/07/10 12:13:56 INFO Utils: Fetching spark://138.4.31.17:40375/jars/com.github.stephenc.jcip_jcip-annotations-1.0-1.jar to /tmp/spark-64875627-1bdb-48c4-b3bf-1e4a10abda6f/userFiles-960e7b42-c0c2-4b0d-b262-4e9d8197476a/fetchFileTemp14331437692019413813.tmp
[2025-07-10T12:13:56.542+0200] {subprocess.py:93} INFO - 25/07/10 12:13:56 INFO Utils: /tmp/spark-64875627-1bdb-48c4-b3bf-1e4a10abda6f/userFiles-960e7b42-c0c2-4b0d-b262-4e9d8197476a/fetchFileTemp14331437692019413813.tmp has been previously copied to /tmp/spark-64875627-1bdb-48c4-b3bf-1e4a10abda6f/userFiles-960e7b42-c0c2-4b0d-b262-4e9d8197476a/com.github.stephenc.jcip_jcip-annotations-1.0-1.jar
[2025-07-10T12:13:56.546+0200] {subprocess.py:93} INFO - 25/07/10 12:13:56 INFO Executor: Adding file:/tmp/spark-64875627-1bdb-48c4-b3bf-1e4a10abda6f/userFiles-960e7b42-c0c2-4b0d-b262-4e9d8197476a/com.github.stephenc.jcip_jcip-annotations-1.0-1.jar to class loader default
[2025-07-10T12:13:56.546+0200] {subprocess.py:93} INFO - 25/07/10 12:13:56 INFO Executor: Fetching spark://138.4.31.17:40375/jars/com.datastax.oss_java-driver-core-shaded-4.13.0.jar with timestamp 1752142434871
[2025-07-10T12:13:56.546+0200] {subprocess.py:93} INFO - 25/07/10 12:13:56 INFO Utils: Fetching spark://138.4.31.17:40375/jars/com.datastax.oss_java-driver-core-shaded-4.13.0.jar to /tmp/spark-64875627-1bdb-48c4-b3bf-1e4a10abda6f/userFiles-960e7b42-c0c2-4b0d-b262-4e9d8197476a/fetchFileTemp10385294714359714565.tmp
[2025-07-10T12:13:56.578+0200] {subprocess.py:93} INFO - 25/07/10 12:13:56 INFO Utils: /tmp/spark-64875627-1bdb-48c4-b3bf-1e4a10abda6f/userFiles-960e7b42-c0c2-4b0d-b262-4e9d8197476a/fetchFileTemp10385294714359714565.tmp has been previously copied to /tmp/spark-64875627-1bdb-48c4-b3bf-1e4a10abda6f/userFiles-960e7b42-c0c2-4b0d-b262-4e9d8197476a/com.datastax.oss_java-driver-core-shaded-4.13.0.jar
[2025-07-10T12:13:56.584+0200] {subprocess.py:93} INFO - 25/07/10 12:13:56 INFO Executor: Adding file:/tmp/spark-64875627-1bdb-48c4-b3bf-1e4a10abda6f/userFiles-960e7b42-c0c2-4b0d-b262-4e9d8197476a/com.datastax.oss_java-driver-core-shaded-4.13.0.jar to class loader default
[2025-07-10T12:13:56.584+0200] {subprocess.py:93} INFO - 25/07/10 12:13:56 INFO Executor: Fetching spark://138.4.31.17:40375/jars/org.scala-lang.modules_scala-collection-compat_2.12-2.11.0.jar with timestamp 1752142434871
[2025-07-10T12:13:56.584+0200] {subprocess.py:93} INFO - 25/07/10 12:13:56 INFO Utils: Fetching spark://138.4.31.17:40375/jars/org.scala-lang.modules_scala-collection-compat_2.12-2.11.0.jar to /tmp/spark-64875627-1bdb-48c4-b3bf-1e4a10abda6f/userFiles-960e7b42-c0c2-4b0d-b262-4e9d8197476a/fetchFileTemp10498402878578954307.tmp
[2025-07-10T12:13:56.586+0200] {subprocess.py:93} INFO - 25/07/10 12:13:56 INFO Utils: /tmp/spark-64875627-1bdb-48c4-b3bf-1e4a10abda6f/userFiles-960e7b42-c0c2-4b0d-b262-4e9d8197476a/fetchFileTemp10498402878578954307.tmp has been previously copied to /tmp/spark-64875627-1bdb-48c4-b3bf-1e4a10abda6f/userFiles-960e7b42-c0c2-4b0d-b262-4e9d8197476a/org.scala-lang.modules_scala-collection-compat_2.12-2.11.0.jar
[2025-07-10T12:13:56.590+0200] {subprocess.py:93} INFO - 25/07/10 12:13:56 INFO Executor: Adding file:/tmp/spark-64875627-1bdb-48c4-b3bf-1e4a10abda6f/userFiles-960e7b42-c0c2-4b0d-b262-4e9d8197476a/org.scala-lang.modules_scala-collection-compat_2.12-2.11.0.jar to class loader default
[2025-07-10T12:13:56.590+0200] {subprocess.py:93} INFO - 25/07/10 12:13:56 INFO Executor: Fetching spark://138.4.31.17:40375/jars/org.scala-lang_scala-reflect-2.12.11.jar with timestamp 1752142434871
[2025-07-10T12:13:56.590+0200] {subprocess.py:93} INFO - 25/07/10 12:13:56 INFO Utils: Fetching spark://138.4.31.17:40375/jars/org.scala-lang_scala-reflect-2.12.11.jar to /tmp/spark-64875627-1bdb-48c4-b3bf-1e4a10abda6f/userFiles-960e7b42-c0c2-4b0d-b262-4e9d8197476a/fetchFileTemp12197966608295170227.tmp
[2025-07-10T12:13:56.606+0200] {subprocess.py:93} INFO - 25/07/10 12:13:56 INFO Utils: /tmp/spark-64875627-1bdb-48c4-b3bf-1e4a10abda6f/userFiles-960e7b42-c0c2-4b0d-b262-4e9d8197476a/fetchFileTemp12197966608295170227.tmp has been previously copied to /tmp/spark-64875627-1bdb-48c4-b3bf-1e4a10abda6f/userFiles-960e7b42-c0c2-4b0d-b262-4e9d8197476a/org.scala-lang_scala-reflect-2.12.11.jar
[2025-07-10T12:13:56.610+0200] {subprocess.py:93} INFO - 25/07/10 12:13:56 INFO Executor: Adding file:/tmp/spark-64875627-1bdb-48c4-b3bf-1e4a10abda6f/userFiles-960e7b42-c0c2-4b0d-b262-4e9d8197476a/org.scala-lang_scala-reflect-2.12.11.jar to class loader default
[2025-07-10T12:13:56.610+0200] {subprocess.py:93} INFO - 25/07/10 12:13:56 INFO Executor: Fetching spark://138.4.31.17:40375/jars/com.datastax.oss_native-protocol-1.5.0.jar with timestamp 1752142434871
[2025-07-10T12:13:56.611+0200] {subprocess.py:93} INFO - 25/07/10 12:13:56 INFO Utils: Fetching spark://138.4.31.17:40375/jars/com.datastax.oss_native-protocol-1.5.0.jar to /tmp/spark-64875627-1bdb-48c4-b3bf-1e4a10abda6f/userFiles-960e7b42-c0c2-4b0d-b262-4e9d8197476a/fetchFileTemp15363302688123411144.tmp
[2025-07-10T12:13:56.613+0200] {subprocess.py:93} INFO - 25/07/10 12:13:56 INFO Utils: /tmp/spark-64875627-1bdb-48c4-b3bf-1e4a10abda6f/userFiles-960e7b42-c0c2-4b0d-b262-4e9d8197476a/fetchFileTemp15363302688123411144.tmp has been previously copied to /tmp/spark-64875627-1bdb-48c4-b3bf-1e4a10abda6f/userFiles-960e7b42-c0c2-4b0d-b262-4e9d8197476a/com.datastax.oss_native-protocol-1.5.0.jar
[2025-07-10T12:13:56.617+0200] {subprocess.py:93} INFO - 25/07/10 12:13:56 INFO Executor: Adding file:/tmp/spark-64875627-1bdb-48c4-b3bf-1e4a10abda6f/userFiles-960e7b42-c0c2-4b0d-b262-4e9d8197476a/com.datastax.oss_native-protocol-1.5.0.jar to class loader default
[2025-07-10T12:13:56.617+0200] {subprocess.py:93} INFO - 25/07/10 12:13:56 INFO Executor: Fetching spark://138.4.31.17:40375/jars/org.reactivestreams_reactive-streams-1.0.3.jar with timestamp 1752142434871
[2025-07-10T12:13:56.619+0200] {subprocess.py:93} INFO - 25/07/10 12:13:56 INFO Utils: Fetching spark://138.4.31.17:40375/jars/org.reactivestreams_reactive-streams-1.0.3.jar to /tmp/spark-64875627-1bdb-48c4-b3bf-1e4a10abda6f/userFiles-960e7b42-c0c2-4b0d-b262-4e9d8197476a/fetchFileTemp8231067521758644749.tmp
[2025-07-10T12:13:56.619+0200] {subprocess.py:93} INFO - 25/07/10 12:13:56 INFO Utils: /tmp/spark-64875627-1bdb-48c4-b3bf-1e4a10abda6f/userFiles-960e7b42-c0c2-4b0d-b262-4e9d8197476a/fetchFileTemp8231067521758644749.tmp has been previously copied to /tmp/spark-64875627-1bdb-48c4-b3bf-1e4a10abda6f/userFiles-960e7b42-c0c2-4b0d-b262-4e9d8197476a/org.reactivestreams_reactive-streams-1.0.3.jar
[2025-07-10T12:13:56.623+0200] {subprocess.py:93} INFO - 25/07/10 12:13:56 INFO Executor: Adding file:/tmp/spark-64875627-1bdb-48c4-b3bf-1e4a10abda6f/userFiles-960e7b42-c0c2-4b0d-b262-4e9d8197476a/org.reactivestreams_reactive-streams-1.0.3.jar to class loader default
[2025-07-10T12:13:56.624+0200] {subprocess.py:93} INFO - 25/07/10 12:13:56 INFO Executor: Fetching spark://138.4.31.17:40375/jars/org.hdrhistogram_HdrHistogram-2.1.12.jar with timestamp 1752142434871
[2025-07-10T12:13:56.624+0200] {subprocess.py:93} INFO - 25/07/10 12:13:56 INFO Utils: Fetching spark://138.4.31.17:40375/jars/org.hdrhistogram_HdrHistogram-2.1.12.jar to /tmp/spark-64875627-1bdb-48c4-b3bf-1e4a10abda6f/userFiles-960e7b42-c0c2-4b0d-b262-4e9d8197476a/fetchFileTemp15009477793973928815.tmp
[2025-07-10T12:13:56.626+0200] {subprocess.py:93} INFO - 25/07/10 12:13:56 INFO Utils: /tmp/spark-64875627-1bdb-48c4-b3bf-1e4a10abda6f/userFiles-960e7b42-c0c2-4b0d-b262-4e9d8197476a/fetchFileTemp15009477793973928815.tmp has been previously copied to /tmp/spark-64875627-1bdb-48c4-b3bf-1e4a10abda6f/userFiles-960e7b42-c0c2-4b0d-b262-4e9d8197476a/org.hdrhistogram_HdrHistogram-2.1.12.jar
[2025-07-10T12:13:56.630+0200] {subprocess.py:93} INFO - 25/07/10 12:13:56 INFO Executor: Adding file:/tmp/spark-64875627-1bdb-48c4-b3bf-1e4a10abda6f/userFiles-960e7b42-c0c2-4b0d-b262-4e9d8197476a/org.hdrhistogram_HdrHistogram-2.1.12.jar to class loader default
[2025-07-10T12:13:56.630+0200] {subprocess.py:93} INFO - 25/07/10 12:13:56 INFO Executor: Fetching spark://138.4.31.17:40375/jars/io.dropwizard.metrics_metrics-core-4.1.18.jar with timestamp 1752142434871
[2025-07-10T12:13:56.631+0200] {subprocess.py:93} INFO - 25/07/10 12:13:56 INFO Utils: Fetching spark://138.4.31.17:40375/jars/io.dropwizard.metrics_metrics-core-4.1.18.jar to /tmp/spark-64875627-1bdb-48c4-b3bf-1e4a10abda6f/userFiles-960e7b42-c0c2-4b0d-b262-4e9d8197476a/fetchFileTemp6676458292332737166.tmp
[2025-07-10T12:13:56.632+0200] {subprocess.py:93} INFO - 25/07/10 12:13:56 INFO Utils: /tmp/spark-64875627-1bdb-48c4-b3bf-1e4a10abda6f/userFiles-960e7b42-c0c2-4b0d-b262-4e9d8197476a/fetchFileTemp6676458292332737166.tmp has been previously copied to /tmp/spark-64875627-1bdb-48c4-b3bf-1e4a10abda6f/userFiles-960e7b42-c0c2-4b0d-b262-4e9d8197476a/io.dropwizard.metrics_metrics-core-4.1.18.jar
[2025-07-10T12:13:56.635+0200] {subprocess.py:93} INFO - 25/07/10 12:13:56 INFO Executor: Adding file:/tmp/spark-64875627-1bdb-48c4-b3bf-1e4a10abda6f/userFiles-960e7b42-c0c2-4b0d-b262-4e9d8197476a/io.dropwizard.metrics_metrics-core-4.1.18.jar to class loader default
[2025-07-10T12:13:56.636+0200] {subprocess.py:93} INFO - 25/07/10 12:13:56 INFO Executor: Fetching spark://138.4.31.17:40375/jars/org.slf4j_slf4j-api-1.7.26.jar with timestamp 1752142434871
[2025-07-10T12:13:56.636+0200] {subprocess.py:93} INFO - 25/07/10 12:13:56 INFO Utils: Fetching spark://138.4.31.17:40375/jars/org.slf4j_slf4j-api-1.7.26.jar to /tmp/spark-64875627-1bdb-48c4-b3bf-1e4a10abda6f/userFiles-960e7b42-c0c2-4b0d-b262-4e9d8197476a/fetchFileTemp7351157867312884277.tmp
[2025-07-10T12:13:56.638+0200] {subprocess.py:93} INFO - 25/07/10 12:13:56 INFO Utils: /tmp/spark-64875627-1bdb-48c4-b3bf-1e4a10abda6f/userFiles-960e7b42-c0c2-4b0d-b262-4e9d8197476a/fetchFileTemp7351157867312884277.tmp has been previously copied to /tmp/spark-64875627-1bdb-48c4-b3bf-1e4a10abda6f/userFiles-960e7b42-c0c2-4b0d-b262-4e9d8197476a/org.slf4j_slf4j-api-1.7.26.jar
[2025-07-10T12:13:56.642+0200] {subprocess.py:93} INFO - 25/07/10 12:13:56 INFO Executor: Adding file:/tmp/spark-64875627-1bdb-48c4-b3bf-1e4a10abda6f/userFiles-960e7b42-c0c2-4b0d-b262-4e9d8197476a/org.slf4j_slf4j-api-1.7.26.jar to class loader default
[2025-07-10T12:13:56.642+0200] {subprocess.py:93} INFO - 25/07/10 12:13:56 INFO Executor: Fetching spark://138.4.31.17:40375/jars/com.datastax.oss_java-driver-query-builder-4.13.0.jar with timestamp 1752142434871
[2025-07-10T12:13:56.643+0200] {subprocess.py:93} INFO - 25/07/10 12:13:56 INFO Utils: Fetching spark://138.4.31.17:40375/jars/com.datastax.oss_java-driver-query-builder-4.13.0.jar to /tmp/spark-64875627-1bdb-48c4-b3bf-1e4a10abda6f/userFiles-960e7b42-c0c2-4b0d-b262-4e9d8197476a/fetchFileTemp10964738132745555320.tmp
[2025-07-10T12:13:56.644+0200] {subprocess.py:93} INFO - 25/07/10 12:13:56 INFO Utils: /tmp/spark-64875627-1bdb-48c4-b3bf-1e4a10abda6f/userFiles-960e7b42-c0c2-4b0d-b262-4e9d8197476a/fetchFileTemp10964738132745555320.tmp has been previously copied to /tmp/spark-64875627-1bdb-48c4-b3bf-1e4a10abda6f/userFiles-960e7b42-c0c2-4b0d-b262-4e9d8197476a/com.datastax.oss_java-driver-query-builder-4.13.0.jar
[2025-07-10T12:13:56.648+0200] {subprocess.py:93} INFO - 25/07/10 12:13:56 INFO Executor: Adding file:/tmp/spark-64875627-1bdb-48c4-b3bf-1e4a10abda6f/userFiles-960e7b42-c0c2-4b0d-b262-4e9d8197476a/com.datastax.oss_java-driver-query-builder-4.13.0.jar to class loader default
[2025-07-10T12:13:56.649+0200] {subprocess.py:93} INFO - 25/07/10 12:13:56 INFO Executor: Fetching spark://138.4.31.17:40375/jars/com.datastax.spark_spark-cassandra-connector-driver_2.12-3.5.0.jar with timestamp 1752142434871
[2025-07-10T12:13:56.649+0200] {subprocess.py:93} INFO - 25/07/10 12:13:56 INFO Utils: Fetching spark://138.4.31.17:40375/jars/com.datastax.spark_spark-cassandra-connector-driver_2.12-3.5.0.jar to /tmp/spark-64875627-1bdb-48c4-b3bf-1e4a10abda6f/userFiles-960e7b42-c0c2-4b0d-b262-4e9d8197476a/fetchFileTemp6923533011690510558.tmp
[2025-07-10T12:13:56.653+0200] {subprocess.py:93} INFO - 25/07/10 12:13:56 INFO Utils: /tmp/spark-64875627-1bdb-48c4-b3bf-1e4a10abda6f/userFiles-960e7b42-c0c2-4b0d-b262-4e9d8197476a/fetchFileTemp6923533011690510558.tmp has been previously copied to /tmp/spark-64875627-1bdb-48c4-b3bf-1e4a10abda6f/userFiles-960e7b42-c0c2-4b0d-b262-4e9d8197476a/com.datastax.spark_spark-cassandra-connector-driver_2.12-3.5.0.jar
[2025-07-10T12:13:56.657+0200] {subprocess.py:93} INFO - 25/07/10 12:13:56 INFO Executor: Adding file:/tmp/spark-64875627-1bdb-48c4-b3bf-1e4a10abda6f/userFiles-960e7b42-c0c2-4b0d-b262-4e9d8197476a/com.datastax.spark_spark-cassandra-connector-driver_2.12-3.5.0.jar to class loader default
[2025-07-10T12:13:56.657+0200] {subprocess.py:93} INFO - 25/07/10 12:13:56 INFO Executor: Fetching spark://138.4.31.17:40375/jars/org.apache.commons_commons-lang3-3.10.jar with timestamp 1752142434871
[2025-07-10T12:13:56.658+0200] {subprocess.py:93} INFO - 25/07/10 12:13:56 INFO Utils: Fetching spark://138.4.31.17:40375/jars/org.apache.commons_commons-lang3-3.10.jar to /tmp/spark-64875627-1bdb-48c4-b3bf-1e4a10abda6f/userFiles-960e7b42-c0c2-4b0d-b262-4e9d8197476a/fetchFileTemp6603977232356661802.tmp
[2025-07-10T12:13:56.660+0200] {subprocess.py:93} INFO - 25/07/10 12:13:56 INFO Utils: /tmp/spark-64875627-1bdb-48c4-b3bf-1e4a10abda6f/userFiles-960e7b42-c0c2-4b0d-b262-4e9d8197476a/fetchFileTemp6603977232356661802.tmp has been previously copied to /tmp/spark-64875627-1bdb-48c4-b3bf-1e4a10abda6f/userFiles-960e7b42-c0c2-4b0d-b262-4e9d8197476a/org.apache.commons_commons-lang3-3.10.jar
[2025-07-10T12:13:56.664+0200] {subprocess.py:93} INFO - 25/07/10 12:13:56 INFO Executor: Adding file:/tmp/spark-64875627-1bdb-48c4-b3bf-1e4a10abda6f/userFiles-960e7b42-c0c2-4b0d-b262-4e9d8197476a/org.apache.commons_commons-lang3-3.10.jar to class loader default
[2025-07-10T12:13:56.664+0200] {subprocess.py:93} INFO - 25/07/10 12:13:56 INFO Executor: Fetching spark://138.4.31.17:40375/jars/com.thoughtworks.paranamer_paranamer-2.8.jar with timestamp 1752142434871
[2025-07-10T12:13:56.665+0200] {subprocess.py:93} INFO - 25/07/10 12:13:56 INFO Utils: Fetching spark://138.4.31.17:40375/jars/com.thoughtworks.paranamer_paranamer-2.8.jar to /tmp/spark-64875627-1bdb-48c4-b3bf-1e4a10abda6f/userFiles-960e7b42-c0c2-4b0d-b262-4e9d8197476a/fetchFileTemp6611106587738314045.tmp
[2025-07-10T12:13:56.666+0200] {subprocess.py:93} INFO - 25/07/10 12:13:56 INFO Utils: /tmp/spark-64875627-1bdb-48c4-b3bf-1e4a10abda6f/userFiles-960e7b42-c0c2-4b0d-b262-4e9d8197476a/fetchFileTemp6611106587738314045.tmp has been previously copied to /tmp/spark-64875627-1bdb-48c4-b3bf-1e4a10abda6f/userFiles-960e7b42-c0c2-4b0d-b262-4e9d8197476a/com.thoughtworks.paranamer_paranamer-2.8.jar
[2025-07-10T12:13:56.670+0200] {subprocess.py:93} INFO - 25/07/10 12:13:56 INFO Executor: Adding file:/tmp/spark-64875627-1bdb-48c4-b3bf-1e4a10abda6f/userFiles-960e7b42-c0c2-4b0d-b262-4e9d8197476a/com.thoughtworks.paranamer_paranamer-2.8.jar to class loader default
[2025-07-10T12:13:56.677+0200] {subprocess.py:93} INFO - 25/07/10 12:13:56 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 32855.
[2025-07-10T12:13:56.678+0200] {subprocess.py:93} INFO - 25/07/10 12:13:56 INFO NettyBlockTransferService: Server created on 138.4.31.17:32855
[2025-07-10T12:13:56.679+0200] {subprocess.py:93} INFO - 25/07/10 12:13:56 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[2025-07-10T12:13:56.686+0200] {subprocess.py:93} INFO - 25/07/10 12:13:56 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 138.4.31.17, 32855, None)
[2025-07-10T12:13:56.689+0200] {subprocess.py:93} INFO - 25/07/10 12:13:56 INFO BlockManagerMasterEndpoint: Registering block manager 138.4.31.17:32855 with 434.4 MiB RAM, BlockManagerId(driver, 138.4.31.17, 32855, None)
[2025-07-10T12:13:56.691+0200] {subprocess.py:93} INFO - 25/07/10 12:13:56 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 138.4.31.17, 32855, None)
[2025-07-10T12:13:56.692+0200] {subprocess.py:93} INFO - 25/07/10 12:13:56 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 138.4.31.17, 32855, None)
[2025-07-10T12:13:57.169+0200] {subprocess.py:93} INFO - 25/07/10 12:13:57 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
[2025-07-10T12:13:57.171+0200] {subprocess.py:93} INFO - 25/07/10 12:13:57 INFO SharedState: Warehouse path is 'file:/tmp/airflowtmpxvk4qa81/spark-warehouse'.
[2025-07-10T12:13:58.164+0200] {subprocess.py:93} INFO - 25/07/10 12:13:58 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.
[2025-07-10T12:13:59.462+0200] {subprocess.py:93} INFO - MLflow Run ID: f3af17c8fc474d6eb3af8c9b3209bf7d
[2025-07-10T12:13:59.462+0200] {subprocess.py:93} INFO - MLflow Tracking URI: file:///home/monica.fernandez/practica_creativa/mlruns
[2025-07-10T12:13:59.709+0200] {subprocess.py:93} INFO - 25/07/10 12:13:59 INFO InMemoryFileIndex: It took 53 ms to list leaf files for 1 paths.
[2025-07-10T12:14:01.703+0200] {subprocess.py:93} INFO - 25/07/10 12:14:01 INFO DefaultMavenCoordinates: DataStax Java driver for Apache Cassandra(R) (com.datastax.oss:java-driver-core-shaded) version 4.13.0
[2025-07-10T12:14:01.844+0200] {subprocess.py:93} INFO - 25/07/10 12:14:01 INFO Native: Unable to load JNR native implementation. This could be normal if JNR is excluded from the classpath
[2025-07-10T12:14:01.844+0200] {subprocess.py:93} INFO - java.lang.NoClassDefFoundError: jnr/posix/POSIXHandler
[2025-07-10T12:14:01.844+0200] {subprocess.py:93} INFO - 	at com.datastax.oss.driver.internal.core.os.Native$LibcLoader.load(Native.java:42)
[2025-07-10T12:14:01.844+0200] {subprocess.py:93} INFO - 	at com.datastax.oss.driver.internal.core.os.Native.<clinit>(Native.java:59)
[2025-07-10T12:14:01.844+0200] {subprocess.py:93} INFO - 	at com.datastax.oss.driver.internal.core.time.Clock.getInstance(Clock.java:41)
[2025-07-10T12:14:01.844+0200] {subprocess.py:93} INFO - 	at com.datastax.oss.driver.internal.core.time.MonotonicTimestampGenerator.buildClock(MonotonicTimestampGenerator.java:109)
[2025-07-10T12:14:01.844+0200] {subprocess.py:93} INFO - 	at com.datastax.oss.driver.internal.core.time.MonotonicTimestampGenerator.<init>(MonotonicTimestampGenerator.java:43)
[2025-07-10T12:14:01.845+0200] {subprocess.py:93} INFO - 	at com.datastax.oss.driver.internal.core.time.AtomicTimestampGenerator.<init>(AtomicTimestampGenerator.java:52)
[2025-07-10T12:14:01.845+0200] {subprocess.py:93} INFO - 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
[2025-07-10T12:14:01.845+0200] {subprocess.py:93} INFO - 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:77)
[2025-07-10T12:14:01.845+0200] {subprocess.py:93} INFO - 	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
[2025-07-10T12:14:01.845+0200] {subprocess.py:93} INFO - 	at java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:500)
[2025-07-10T12:14:01.845+0200] {subprocess.py:93} INFO - 	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:481)
[2025-07-10T12:14:01.845+0200] {subprocess.py:93} INFO - 	at com.datastax.oss.driver.internal.core.util.Reflection.resolveClass(Reflection.java:329)
[2025-07-10T12:14:01.845+0200] {subprocess.py:93} INFO - 	at com.datastax.oss.driver.internal.core.util.Reflection.buildFromConfig(Reflection.java:235)
[2025-07-10T12:14:01.845+0200] {subprocess.py:93} INFO - 	at com.datastax.oss.driver.internal.core.util.Reflection.buildFromConfig(Reflection.java:110)
[2025-07-10T12:14:01.845+0200] {subprocess.py:93} INFO - 	at com.datastax.oss.driver.internal.core.context.DefaultDriverContext.buildTimestampGenerator(DefaultDriverContext.java:377)
[2025-07-10T12:14:01.845+0200] {subprocess.py:93} INFO - 	at com.datastax.oss.driver.internal.core.util.concurrent.LazyReference.get(LazyReference.java:55)
[2025-07-10T12:14:01.845+0200] {subprocess.py:93} INFO - 	at com.datastax.oss.driver.internal.core.context.DefaultDriverContext.getTimestampGenerator(DefaultDriverContext.java:773)
[2025-07-10T12:14:01.845+0200] {subprocess.py:93} INFO - 	at com.datastax.oss.driver.internal.core.session.DefaultSession$SingleThreaded.init(DefaultSession.java:349)
[2025-07-10T12:14:01.845+0200] {subprocess.py:93} INFO - 	at com.datastax.oss.driver.internal.core.session.DefaultSession$SingleThreaded.access$1100(DefaultSession.java:300)
[2025-07-10T12:14:01.845+0200] {subprocess.py:93} INFO - 	at com.datastax.oss.driver.internal.core.session.DefaultSession.lambda$init$0(DefaultSession.java:146)
[2025-07-10T12:14:01.845+0200] {subprocess.py:93} INFO - 	at com.datastax.oss.driver.shaded.netty.util.concurrent.PromiseTask.runTask(PromiseTask.java:98)
[2025-07-10T12:14:01.845+0200] {subprocess.py:93} INFO - 	at com.datastax.oss.driver.shaded.netty.util.concurrent.PromiseTask.run(PromiseTask.java:106)
[2025-07-10T12:14:01.845+0200] {subprocess.py:93} INFO - 	at com.datastax.oss.driver.shaded.netty.channel.DefaultEventLoop.run(DefaultEventLoop.java:54)
[2025-07-10T12:14:01.845+0200] {subprocess.py:93} INFO - 	at com.datastax.oss.driver.shaded.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989)
[2025-07-10T12:14:01.845+0200] {subprocess.py:93} INFO - 	at com.datastax.oss.driver.shaded.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
[2025-07-10T12:14:01.845+0200] {subprocess.py:93} INFO - 	at com.datastax.oss.driver.shaded.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
[2025-07-10T12:14:01.846+0200] {subprocess.py:93} INFO - 	at java.base/java.lang.Thread.run(Thread.java:840)
[2025-07-10T12:14:01.846+0200] {subprocess.py:93} INFO - Caused by: java.lang.ClassNotFoundException: jnr.posix.POSIXHandler
[2025-07-10T12:14:01.846+0200] {subprocess.py:93} INFO - 	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:445)
[2025-07-10T12:14:01.846+0200] {subprocess.py:93} INFO - 	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:592)
[2025-07-10T12:14:01.846+0200] {subprocess.py:93} INFO - 	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:525)
[2025-07-10T12:14:01.846+0200] {subprocess.py:93} INFO - 	... 27 more
[2025-07-10T12:14:01.846+0200] {subprocess.py:93} INFO - 25/07/10 12:14:01 INFO Clock: Could not access native clock (see debug logs for details), falling back to Java system clock
[2025-07-10T12:14:02.363+0200] {subprocess.py:93} INFO - 25/07/10 12:14:02 INFO CassandraConnector: Connected to Cassandra cluster.
[2025-07-10T12:14:02.965+0200] {subprocess.py:93} INFO - 25/07/10 12:14:02 INFO FileSystemOverwrite: Path /home/monica.fernandez/practica_creativa/models/arrival_bucketizer_2.0.bin already exists. It will be overwritten.
[2025-07-10T12:14:03.224+0200] {subprocess.py:93} INFO - 25/07/10 12:14:03 INFO deprecation: mapred.output.dir is deprecated. Instead, use mapreduce.output.fileoutputformat.outputdir
[2025-07-10T12:14:03.228+0200] {subprocess.py:93} INFO - 25/07/10 12:14:03 INFO HadoopMapRedCommitProtocol: Using output committer class org.apache.hadoop.mapred.FileOutputCommitter
[2025-07-10T12:14:03.230+0200] {subprocess.py:93} INFO - 25/07/10 12:14:03 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2025-07-10T12:14:03.230+0200] {subprocess.py:93} INFO - 25/07/10 12:14:03 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2025-07-10T12:14:03.286+0200] {subprocess.py:93} INFO - 25/07/10 12:14:03 INFO SparkContext: Starting job: runJob at SparkHadoopWriter.scala:83
[2025-07-10T12:14:03.301+0200] {subprocess.py:93} INFO - 25/07/10 12:14:03 INFO DAGScheduler: Got job 0 (runJob at SparkHadoopWriter.scala:83) with 1 output partitions
[2025-07-10T12:14:03.301+0200] {subprocess.py:93} INFO - 25/07/10 12:14:03 INFO DAGScheduler: Final stage: ResultStage 0 (runJob at SparkHadoopWriter.scala:83)
[2025-07-10T12:14:03.302+0200] {subprocess.py:93} INFO - 25/07/10 12:14:03 INFO DAGScheduler: Parents of final stage: List()
[2025-07-10T12:14:03.302+0200] {subprocess.py:93} INFO - 25/07/10 12:14:03 INFO DAGScheduler: Missing parents: List()
[2025-07-10T12:14:03.305+0200] {subprocess.py:93} INFO - 25/07/10 12:14:03 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[1] at saveAsTextFile at ReadWrite.scala:413), which has no missing parents
[2025-07-10T12:14:03.380+0200] {subprocess.py:93} INFO - 25/07/10 12:14:03 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 101.3 KiB, free 434.3 MiB)
[2025-07-10T12:14:03.435+0200] {subprocess.py:93} INFO - 25/07/10 12:14:03 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 36.5 KiB, free 434.3 MiB)
[2025-07-10T12:14:03.438+0200] {subprocess.py:93} INFO - 25/07/10 12:14:03 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 138.4.31.17:32855 (size: 36.5 KiB, free: 434.4 MiB)
[2025-07-10T12:14:03.440+0200] {subprocess.py:93} INFO - 25/07/10 12:14:03 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1585
[2025-07-10T12:14:03.460+0200] {subprocess.py:93} INFO - 25/07/10 12:14:03 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[1] at saveAsTextFile at ReadWrite.scala:413) (first 15 tasks are for partitions Vector(0))
[2025-07-10T12:14:03.461+0200] {subprocess.py:93} INFO - 25/07/10 12:14:03 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
[2025-07-10T12:14:03.538+0200] {subprocess.py:93} INFO - 25/07/10 12:14:03 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (138.4.31.17, executor driver, partition 0, PROCESS_LOCAL, 13275 bytes)
[2025-07-10T12:14:03.553+0200] {subprocess.py:93} INFO - 25/07/10 12:14:03 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
[2025-07-10T12:14:03.645+0200] {subprocess.py:93} INFO - 25/07/10 12:14:03 INFO HadoopMapRedCommitProtocol: Using output committer class org.apache.hadoop.mapred.FileOutputCommitter
[2025-07-10T12:14:03.645+0200] {subprocess.py:93} INFO - 25/07/10 12:14:03 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2025-07-10T12:14:03.645+0200] {subprocess.py:93} INFO - 25/07/10 12:14:03 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2025-07-10T12:14:03.691+0200] {subprocess.py:93} INFO - 25/07/10 12:14:03 INFO FileOutputCommitter: Saved output of task 'attempt_202507101214037341631513906327663_0001_m_000000_0' to file:/home/monica.fernandez/practica_creativa/models/arrival_bucketizer_2.0.bin/metadata/_temporary/0/task_202507101214037341631513906327663_0001_m_000000
[2025-07-10T12:14:03.691+0200] {subprocess.py:93} INFO - 25/07/10 12:14:03 INFO SparkHadoopMapRedUtil: attempt_202507101214037341631513906327663_0001_m_000000_0: Committed. Elapsed time: 1 ms.
[2025-07-10T12:14:03.719+0200] {subprocess.py:93} INFO - 25/07/10 12:14:03 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 1256 bytes result sent to driver
[2025-07-10T12:14:03.733+0200] {subprocess.py:93} INFO - 25/07/10 12:14:03 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 225 ms on 138.4.31.17 (executor driver) (1/1)
[2025-07-10T12:14:03.735+0200] {subprocess.py:93} INFO - 25/07/10 12:14:03 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool
[2025-07-10T12:14:03.740+0200] {subprocess.py:93} INFO - 25/07/10 12:14:03 INFO DAGScheduler: ResultStage 0 (runJob at SparkHadoopWriter.scala:83) finished in 0,420 s
[2025-07-10T12:14:03.742+0200] {subprocess.py:93} INFO - 25/07/10 12:14:03 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
[2025-07-10T12:14:03.743+0200] {subprocess.py:93} INFO - 25/07/10 12:14:03 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
[2025-07-10T12:14:03.745+0200] {subprocess.py:93} INFO - 25/07/10 12:14:03 INFO DAGScheduler: Job 0 finished: runJob at SparkHadoopWriter.scala:83, took 0,458515 s
[2025-07-10T12:14:03.747+0200] {subprocess.py:93} INFO - 25/07/10 12:14:03 INFO SparkHadoopWriter: Start to commit write Job job_202507101214037341631513906327663_0001.
[2025-07-10T12:14:03.770+0200] {subprocess.py:93} INFO - 25/07/10 12:14:03 INFO SparkHadoopWriter: Write Job job_202507101214037341631513906327663_0001 committed. Elapsed time: 23 ms.
[2025-07-10T12:14:04.334+0200] {subprocess.py:93} INFO - 25/07/10 12:14:04 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 138.4.31.17:32855 in memory (size: 36.5 KiB, free: 434.4 MiB)
[2025-07-10T12:14:04.389+0200] {subprocess.py:93} INFO - 25/07/10 12:14:04 INFO V2ScanRelationPushDown:
[2025-07-10T12:14:04.389+0200] {subprocess.py:93} INFO - Pushing operators to origin_dest_distance
[2025-07-10T12:14:04.389+0200] {subprocess.py:93} INFO - Pushed Filters:
[2025-07-10T12:14:04.389+0200] {subprocess.py:93} INFO - Post-Scan Filters:
[2025-07-10T12:14:04.389+0200] {subprocess.py:93} INFO - 
[2025-07-10T12:14:04.460+0200] {subprocess.py:93} INFO - 25/07/10 12:14:04 INFO V2ScanRelationPushDown:
[2025-07-10T12:14:04.460+0200] {subprocess.py:93} INFO - Output: origin#42, dest#43
[2025-07-10T12:14:04.460+0200] {subprocess.py:93} INFO - 
[2025-07-10T12:14:04.763+0200] {subprocess.py:93} INFO - 25/07/10 12:14:04 INFO FileSourceStrategy: Pushed Filters:
[2025-07-10T12:14:04.764+0200] {subprocess.py:93} INFO - 25/07/10 12:14:04 INFO FileSourceStrategy: Post-Scan Filters:
[2025-07-10T12:14:04.968+0200] {subprocess.py:93} INFO - 25/07/10 12:14:04 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 209.9 KiB, free 434.2 MiB)
[2025-07-10T12:14:04.978+0200] {subprocess.py:93} INFO - 25/07/10 12:14:04 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 37.7 KiB, free 434.2 MiB)
[2025-07-10T12:14:04.978+0200] {subprocess.py:93} INFO - 25/07/10 12:14:04 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 138.4.31.17:32855 (size: 37.7 KiB, free: 434.4 MiB)
[2025-07-10T12:14:04.979+0200] {subprocess.py:93} INFO - 25/07/10 12:14:04 INFO SparkContext: Created broadcast 1 from collect at StringIndexer.scala:204
[2025-07-10T12:14:04.996+0200] {subprocess.py:93} INFO - 25/07/10 12:14:04 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
[2025-07-10T12:14:05.050+0200] {subprocess.py:93} INFO - 25/07/10 12:14:05 INFO DAGScheduler: Registering RDD 4 (collect at StringIndexer.scala:204) as input to shuffle 0
[2025-07-10T12:14:05.054+0200] {subprocess.py:93} INFO - 25/07/10 12:14:05 INFO DAGScheduler: Got map stage job 1 (collect at StringIndexer.scala:204) with 2 output partitions
[2025-07-10T12:14:05.054+0200] {subprocess.py:93} INFO - 25/07/10 12:14:05 INFO DAGScheduler: Final stage: ShuffleMapStage 1 (collect at StringIndexer.scala:204)
[2025-07-10T12:14:05.054+0200] {subprocess.py:93} INFO - 25/07/10 12:14:05 INFO DAGScheduler: Parents of final stage: List()
[2025-07-10T12:14:05.055+0200] {subprocess.py:93} INFO - 25/07/10 12:14:05 INFO DAGScheduler: Missing parents: List()
[2025-07-10T12:14:05.058+0200] {subprocess.py:93} INFO - 25/07/10 12:14:05 INFO DAGScheduler: Submitting ShuffleMapStage 1 (MapPartitionsRDD[4] at collect at StringIndexer.scala:204), which has no missing parents
[2025-07-10T12:14:05.099+0200] {subprocess.py:93} INFO - 25/07/10 12:14:05 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 13.8 KiB, free 434.1 MiB)
[2025-07-10T12:14:05.100+0200] {subprocess.py:93} INFO - 25/07/10 12:14:05 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 7.1 KiB, free 434.1 MiB)
[2025-07-10T12:14:05.101+0200] {subprocess.py:93} INFO - 25/07/10 12:14:05 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 138.4.31.17:32855 (size: 7.1 KiB, free: 434.4 MiB)
[2025-07-10T12:14:05.102+0200] {subprocess.py:93} INFO - 25/07/10 12:14:05 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1585
[2025-07-10T12:14:05.104+0200] {subprocess.py:93} INFO - 25/07/10 12:14:05 INFO DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 1 (MapPartitionsRDD[4] at collect at StringIndexer.scala:204) (first 15 tasks are for partitions Vector(0, 1))
[2025-07-10T12:14:05.104+0200] {subprocess.py:93} INFO - 25/07/10 12:14:05 INFO TaskSchedulerImpl: Adding task set 1.0 with 2 tasks resource profile 0
[2025-07-10T12:14:05.108+0200] {subprocess.py:93} INFO - 25/07/10 12:14:05 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (138.4.31.17, executor driver, partition 0, PROCESS_LOCAL, 13595 bytes)
[2025-07-10T12:14:05.111+0200] {subprocess.py:93} INFO - 25/07/10 12:14:05 INFO TaskSetManager: Starting task 1.0 in stage 1.0 (TID 2) (138.4.31.17, executor driver, partition 1, PROCESS_LOCAL, 13595 bytes)
[2025-07-10T12:14:05.113+0200] {subprocess.py:93} INFO - 25/07/10 12:14:05 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)
[2025-07-10T12:14:05.115+0200] {subprocess.py:93} INFO - 25/07/10 12:14:05 INFO Executor: Running task 1.0 in stage 1.0 (TID 2)
[2025-07-10T12:14:05.301+0200] {subprocess.py:93} INFO - 25/07/10 12:14:05 INFO CodeGenerator: Code generated in 202.338139 ms
[2025-07-10T12:14:05.303+0200] {subprocess.py:93} INFO - 25/07/10 12:14:05 INFO CodeGenerator: Code generated in 129.344452 ms
[2025-07-10T12:14:05.317+0200] {subprocess.py:93} INFO - 25/07/10 12:14:05 INFO DAGScheduler: Registering RDD 8 (collect at StringIndexer.scala:204) as input to shuffle 1
[2025-07-10T12:14:05.317+0200] {subprocess.py:93} INFO - 25/07/10 12:14:05 INFO DAGScheduler: Got map stage job 2 (collect at StringIndexer.scala:204) with 13 output partitions
[2025-07-10T12:14:05.317+0200] {subprocess.py:93} INFO - 25/07/10 12:14:05 INFO DAGScheduler: Final stage: ShuffleMapStage 2 (collect at StringIndexer.scala:204)
[2025-07-10T12:14:05.317+0200] {subprocess.py:93} INFO - 25/07/10 12:14:05 INFO DAGScheduler: Parents of final stage: List()
[2025-07-10T12:14:05.317+0200] {subprocess.py:93} INFO - 25/07/10 12:14:05 INFO DAGScheduler: Missing parents: List()
[2025-07-10T12:14:05.318+0200] {subprocess.py:93} INFO - 25/07/10 12:14:05 INFO FileScanRDD: Reading File path: file:///home/monica.fernandez/practica_creativa/data/simple_flight_delay_features.jsonl.bz2, range: 4194304-4676447, partition values: [empty row]
[2025-07-10T12:14:05.318+0200] {subprocess.py:93} INFO - 25/07/10 12:14:05 INFO DAGScheduler: Submitting ShuffleMapStage 2 (MapPartitionsRDD[8] at collect at StringIndexer.scala:204), which has no missing parents
[2025-07-10T12:14:05.319+0200] {subprocess.py:93} INFO - 25/07/10 12:14:05 INFO FileScanRDD: Reading File path: file:///home/monica.fernandez/practica_creativa/data/simple_flight_delay_features.jsonl.bz2, range: 0-4194304, partition values: [empty row]
[2025-07-10T12:14:05.334+0200] {subprocess.py:93} INFO - 25/07/10 12:14:05 INFO CodeGenerator: Code generated in 12.926968 ms
[2025-07-10T12:14:05.360+0200] {subprocess.py:93} INFO - 25/07/10 12:14:05 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 18.2 KiB, free 434.1 MiB)
[2025-07-10T12:14:05.364+0200] {subprocess.py:93} INFO - 25/07/10 12:14:05 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 9.0 KiB, free 434.1 MiB)
[2025-07-10T12:14:05.365+0200] {subprocess.py:93} INFO - 25/07/10 12:14:05 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 138.4.31.17:32855 (size: 9.0 KiB, free: 434.3 MiB)
[2025-07-10T12:14:05.366+0200] {subprocess.py:93} INFO - 25/07/10 12:14:05 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1585
[2025-07-10T12:14:05.367+0200] {subprocess.py:93} INFO - 25/07/10 12:14:05 INFO DAGScheduler: Submitting 13 missing tasks from ShuffleMapStage 2 (MapPartitionsRDD[8] at collect at StringIndexer.scala:204) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12))
[2025-07-10T12:14:05.367+0200] {subprocess.py:93} INFO - 25/07/10 12:14:05 INFO TaskSchedulerImpl: Adding task set 2.0 with 13 tasks resource profile 0
[2025-07-10T12:14:05.376+0200] {subprocess.py:93} INFO - 25/07/10 12:14:05 INFO CodecPool: Got brand-new decompressor [.bz2]
[2025-07-10T12:14:05.377+0200] {subprocess.py:93} INFO - 25/07/10 12:14:05 INFO CodecPool: Got brand-new decompressor [.bz2]
[2025-07-10T12:14:05.380+0200] {subprocess.py:93} INFO - 25/07/10 12:14:05 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 3) (138.4.31.17, executor driver, partition 0, ANY, 14843 bytes)
[2025-07-10T12:14:05.389+0200] {subprocess.py:93} INFO - 25/07/10 12:14:05 INFO TaskSetManager: Starting task 1.0 in stage 2.0 (TID 4) (138.4.31.17, executor driver, partition 1, ANY, 14843 bytes)
[2025-07-10T12:14:05.402+0200] {subprocess.py:93} INFO - 25/07/10 12:14:05 INFO Executor: Running task 0.0 in stage 2.0 (TID 3)
[2025-07-10T12:14:05.423+0200] {subprocess.py:93} INFO - 25/07/10 12:14:05 INFO Executor: Running task 1.0 in stage 2.0 (TID 4)
[2025-07-10T12:14:05.474+0200] {subprocess.py:93} INFO - 25/07/10 12:14:05 INFO CodeGenerator: Code generated in 33.792472 ms
[2025-07-10T12:14:05.503+0200] {subprocess.py:93} INFO - 25/07/10 12:14:05 INFO CodeGenerator: Code generated in 20.885663 ms
[2025-07-10T12:14:05.937+0200] {subprocess.py:93} INFO - 25/07/10 12:14:05 INFO Executor: Finished task 0.0 in stage 2.0 (TID 3). 1902 bytes result sent to driver
[2025-07-10T12:14:05.943+0200] {subprocess.py:93} INFO - 25/07/10 12:14:05 INFO Executor: Finished task 1.0 in stage 2.0 (TID 4). 1859 bytes result sent to driver
[2025-07-10T12:14:05.944+0200] {subprocess.py:93} INFO - 25/07/10 12:14:05 INFO TaskSetManager: Starting task 2.0 in stage 2.0 (TID 5) (138.4.31.17, executor driver, partition 2, ANY, 14843 bytes)
[2025-07-10T12:14:05.945+0200] {subprocess.py:93} INFO - 25/07/10 12:14:05 INFO Executor: Running task 2.0 in stage 2.0 (TID 5)
[2025-07-10T12:14:05.951+0200] {subprocess.py:93} INFO - 25/07/10 12:14:05 INFO TaskSetManager: Starting task 3.0 in stage 2.0 (TID 6) (138.4.31.17, executor driver, partition 3, ANY, 14963 bytes)
[2025-07-10T12:14:05.954+0200] {subprocess.py:93} INFO - 25/07/10 12:14:05 INFO Executor: Running task 3.0 in stage 2.0 (TID 6)
[2025-07-10T12:14:05.964+0200] {subprocess.py:93} INFO - 25/07/10 12:14:05 INFO TaskSetManager: Finished task 1.0 in stage 2.0 (TID 4) in 583 ms on 138.4.31.17 (executor driver) (1/13)
[2025-07-10T12:14:05.972+0200] {subprocess.py:93} INFO - 25/07/10 12:14:05 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 3) in 600 ms on 138.4.31.17 (executor driver) (2/13)
[2025-07-10T12:14:06.005+0200] {subprocess.py:93} INFO - 25/07/10 12:14:06 INFO Executor: Finished task 3.0 in stage 2.0 (TID 6). 1816 bytes result sent to driver
[2025-07-10T12:14:06.016+0200] {subprocess.py:93} INFO - 25/07/10 12:14:06 INFO Executor: Finished task 2.0 in stage 2.0 (TID 5). 1816 bytes result sent to driver
[2025-07-10T12:14:06.016+0200] {subprocess.py:93} INFO - 25/07/10 12:14:06 INFO TaskSetManager: Starting task 4.0 in stage 2.0 (TID 7) (138.4.31.17, executor driver, partition 4, ANY, 14843 bytes)
[2025-07-10T12:14:06.016+0200] {subprocess.py:93} INFO - 25/07/10 12:14:06 INFO Executor: Running task 4.0 in stage 2.0 (TID 7)
[2025-07-10T12:14:06.016+0200] {subprocess.py:93} INFO - 25/07/10 12:14:06 INFO TaskSetManager: Starting task 5.0 in stage 2.0 (TID 8) (138.4.31.17, executor driver, partition 5, ANY, 14843 bytes)
[2025-07-10T12:14:06.018+0200] {subprocess.py:93} INFO - 25/07/10 12:14:06 INFO Executor: Running task 5.0 in stage 2.0 (TID 8)
[2025-07-10T12:14:06.018+0200] {subprocess.py:93} INFO - 25/07/10 12:14:06 INFO TaskSetManager: Finished task 2.0 in stage 2.0 (TID 5) in 74 ms on 138.4.31.17 (executor driver) (3/13)
[2025-07-10T12:14:06.023+0200] {subprocess.py:93} INFO - 25/07/10 12:14:06 INFO TaskSetManager: Finished task 3.0 in stage 2.0 (TID 6) in 72 ms on 138.4.31.17 (executor driver) (4/13)
[2025-07-10T12:14:06.044+0200] {subprocess.py:93} INFO - 25/07/10 12:14:06 INFO Executor: Finished task 4.0 in stage 2.0 (TID 7). 1816 bytes result sent to driver
[2025-07-10T12:14:06.046+0200] {subprocess.py:93} INFO - 25/07/10 12:14:06 INFO TaskSetManager: Starting task 6.0 in stage 2.0 (TID 9) (138.4.31.17, executor driver, partition 6, ANY, 14843 bytes)
[2025-07-10T12:14:06.048+0200] {subprocess.py:93} INFO - 25/07/10 12:14:06 INFO TaskSetManager: Finished task 4.0 in stage 2.0 (TID 7) in 33 ms on 138.4.31.17 (executor driver) (5/13)
[2025-07-10T12:14:06.051+0200] {subprocess.py:93} INFO - 25/07/10 12:14:06 INFO Executor: Finished task 5.0 in stage 2.0 (TID 8). 1816 bytes result sent to driver
[2025-07-10T12:14:06.053+0200] {subprocess.py:93} INFO - 25/07/10 12:14:06 INFO Executor: Running task 6.0 in stage 2.0 (TID 9)
[2025-07-10T12:14:06.053+0200] {subprocess.py:93} INFO - 25/07/10 12:14:06 INFO TaskSetManager: Starting task 7.0 in stage 2.0 (TID 10) (138.4.31.17, executor driver, partition 7, ANY, 14843 bytes)
[2025-07-10T12:14:06.053+0200] {subprocess.py:93} INFO - 25/07/10 12:14:06 INFO TaskSetManager: Finished task 5.0 in stage 2.0 (TID 8) in 38 ms on 138.4.31.17 (executor driver) (6/13)
[2025-07-10T12:14:06.054+0200] {subprocess.py:93} INFO - 25/07/10 12:14:06 INFO Executor: Running task 7.0 in stage 2.0 (TID 10)
[2025-07-10T12:14:06.072+0200] {subprocess.py:93} INFO - 25/07/10 12:14:06 INFO Executor: Finished task 7.0 in stage 2.0 (TID 10). 1816 bytes result sent to driver
[2025-07-10T12:14:06.074+0200] {subprocess.py:93} INFO - 25/07/10 12:14:06 INFO TaskSetManager: Starting task 8.0 in stage 2.0 (TID 11) (138.4.31.17, executor driver, partition 8, ANY, 14843 bytes)
[2025-07-10T12:14:06.075+0200] {subprocess.py:93} INFO - 25/07/10 12:14:06 INFO TaskSetManager: Finished task 7.0 in stage 2.0 (TID 10) in 23 ms on 138.4.31.17 (executor driver) (7/13)
[2025-07-10T12:14:06.078+0200] {subprocess.py:93} INFO - 25/07/10 12:14:06 INFO Executor: Running task 8.0 in stage 2.0 (TID 11)
[2025-07-10T12:14:06.096+0200] {subprocess.py:93} INFO - 25/07/10 12:14:06 INFO Executor: Finished task 8.0 in stage 2.0 (TID 11). 1816 bytes result sent to driver
[2025-07-10T12:14:06.098+0200] {subprocess.py:93} INFO - 25/07/10 12:14:06 INFO TaskSetManager: Starting task 9.0 in stage 2.0 (TID 12) (138.4.31.17, executor driver, partition 9, ANY, 14963 bytes)
[2025-07-10T12:14:06.098+0200] {subprocess.py:93} INFO - 25/07/10 12:14:06 INFO Executor: Running task 9.0 in stage 2.0 (TID 12)
[2025-07-10T12:14:06.100+0200] {subprocess.py:93} INFO - 25/07/10 12:14:06 INFO TaskSetManager: Finished task 8.0 in stage 2.0 (TID 11) in 27 ms on 138.4.31.17 (executor driver) (8/13)
[2025-07-10T12:14:06.125+0200] {subprocess.py:93} INFO - 25/07/10 12:14:06 INFO Executor: Finished task 6.0 in stage 2.0 (TID 9). 1859 bytes result sent to driver
[2025-07-10T12:14:06.128+0200] {subprocess.py:93} INFO - 25/07/10 12:14:06 INFO TaskSetManager: Starting task 10.0 in stage 2.0 (TID 13) (138.4.31.17, executor driver, partition 10, ANY, 14963 bytes)
[2025-07-10T12:14:06.130+0200] {subprocess.py:93} INFO - 25/07/10 12:14:06 INFO TaskSetManager: Finished task 6.0 in stage 2.0 (TID 9) in 84 ms on 138.4.31.17 (executor driver) (9/13)
[2025-07-10T12:14:06.132+0200] {subprocess.py:93} INFO - 25/07/10 12:14:06 INFO Executor: Running task 10.0 in stage 2.0 (TID 13)
[2025-07-10T12:14:06.149+0200] {subprocess.py:93} INFO - 25/07/10 12:14:06 INFO Executor: Finished task 9.0 in stage 2.0 (TID 12). 1816 bytes result sent to driver
[2025-07-10T12:14:06.153+0200] {subprocess.py:93} INFO - 25/07/10 12:14:06 INFO TaskSetManager: Starting task 11.0 in stage 2.0 (TID 14) (138.4.31.17, executor driver, partition 11, ANY, 14839 bytes)
[2025-07-10T12:14:06.153+0200] {subprocess.py:93} INFO - 25/07/10 12:14:06 INFO TaskSetManager: Finished task 9.0 in stage 2.0 (TID 12) in 56 ms on 138.4.31.17 (executor driver) (10/13)
[2025-07-10T12:14:06.155+0200] {subprocess.py:93} INFO - 25/07/10 12:14:06 INFO Executor: Running task 11.0 in stage 2.0 (TID 14)
[2025-07-10T12:14:06.186+0200] {subprocess.py:93} INFO - 25/07/10 12:14:06 INFO Executor: Finished task 10.0 in stage 2.0 (TID 13). 1816 bytes result sent to driver
[2025-07-10T12:14:06.189+0200] {subprocess.py:93} INFO - 25/07/10 12:14:06 INFO TaskSetManager: Starting task 12.0 in stage 2.0 (TID 15) (138.4.31.17, executor driver, partition 12, ANY, 14843 bytes)
[2025-07-10T12:14:06.190+0200] {subprocess.py:93} INFO - 25/07/10 12:14:06 INFO TaskSetManager: Finished task 10.0 in stage 2.0 (TID 13) in 62 ms on 138.4.31.17 (executor driver) (11/13)
[2025-07-10T12:14:06.190+0200] {subprocess.py:93} INFO - 25/07/10 12:14:06 INFO Executor: Running task 12.0 in stage 2.0 (TID 15)
[2025-07-10T12:14:06.190+0200] {subprocess.py:93} INFO - 25/07/10 12:14:06 INFO Executor: Finished task 11.0 in stage 2.0 (TID 14). 1816 bytes result sent to driver
[2025-07-10T12:14:06.193+0200] {subprocess.py:93} INFO - 25/07/10 12:14:06 INFO TaskSetManager: Finished task 11.0 in stage 2.0 (TID 14) in 41 ms on 138.4.31.17 (executor driver) (12/13)
[2025-07-10T12:14:06.221+0200] {subprocess.py:93} INFO - 25/07/10 12:14:06 INFO Executor: Finished task 12.0 in stage 2.0 (TID 15). 1816 bytes result sent to driver
[2025-07-10T12:14:06.226+0200] {subprocess.py:93} INFO - 25/07/10 12:14:06 INFO TaskSetManager: Finished task 12.0 in stage 2.0 (TID 15) in 36 ms on 138.4.31.17 (executor driver) (13/13)
[2025-07-10T12:14:06.226+0200] {subprocess.py:93} INFO - 25/07/10 12:14:06 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool
[2025-07-10T12:14:06.226+0200] {subprocess.py:93} INFO - 25/07/10 12:14:06 INFO DAGScheduler: ShuffleMapStage 2 (collect at StringIndexer.scala:204) finished in 0,902 s
[2025-07-10T12:14:06.231+0200] {subprocess.py:93} INFO - 25/07/10 12:14:06 INFO DAGScheduler: looking for newly runnable stages
[2025-07-10T12:14:06.231+0200] {subprocess.py:93} INFO - 25/07/10 12:14:06 INFO DAGScheduler: running: Set(ShuffleMapStage 1)
[2025-07-10T12:14:06.232+0200] {subprocess.py:93} INFO - 25/07/10 12:14:06 INFO DAGScheduler: waiting: Set()
[2025-07-10T12:14:06.232+0200] {subprocess.py:93} INFO - 25/07/10 12:14:06 INFO DAGScheduler: failed: Set()
[2025-07-10T12:14:07.498+0200] {subprocess.py:93} INFO - 25/07/10 12:14:07 INFO Executor: Finished task 1.0 in stage 1.0 (TID 2). 1959 bytes result sent to driver
[2025-07-10T12:14:07.501+0200] {subprocess.py:93} INFO - 25/07/10 12:14:07 INFO TaskSetManager: Finished task 1.0 in stage 1.0 (TID 2) in 2388 ms on 138.4.31.17 (executor driver) (1/2)
[2025-07-10T12:14:13.698+0200] {subprocess.py:93} INFO - 25/07/10 12:14:13 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 1959 bytes result sent to driver
[2025-07-10T12:14:13.700+0200] {subprocess.py:93} INFO - 25/07/10 12:14:13 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 8594 ms on 138.4.31.17 (executor driver) (2/2)
[2025-07-10T12:14:13.700+0200] {subprocess.py:93} INFO - 25/07/10 12:14:13 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool
[2025-07-10T12:14:13.702+0200] {subprocess.py:93} INFO - 25/07/10 12:14:13 INFO DAGScheduler: ShuffleMapStage 1 (collect at StringIndexer.scala:204) finished in 8,636 s
[2025-07-10T12:14:13.702+0200] {subprocess.py:93} INFO - 25/07/10 12:14:13 INFO DAGScheduler: looking for newly runnable stages
[2025-07-10T12:14:13.703+0200] {subprocess.py:93} INFO - 25/07/10 12:14:13 INFO DAGScheduler: running: Set()
[2025-07-10T12:14:13.703+0200] {subprocess.py:93} INFO - 25/07/10 12:14:13 INFO DAGScheduler: waiting: Set()
[2025-07-10T12:14:13.703+0200] {subprocess.py:93} INFO - 25/07/10 12:14:13 INFO DAGScheduler: failed: Set()
[2025-07-10T12:14:13.718+0200] {subprocess.py:93} INFO - 25/07/10 12:14:13 INFO ShufflePartitionsUtil: For shuffle(0), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576
[2025-07-10T12:14:13.751+0200] {subprocess.py:93} INFO - 25/07/10 12:14:13 INFO CodeGenerator: Code generated in 6.956088 ms
[2025-07-10T12:14:13.774+0200] {subprocess.py:93} INFO - 25/07/10 12:14:13 INFO DAGScheduler: Registering RDD 12 (collect at StringIndexer.scala:204) as input to shuffle 2
[2025-07-10T12:14:13.774+0200] {subprocess.py:93} INFO - 25/07/10 12:14:13 INFO DAGScheduler: Got map stage job 3 (collect at StringIndexer.scala:204) with 1 output partitions
[2025-07-10T12:14:13.774+0200] {subprocess.py:93} INFO - 25/07/10 12:14:13 INFO DAGScheduler: Final stage: ShuffleMapStage 4 (collect at StringIndexer.scala:204)
[2025-07-10T12:14:13.774+0200] {subprocess.py:93} INFO - 25/07/10 12:14:13 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 3)
[2025-07-10T12:14:13.774+0200] {subprocess.py:93} INFO - 25/07/10 12:14:13 INFO DAGScheduler: Missing parents: List()
[2025-07-10T12:14:13.777+0200] {subprocess.py:93} INFO - 25/07/10 12:14:13 INFO DAGScheduler: Submitting ShuffleMapStage 4 (MapPartitionsRDD[12] at collect at StringIndexer.scala:204), which has no missing parents
[2025-07-10T12:14:13.842+0200] {subprocess.py:93} INFO - 25/07/10 12:14:13 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 37.9 KiB, free 434.1 MiB)
[2025-07-10T12:14:13.843+0200] {subprocess.py:93} INFO - 25/07/10 12:14:13 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 18.2 KiB, free 434.1 MiB)
[2025-07-10T12:14:13.845+0200] {subprocess.py:93} INFO - 25/07/10 12:14:13 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 138.4.31.17:32855 (size: 18.2 KiB, free: 434.3 MiB)
[2025-07-10T12:14:13.845+0200] {subprocess.py:93} INFO - 25/07/10 12:14:13 INFO SparkContext: Created broadcast 4 from broadcast at DAGScheduler.scala:1585
[2025-07-10T12:14:13.845+0200] {subprocess.py:93} INFO - 25/07/10 12:14:13 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 4 (MapPartitionsRDD[12] at collect at StringIndexer.scala:204) (first 15 tasks are for partitions Vector(0))
[2025-07-10T12:14:13.846+0200] {subprocess.py:93} INFO - 25/07/10 12:14:13 INFO TaskSchedulerImpl: Adding task set 4.0 with 1 tasks resource profile 0
[2025-07-10T12:14:13.848+0200] {subprocess.py:93} INFO - 25/07/10 12:14:13 INFO TaskSetManager: Starting task 0.0 in stage 4.0 (TID 16) (138.4.31.17, executor driver, partition 0, NODE_LOCAL, 12941 bytes)
[2025-07-10T12:14:13.849+0200] {subprocess.py:93} INFO - 25/07/10 12:14:13 INFO Executor: Running task 0.0 in stage 4.0 (TID 16)
[2025-07-10T12:14:13.893+0200] {subprocess.py:93} INFO - 25/07/10 12:14:13 INFO ShuffleBlockFetcherIterator: Getting 2 (1126.9 KiB) non-empty blocks including 2 (1126.9 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2025-07-10T12:14:13.894+0200] {subprocess.py:93} INFO - 25/07/10 12:14:13 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 11 ms
[2025-07-10T12:14:13.905+0200] {subprocess.py:93} INFO - 25/07/10 12:14:13 INFO CodeGenerator: Code generated in 8.303081 ms
[2025-07-10T12:14:13.941+0200] {subprocess.py:93} INFO - 25/07/10 12:14:13 INFO CodeGenerator: Code generated in 7.328045 ms
[2025-07-10T12:14:13.952+0200] {subprocess.py:93} INFO - 25/07/10 12:14:13 INFO CodeGenerator: Code generated in 5.488204 ms
[2025-07-10T12:14:13.963+0200] {subprocess.py:93} INFO - 25/07/10 12:14:13 INFO CodeGenerator: Code generated in 6.811739 ms
[2025-07-10T12:14:14.005+0200] {subprocess.py:93} INFO - 25/07/10 12:14:14 INFO CodeGenerator: Code generated in 7.982102 ms
[2025-07-10T12:14:14.400+0200] {subprocess.py:93} INFO - 25/07/10 12:14:14 INFO CodeGenerator: Code generated in 15.425616 ms
[2025-07-10T12:14:14.648+0200] {subprocess.py:93} INFO - 25/07/10 12:14:14 INFO Executor: Finished task 0.0 in stage 4.0 (TID 16). 4707 bytes result sent to driver
[2025-07-10T12:14:14.650+0200] {subprocess.py:93} INFO - 25/07/10 12:14:14 INFO TaskSetManager: Finished task 0.0 in stage 4.0 (TID 16) in 803 ms on 138.4.31.17 (executor driver) (1/1)
[2025-07-10T12:14:14.650+0200] {subprocess.py:93} INFO - 25/07/10 12:14:14 INFO TaskSchedulerImpl: Removed TaskSet 4.0, whose tasks have all completed, from pool
[2025-07-10T12:14:14.652+0200] {subprocess.py:93} INFO - 25/07/10 12:14:14 INFO DAGScheduler: ShuffleMapStage 4 (collect at StringIndexer.scala:204) finished in 0,856 s
[2025-07-10T12:14:14.652+0200] {subprocess.py:93} INFO - 25/07/10 12:14:14 INFO DAGScheduler: looking for newly runnable stages
[2025-07-10T12:14:14.652+0200] {subprocess.py:93} INFO - 25/07/10 12:14:14 INFO DAGScheduler: running: Set()
[2025-07-10T12:14:14.652+0200] {subprocess.py:93} INFO - 25/07/10 12:14:14 INFO DAGScheduler: waiting: Set()
[2025-07-10T12:14:14.652+0200] {subprocess.py:93} INFO - 25/07/10 12:14:14 INFO DAGScheduler: failed: Set()
[2025-07-10T12:14:14.686+0200] {subprocess.py:93} INFO - 25/07/10 12:14:14 INFO SparkContext: Starting job: collect at StringIndexer.scala:204
[2025-07-10T12:14:14.687+0200] {subprocess.py:93} INFO - 25/07/10 12:14:14 INFO DAGScheduler: Got job 4 (collect at StringIndexer.scala:204) with 1 output partitions
[2025-07-10T12:14:14.687+0200] {subprocess.py:93} INFO - 25/07/10 12:14:14 INFO DAGScheduler: Final stage: ResultStage 7 (collect at StringIndexer.scala:204)
[2025-07-10T12:14:14.687+0200] {subprocess.py:93} INFO - 25/07/10 12:14:14 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 6)
[2025-07-10T12:14:14.687+0200] {subprocess.py:93} INFO - 25/07/10 12:14:14 INFO DAGScheduler: Missing parents: List()
[2025-07-10T12:14:14.687+0200] {subprocess.py:93} INFO - 25/07/10 12:14:14 INFO DAGScheduler: Submitting ResultStage 7 (MapPartitionsRDD[15] at collect at StringIndexer.scala:204), which has no missing parents
[2025-07-10T12:14:14.692+0200] {subprocess.py:93} INFO - 25/07/10 12:14:14 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 38.7 KiB, free 434.0 MiB)
[2025-07-10T12:14:14.693+0200] {subprocess.py:93} INFO - 25/07/10 12:14:14 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 18.3 KiB, free 434.0 MiB)
[2025-07-10T12:14:14.694+0200] {subprocess.py:93} INFO - 25/07/10 12:14:14 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 138.4.31.17:32855 (size: 18.3 KiB, free: 434.3 MiB)
[2025-07-10T12:14:14.694+0200] {subprocess.py:93} INFO - 25/07/10 12:14:14 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1585
[2025-07-10T12:14:14.695+0200] {subprocess.py:93} INFO - 25/07/10 12:14:14 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 7 (MapPartitionsRDD[15] at collect at StringIndexer.scala:204) (first 15 tasks are for partitions Vector(0))
[2025-07-10T12:14:14.696+0200] {subprocess.py:93} INFO - 25/07/10 12:14:14 INFO TaskSchedulerImpl: Adding task set 7.0 with 1 tasks resource profile 0
[2025-07-10T12:14:14.697+0200] {subprocess.py:93} INFO - 25/07/10 12:14:14 INFO TaskSetManager: Starting task 0.0 in stage 7.0 (TID 17) (138.4.31.17, executor driver, partition 0, NODE_LOCAL, 12952 bytes)
[2025-07-10T12:14:14.702+0200] {subprocess.py:93} INFO - 25/07/10 12:14:14 INFO Executor: Running task 0.0 in stage 7.0 (TID 17)
[2025-07-10T12:14:14.713+0200] {subprocess.py:93} INFO - 25/07/10 12:14:14 INFO ShuffleBlockFetcherIterator: Getting 1 (445.0 B) non-empty blocks including 1 (445.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2025-07-10T12:14:14.714+0200] {subprocess.py:93} INFO - 25/07/10 12:14:14 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
[2025-07-10T12:14:14.742+0200] {subprocess.py:93} INFO - 25/07/10 12:14:14 INFO CodeGenerator: Code generated in 20.066312 ms
[2025-07-10T12:14:14.790+0200] {subprocess.py:93} INFO - 25/07/10 12:14:14 INFO BlockManagerInfo: Removed broadcast_4_piece0 on 138.4.31.17:32855 in memory (size: 18.2 KiB, free: 434.3 MiB)
[2025-07-10T12:14:14.792+0200] {subprocess.py:93} INFO - 25/07/10 12:14:14 INFO Executor: Finished task 0.0 in stage 7.0 (TID 17). 6438 bytes result sent to driver
[2025-07-10T12:14:14.793+0200] {subprocess.py:93} INFO - 25/07/10 12:14:14 INFO TaskSetManager: Finished task 0.0 in stage 7.0 (TID 17) in 97 ms on 138.4.31.17 (executor driver) (1/1)
[2025-07-10T12:14:14.793+0200] {subprocess.py:93} INFO - 25/07/10 12:14:14 INFO TaskSchedulerImpl: Removed TaskSet 7.0, whose tasks have all completed, from pool
[2025-07-10T12:14:14.794+0200] {subprocess.py:93} INFO - 25/07/10 12:14:14 INFO DAGScheduler: ResultStage 7 (collect at StringIndexer.scala:204) finished in 0,104 s
[2025-07-10T12:14:14.796+0200] {subprocess.py:93} INFO - 25/07/10 12:14:14 INFO DAGScheduler: Job 4 is finished. Cancelling potential speculative or zombie tasks for this job
[2025-07-10T12:14:14.797+0200] {subprocess.py:93} INFO - 25/07/10 12:14:14 INFO TaskSchedulerImpl: Killing all running tasks in stage 7: Stage finished
[2025-07-10T12:14:14.797+0200] {subprocess.py:93} INFO - 25/07/10 12:14:14 INFO DAGScheduler: Job 4 finished: collect at StringIndexer.scala:204, took 0,110866 s
[2025-07-10T12:14:14.820+0200] {subprocess.py:93} INFO - 25/07/10 12:14:14 INFO CodeGenerator: Code generated in 17.395864 ms
[2025-07-10T12:14:14.933+0200] {subprocess.py:93} INFO - 25/07/10 12:14:14 INFO FileSystemOverwrite: Path /home/monica.fernandez/practica_creativa/models/string_indexer_model_Carrier.bin already exists. It will be overwritten.
[2025-07-10T12:14:14.948+0200] {subprocess.py:93} INFO - 25/07/10 12:14:14 INFO HadoopMapRedCommitProtocol: Using output committer class org.apache.hadoop.mapred.FileOutputCommitter
[2025-07-10T12:14:14.949+0200] {subprocess.py:93} INFO - 25/07/10 12:14:14 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2025-07-10T12:14:14.949+0200] {subprocess.py:93} INFO - 25/07/10 12:14:14 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2025-07-10T12:14:14.977+0200] {subprocess.py:93} INFO - 25/07/10 12:14:14 INFO SparkContext: Starting job: runJob at SparkHadoopWriter.scala:83
[2025-07-10T12:14:14.978+0200] {subprocess.py:93} INFO - 25/07/10 12:14:14 INFO DAGScheduler: Got job 5 (runJob at SparkHadoopWriter.scala:83) with 1 output partitions
[2025-07-10T12:14:14.978+0200] {subprocess.py:93} INFO - 25/07/10 12:14:14 INFO DAGScheduler: Final stage: ResultStage 8 (runJob at SparkHadoopWriter.scala:83)
[2025-07-10T12:14:14.978+0200] {subprocess.py:93} INFO - 25/07/10 12:14:14 INFO DAGScheduler: Parents of final stage: List()
[2025-07-10T12:14:14.978+0200] {subprocess.py:93} INFO - 25/07/10 12:14:14 INFO DAGScheduler: Missing parents: List()
[2025-07-10T12:14:14.978+0200] {subprocess.py:93} INFO - 25/07/10 12:14:14 INFO DAGScheduler: Submitting ResultStage 8 (MapPartitionsRDD[17] at saveAsTextFile at ReadWrite.scala:413), which has no missing parents
[2025-07-10T12:14:14.987+0200] {subprocess.py:93} INFO - 25/07/10 12:14:14 INFO MemoryStore: Block broadcast_6 stored as values in memory (estimated size 101.3 KiB, free 434.0 MiB)
[2025-07-10T12:14:14.988+0200] {subprocess.py:93} INFO - 25/07/10 12:14:14 INFO MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 36.6 KiB, free 433.9 MiB)
[2025-07-10T12:14:14.988+0200] {subprocess.py:93} INFO - 25/07/10 12:14:14 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on 138.4.31.17:32855 (size: 36.6 KiB, free: 434.3 MiB)
[2025-07-10T12:14:14.989+0200] {subprocess.py:93} INFO - 25/07/10 12:14:14 INFO SparkContext: Created broadcast 6 from broadcast at DAGScheduler.scala:1585
[2025-07-10T12:14:14.989+0200] {subprocess.py:93} INFO - 25/07/10 12:14:14 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 8 (MapPartitionsRDD[17] at saveAsTextFile at ReadWrite.scala:413) (first 15 tasks are for partitions Vector(0))
[2025-07-10T12:14:14.989+0200] {subprocess.py:93} INFO - 25/07/10 12:14:14 INFO TaskSchedulerImpl: Adding task set 8.0 with 1 tasks resource profile 0
[2025-07-10T12:14:14.991+0200] {subprocess.py:93} INFO - 25/07/10 12:14:14 INFO TaskSetManager: Starting task 0.0 in stage 8.0 (TID 18) (138.4.31.17, executor driver, partition 0, PROCESS_LOCAL, 13282 bytes)
[2025-07-10T12:14:14.999+0200] {subprocess.py:93} INFO - 25/07/10 12:14:14 INFO Executor: Running task 0.0 in stage 8.0 (TID 18)
[2025-07-10T12:14:14.999+0200] {subprocess.py:93} INFO - 25/07/10 12:14:14 INFO HadoopMapRedCommitProtocol: Using output committer class org.apache.hadoop.mapred.FileOutputCommitter
[2025-07-10T12:14:15.001+0200] {subprocess.py:93} INFO - 25/07/10 12:14:14 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2025-07-10T12:14:15.002+0200] {subprocess.py:93} INFO - 25/07/10 12:14:14 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2025-07-10T12:14:15.025+0200] {subprocess.py:93} INFO - 25/07/10 12:14:15 INFO FileOutputCommitter: Saved output of task 'attempt_202507101214141178179762313950070_0017_m_000000_0' to file:/home/monica.fernandez/practica_creativa/models/string_indexer_model_Carrier.bin/metadata/_temporary/0/task_202507101214141178179762313950070_0017_m_000000
[2025-07-10T12:14:15.025+0200] {subprocess.py:93} INFO - 25/07/10 12:14:15 INFO SparkHadoopMapRedUtil: attempt_202507101214141178179762313950070_0017_m_000000_0: Committed. Elapsed time: 1 ms.
[2025-07-10T12:14:15.038+0200] {subprocess.py:93} INFO - 25/07/10 12:14:15 INFO Executor: Finished task 0.0 in stage 8.0 (TID 18). 1170 bytes result sent to driver
[2025-07-10T12:14:15.039+0200] {subprocess.py:93} INFO - 25/07/10 12:14:15 INFO TaskSetManager: Finished task 0.0 in stage 8.0 (TID 18) in 48 ms on 138.4.31.17 (executor driver) (1/1)
[2025-07-10T12:14:15.039+0200] {subprocess.py:93} INFO - 25/07/10 12:14:15 INFO TaskSchedulerImpl: Removed TaskSet 8.0, whose tasks have all completed, from pool
[2025-07-10T12:14:15.041+0200] {subprocess.py:93} INFO - 25/07/10 12:14:15 INFO DAGScheduler: ResultStage 8 (runJob at SparkHadoopWriter.scala:83) finished in 0,062 s
[2025-07-10T12:14:15.041+0200] {subprocess.py:93} INFO - 25/07/10 12:14:15 INFO DAGScheduler: Job 5 is finished. Cancelling potential speculative or zombie tasks for this job
[2025-07-10T12:14:15.041+0200] {subprocess.py:93} INFO - 25/07/10 12:14:15 INFO TaskSchedulerImpl: Killing all running tasks in stage 8: Stage finished
[2025-07-10T12:14:15.041+0200] {subprocess.py:93} INFO - 25/07/10 12:14:15 INFO DAGScheduler: Job 5 finished: runJob at SparkHadoopWriter.scala:83, took 0,064223 s
[2025-07-10T12:14:15.042+0200] {subprocess.py:93} INFO - 25/07/10 12:14:15 INFO SparkHadoopWriter: Start to commit write Job job_202507101214141178179762313950070_0017.
[2025-07-10T12:14:15.063+0200] {subprocess.py:93} INFO - 25/07/10 12:14:15 INFO SparkHadoopWriter: Write Job job_202507101214141178179762313950070_0017 committed. Elapsed time: 20 ms.
[2025-07-10T12:14:15.219+0200] {subprocess.py:93} INFO - 25/07/10 12:14:15 INFO CodeGenerator: Code generated in 23.992561 ms
[2025-07-10T12:14:15.233+0200] {subprocess.py:93} INFO - 25/07/10 12:14:15 INFO DAGScheduler: Registering RDD 20 (parquet at StringIndexer.scala:499) as input to shuffle 3
[2025-07-10T12:14:15.233+0200] {subprocess.py:93} INFO - 25/07/10 12:14:15 INFO DAGScheduler: Got map stage job 6 (parquet at StringIndexer.scala:499) with 1 output partitions
[2025-07-10T12:14:15.234+0200] {subprocess.py:93} INFO - 25/07/10 12:14:15 INFO DAGScheduler: Final stage: ShuffleMapStage 9 (parquet at StringIndexer.scala:499)
[2025-07-10T12:14:15.234+0200] {subprocess.py:93} INFO - 25/07/10 12:14:15 INFO DAGScheduler: Parents of final stage: List()
[2025-07-10T12:14:15.234+0200] {subprocess.py:93} INFO - 25/07/10 12:14:15 INFO DAGScheduler: Missing parents: List()
[2025-07-10T12:14:15.235+0200] {subprocess.py:93} INFO - 25/07/10 12:14:15 INFO DAGScheduler: Submitting ShuffleMapStage 9 (MapPartitionsRDD[20] at parquet at StringIndexer.scala:499), which has no missing parents
[2025-07-10T12:14:15.237+0200] {subprocess.py:93} INFO - 25/07/10 12:14:15 INFO MemoryStore: Block broadcast_7 stored as values in memory (estimated size 8.1 KiB, free 433.9 MiB)
[2025-07-10T12:14:15.249+0200] {subprocess.py:93} INFO - 25/07/10 12:14:15 INFO MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 4.4 KiB, free 433.9 MiB)
[2025-07-10T12:14:15.250+0200] {subprocess.py:93} INFO - 25/07/10 12:14:15 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on 138.4.31.17:32855 (size: 4.4 KiB, free: 434.3 MiB)
[2025-07-10T12:14:15.250+0200] {subprocess.py:93} INFO - 25/07/10 12:14:15 INFO SparkContext: Created broadcast 7 from broadcast at DAGScheduler.scala:1585
[2025-07-10T12:14:15.250+0200] {subprocess.py:93} INFO - 25/07/10 12:14:15 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 9 (MapPartitionsRDD[20] at parquet at StringIndexer.scala:499) (first 15 tasks are for partitions Vector(0))
[2025-07-10T12:14:15.250+0200] {subprocess.py:93} INFO - 25/07/10 12:14:15 INFO TaskSchedulerImpl: Adding task set 9.0 with 1 tasks resource profile 0
[2025-07-10T12:14:15.252+0200] {subprocess.py:93} INFO - 25/07/10 12:14:15 INFO TaskSetManager: Starting task 0.0 in stage 9.0 (TID 19) (138.4.31.17, executor driver, partition 0, PROCESS_LOCAL, 13380 bytes)
[2025-07-10T12:14:15.254+0200] {subprocess.py:93} INFO - 25/07/10 12:14:15 INFO Executor: Running task 0.0 in stage 9.0 (TID 19)
[2025-07-10T12:14:15.258+0200] {subprocess.py:93} INFO - 25/07/10 12:14:15 INFO BlockManagerInfo: Removed broadcast_5_piece0 on 138.4.31.17:32855 in memory (size: 18.3 KiB, free: 434.3 MiB)
[2025-07-10T12:14:15.260+0200] {subprocess.py:93} INFO - 25/07/10 12:14:15 INFO Executor: Finished task 0.0 in stage 9.0 (TID 19). 1628 bytes result sent to driver
[2025-07-10T12:14:15.261+0200] {subprocess.py:93} INFO - 25/07/10 12:14:15 INFO TaskSetManager: Finished task 0.0 in stage 9.0 (TID 19) in 10 ms on 138.4.31.17 (executor driver) (1/1)
[2025-07-10T12:14:15.261+0200] {subprocess.py:93} INFO - 25/07/10 12:14:15 INFO TaskSchedulerImpl: Removed TaskSet 9.0, whose tasks have all completed, from pool
[2025-07-10T12:14:15.264+0200] {subprocess.py:93} INFO - 25/07/10 12:14:15 INFO DAGScheduler: ShuffleMapStage 9 (parquet at StringIndexer.scala:499) finished in 0,027 s
[2025-07-10T12:14:15.264+0200] {subprocess.py:93} INFO - 25/07/10 12:14:15 INFO DAGScheduler: looking for newly runnable stages
[2025-07-10T12:14:15.265+0200] {subprocess.py:93} INFO - 25/07/10 12:14:15 INFO DAGScheduler: running: Set()
[2025-07-10T12:14:15.265+0200] {subprocess.py:93} INFO - 25/07/10 12:14:15 INFO DAGScheduler: waiting: Set()
[2025-07-10T12:14:15.266+0200] {subprocess.py:93} INFO - 25/07/10 12:14:15 INFO DAGScheduler: failed: Set()
[2025-07-10T12:14:15.272+0200] {subprocess.py:93} INFO - 25/07/10 12:14:15 INFO BlockManagerInfo: Removed broadcast_6_piece0 on 138.4.31.17:32855 in memory (size: 36.6 KiB, free: 434.3 MiB)
[2025-07-10T12:14:15.323+0200] {subprocess.py:93} INFO - 25/07/10 12:14:15 INFO ParquetUtils: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
[2025-07-10T12:14:15.338+0200] {subprocess.py:93} INFO - 25/07/10 12:14:15 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2025-07-10T12:14:15.338+0200] {subprocess.py:93} INFO - 25/07/10 12:14:15 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2025-07-10T12:14:15.339+0200] {subprocess.py:93} INFO - 25/07/10 12:14:15 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[2025-07-10T12:14:15.339+0200] {subprocess.py:93} INFO - 25/07/10 12:14:15 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2025-07-10T12:14:15.339+0200] {subprocess.py:93} INFO - 25/07/10 12:14:15 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2025-07-10T12:14:15.339+0200] {subprocess.py:93} INFO - 25/07/10 12:14:15 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[2025-07-10T12:14:15.359+0200] {subprocess.py:93} INFO - 25/07/10 12:14:15 INFO SparkContext: Starting job: parquet at StringIndexer.scala:499
[2025-07-10T12:14:15.360+0200] {subprocess.py:93} INFO - 25/07/10 12:14:15 INFO DAGScheduler: Got job 7 (parquet at StringIndexer.scala:499) with 1 output partitions
[2025-07-10T12:14:15.360+0200] {subprocess.py:93} INFO - 25/07/10 12:14:15 INFO DAGScheduler: Final stage: ResultStage 11 (parquet at StringIndexer.scala:499)
[2025-07-10T12:14:15.360+0200] {subprocess.py:93} INFO - 25/07/10 12:14:15 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 10)
[2025-07-10T12:14:15.360+0200] {subprocess.py:93} INFO - 25/07/10 12:14:15 INFO DAGScheduler: Missing parents: List()
[2025-07-10T12:14:15.360+0200] {subprocess.py:93} INFO - 25/07/10 12:14:15 INFO DAGScheduler: Submitting ResultStage 11 (MapPartitionsRDD[22] at parquet at StringIndexer.scala:499), which has no missing parents
[2025-07-10T12:14:15.381+0200] {subprocess.py:93} INFO - 25/07/10 12:14:15 INFO MemoryStore: Block broadcast_8 stored as values in memory (estimated size 230.2 KiB, free 433.9 MiB)
[2025-07-10T12:14:15.387+0200] {subprocess.py:93} INFO - 25/07/10 12:14:15 INFO MemoryStore: Block broadcast_8_piece0 stored as bytes in memory (estimated size 81.0 KiB, free 433.8 MiB)
[2025-07-10T12:14:15.388+0200] {subprocess.py:93} INFO - 25/07/10 12:14:15 INFO BlockManagerInfo: Removed broadcast_7_piece0 on 138.4.31.17:32855 in memory (size: 4.4 KiB, free: 434.3 MiB)
[2025-07-10T12:14:15.388+0200] {subprocess.py:93} INFO - 25/07/10 12:14:15 INFO BlockManagerInfo: Added broadcast_8_piece0 in memory on 138.4.31.17:32855 (size: 81.0 KiB, free: 434.3 MiB)
[2025-07-10T12:14:15.388+0200] {subprocess.py:93} INFO - 25/07/10 12:14:15 INFO SparkContext: Created broadcast 8 from broadcast at DAGScheduler.scala:1585
[2025-07-10T12:14:15.389+0200] {subprocess.py:93} INFO - 25/07/10 12:14:15 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 11 (MapPartitionsRDD[22] at parquet at StringIndexer.scala:499) (first 15 tasks are for partitions Vector(0))
[2025-07-10T12:14:15.389+0200] {subprocess.py:93} INFO - 25/07/10 12:14:15 INFO TaskSchedulerImpl: Adding task set 11.0 with 1 tasks resource profile 0
[2025-07-10T12:14:15.390+0200] {subprocess.py:93} INFO - 25/07/10 12:14:15 INFO TaskSetManager: Starting task 0.0 in stage 11.0 (TID 20) (138.4.31.17, executor driver, partition 0, NODE_LOCAL, 12952 bytes)
[2025-07-10T12:14:15.391+0200] {subprocess.py:93} INFO - 25/07/10 12:14:15 INFO Executor: Running task 0.0 in stage 11.0 (TID 20)
[2025-07-10T12:14:15.413+0200] {subprocess.py:93} INFO - 25/07/10 12:14:15 INFO ShuffleBlockFetcherIterator: Getting 1 (228.0 B) non-empty blocks including 1 (228.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2025-07-10T12:14:15.413+0200] {subprocess.py:93} INFO - 25/07/10 12:14:15 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2025-07-10T12:14:15.414+0200] {subprocess.py:93} INFO - 25/07/10 12:14:15 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2025-07-10T12:14:15.414+0200] {subprocess.py:93} INFO - 25/07/10 12:14:15 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2025-07-10T12:14:15.415+0200] {subprocess.py:93} INFO - 25/07/10 12:14:15 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[2025-07-10T12:14:15.415+0200] {subprocess.py:93} INFO - 25/07/10 12:14:15 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2025-07-10T12:14:15.415+0200] {subprocess.py:93} INFO - 25/07/10 12:14:15 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2025-07-10T12:14:15.415+0200] {subprocess.py:93} INFO - 25/07/10 12:14:15 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[2025-07-10T12:14:15.420+0200] {subprocess.py:93} INFO - 25/07/10 12:14:15 INFO CodecConfig: Compression: SNAPPY
[2025-07-10T12:14:15.422+0200] {subprocess.py:93} INFO - 25/07/10 12:14:15 INFO CodecConfig: Compression: SNAPPY
[2025-07-10T12:14:15.441+0200] {subprocess.py:93} INFO - 25/07/10 12:14:15 INFO ParquetOutputFormat: ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]
[2025-07-10T12:14:15.465+0200] {subprocess.py:93} INFO - 25/07/10 12:14:15 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
[2025-07-10T12:14:15.465+0200] {subprocess.py:93} INFO - {
[2025-07-10T12:14:15.465+0200] {subprocess.py:93} INFO -   "type" : "struct",
[2025-07-10T12:14:15.465+0200] {subprocess.py:93} INFO -   "fields" : [ {
[2025-07-10T12:14:15.465+0200] {subprocess.py:93} INFO -     "name" : "labelsArray",
[2025-07-10T12:14:15.465+0200] {subprocess.py:93} INFO -     "type" : {
[2025-07-10T12:14:15.465+0200] {subprocess.py:93} INFO -       "type" : "array",
[2025-07-10T12:14:15.465+0200] {subprocess.py:93} INFO -       "elementType" : {
[2025-07-10T12:14:15.465+0200] {subprocess.py:93} INFO -         "type" : "array",
[2025-07-10T12:14:15.465+0200] {subprocess.py:93} INFO -         "elementType" : "string",
[2025-07-10T12:14:15.465+0200] {subprocess.py:93} INFO -         "containsNull" : true
[2025-07-10T12:14:15.465+0200] {subprocess.py:93} INFO -       },
[2025-07-10T12:14:15.465+0200] {subprocess.py:93} INFO -       "containsNull" : true
[2025-07-10T12:14:15.465+0200] {subprocess.py:93} INFO -     },
[2025-07-10T12:14:15.466+0200] {subprocess.py:93} INFO -     "nullable" : true,
[2025-07-10T12:14:15.466+0200] {subprocess.py:93} INFO -     "metadata" : { }
[2025-07-10T12:14:15.466+0200] {subprocess.py:93} INFO -   } ]
[2025-07-10T12:14:15.466+0200] {subprocess.py:93} INFO - }
[2025-07-10T12:14:15.466+0200] {subprocess.py:93} INFO - and corresponding Parquet message type:
[2025-07-10T12:14:15.466+0200] {subprocess.py:93} INFO - message spark_schema {
[2025-07-10T12:14:15.466+0200] {subprocess.py:93} INFO -   optional group labelsArray (LIST) {
[2025-07-10T12:14:15.466+0200] {subprocess.py:93} INFO -     repeated group list {
[2025-07-10T12:14:15.466+0200] {subprocess.py:93} INFO -       optional group element (LIST) {
[2025-07-10T12:14:15.466+0200] {subprocess.py:93} INFO -         repeated group list {
[2025-07-10T12:14:15.466+0200] {subprocess.py:93} INFO -           optional binary element (STRING);
[2025-07-10T12:14:15.466+0200] {subprocess.py:93} INFO -         }
[2025-07-10T12:14:15.466+0200] {subprocess.py:93} INFO -       }
[2025-07-10T12:14:15.466+0200] {subprocess.py:93} INFO -     }
[2025-07-10T12:14:15.466+0200] {subprocess.py:93} INFO -   }
[2025-07-10T12:14:15.466+0200] {subprocess.py:93} INFO - }
[2025-07-10T12:14:15.466+0200] {subprocess.py:93} INFO - 
[2025-07-10T12:14:15.466+0200] {subprocess.py:93} INFO - 
[2025-07-10T12:14:15.539+0200] {subprocess.py:93} INFO - 25/07/10 12:14:15 INFO CodecPool: Got brand-new compressor [.snappy]
[2025-07-10T12:14:16.018+0200] {subprocess.py:93} INFO - 25/07/10 12:14:16 INFO FileOutputCommitter: Saved output of task 'attempt_202507101214153071920827302295971_0011_m_000000_20' to file:/home/monica.fernandez/practica_creativa/models/string_indexer_model_Carrier.bin/data/_temporary/0/task_202507101214153071920827302295971_0011_m_000000
[2025-07-10T12:14:16.018+0200] {subprocess.py:93} INFO - 25/07/10 12:14:16 INFO SparkHadoopMapRedUtil: attempt_202507101214153071920827302295971_0011_m_000000_20: Committed. Elapsed time: 1 ms.
[2025-07-10T12:14:16.022+0200] {subprocess.py:93} INFO - 25/07/10 12:14:16 INFO Executor: Finished task 0.0 in stage 11.0 (TID 20). 4783 bytes result sent to driver
[2025-07-10T12:14:16.027+0200] {subprocess.py:93} INFO - 25/07/10 12:14:16 INFO TaskSetManager: Finished task 0.0 in stage 11.0 (TID 20) in 634 ms on 138.4.31.17 (executor driver) (1/1)
[2025-07-10T12:14:16.027+0200] {subprocess.py:93} INFO - 25/07/10 12:14:16 INFO TaskSchedulerImpl: Removed TaskSet 11.0, whose tasks have all completed, from pool
[2025-07-10T12:14:16.027+0200] {subprocess.py:93} INFO - 25/07/10 12:14:16 INFO DAGScheduler: ResultStage 11 (parquet at StringIndexer.scala:499) finished in 0,663 s
[2025-07-10T12:14:16.027+0200] {subprocess.py:93} INFO - 25/07/10 12:14:16 INFO DAGScheduler: Job 7 is finished. Cancelling potential speculative or zombie tasks for this job
[2025-07-10T12:14:16.027+0200] {subprocess.py:93} INFO - 25/07/10 12:14:16 INFO TaskSchedulerImpl: Killing all running tasks in stage 11: Stage finished
[2025-07-10T12:14:16.027+0200] {subprocess.py:93} INFO - 25/07/10 12:14:16 INFO DAGScheduler: Job 7 finished: parquet at StringIndexer.scala:499, took 0,666775 s
[2025-07-10T12:14:16.028+0200] {subprocess.py:93} INFO - 25/07/10 12:14:16 INFO FileFormatWriter: Start to commit write Job 53fc74b8-b811-4eec-9398-0e5dcc3c78ff.
[2025-07-10T12:14:16.067+0200] {subprocess.py:93} INFO - 25/07/10 12:14:16 INFO FileFormatWriter: Write Job 53fc74b8-b811-4eec-9398-0e5dcc3c78ff committed. Elapsed time: 39 ms.
[2025-07-10T12:14:16.074+0200] {subprocess.py:93} INFO - 25/07/10 12:14:16 INFO FileFormatWriter: Finished processing stats for write job 53fc74b8-b811-4eec-9398-0e5dcc3c78ff.
[2025-07-10T12:14:16.140+0200] {subprocess.py:93} INFO - 25/07/10 12:14:16 INFO V2ScanRelationPushDown:
[2025-07-10T12:14:16.140+0200] {subprocess.py:93} INFO - Pushing operators to origin_dest_distance
[2025-07-10T12:14:16.140+0200] {subprocess.py:93} INFO - Pushed Filters:
[2025-07-10T12:14:16.140+0200] {subprocess.py:93} INFO - Post-Scan Filters:
[2025-07-10T12:14:16.140+0200] {subprocess.py:93} INFO - 
[2025-07-10T12:14:16.263+0200] {subprocess.py:93} INFO - 25/07/10 12:14:16 INFO V2ScanRelationPushDown:
[2025-07-10T12:14:16.263+0200] {subprocess.py:93} INFO - Output: origin#42, dest#43
[2025-07-10T12:14:16.263+0200] {subprocess.py:93} INFO - 
[2025-07-10T12:14:16.299+0200] {subprocess.py:93} INFO - 25/07/10 12:14:16 INFO FileSourceStrategy: Pushed Filters:
[2025-07-10T12:14:16.299+0200] {subprocess.py:93} INFO - 25/07/10 12:14:16 INFO FileSourceStrategy: Post-Scan Filters:
[2025-07-10T12:14:16.347+0200] {subprocess.py:93} INFO - 25/07/10 12:14:16 INFO MemoryStore: Block broadcast_9 stored as values in memory (estimated size 209.9 KiB, free 433.6 MiB)
[2025-07-10T12:14:16.375+0200] {subprocess.py:93} INFO - 25/07/10 12:14:16 INFO MemoryStore: Block broadcast_9_piece0 stored as bytes in memory (estimated size 37.7 KiB, free 433.6 MiB)
[2025-07-10T12:14:16.375+0200] {subprocess.py:93} INFO - 25/07/10 12:14:16 INFO BlockManagerInfo: Added broadcast_9_piece0 in memory on 138.4.31.17:32855 (size: 37.7 KiB, free: 434.2 MiB)
[2025-07-10T12:14:16.376+0200] {subprocess.py:93} INFO - 25/07/10 12:14:16 INFO BlockManagerInfo: Removed broadcast_8_piece0 on 138.4.31.17:32855 in memory (size: 81.0 KiB, free: 434.3 MiB)
[2025-07-10T12:14:16.377+0200] {subprocess.py:93} INFO - 25/07/10 12:14:16 INFO SparkContext: Created broadcast 9 from collect at StringIndexer.scala:204
[2025-07-10T12:14:16.378+0200] {subprocess.py:93} INFO - 25/07/10 12:14:16 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
[2025-07-10T12:14:16.381+0200] {subprocess.py:93} INFO - 25/07/10 12:14:16 INFO DAGScheduler: Registering RDD 25 (collect at StringIndexer.scala:204) as input to shuffle 4
[2025-07-10T12:14:16.381+0200] {subprocess.py:93} INFO - 25/07/10 12:14:16 INFO DAGScheduler: Got map stage job 8 (collect at StringIndexer.scala:204) with 2 output partitions
[2025-07-10T12:14:16.381+0200] {subprocess.py:93} INFO - 25/07/10 12:14:16 INFO DAGScheduler: Final stage: ShuffleMapStage 12 (collect at StringIndexer.scala:204)
[2025-07-10T12:14:16.381+0200] {subprocess.py:93} INFO - 25/07/10 12:14:16 INFO DAGScheduler: Parents of final stage: List()
[2025-07-10T12:14:16.381+0200] {subprocess.py:93} INFO - 25/07/10 12:14:16 INFO DAGScheduler: Missing parents: List()
[2025-07-10T12:14:16.381+0200] {subprocess.py:93} INFO - 25/07/10 12:14:16 INFO DAGScheduler: Submitting ShuffleMapStage 12 (MapPartitionsRDD[25] at collect at StringIndexer.scala:204), which has no missing parents
[2025-07-10T12:14:16.386+0200] {subprocess.py:93} INFO - 25/07/10 12:14:16 INFO MemoryStore: Block broadcast_10 stored as values in memory (estimated size 13.7 KiB, free 433.9 MiB)
[2025-07-10T12:14:16.399+0200] {subprocess.py:93} INFO - 25/07/10 12:14:16 INFO MemoryStore: Block broadcast_10_piece0 stored as bytes in memory (estimated size 7.0 KiB, free 433.8 MiB)
[2025-07-10T12:14:16.400+0200] {subprocess.py:93} INFO - 25/07/10 12:14:16 INFO BlockManagerInfo: Added broadcast_10_piece0 in memory on 138.4.31.17:32855 (size: 7.0 KiB, free: 434.3 MiB)
[2025-07-10T12:14:16.403+0200] {subprocess.py:93} INFO - 25/07/10 12:14:16 INFO SparkContext: Created broadcast 10 from broadcast at DAGScheduler.scala:1585
[2025-07-10T12:14:16.404+0200] {subprocess.py:93} INFO - 25/07/10 12:14:16 INFO DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 12 (MapPartitionsRDD[25] at collect at StringIndexer.scala:204) (first 15 tasks are for partitions Vector(0, 1))
[2025-07-10T12:14:16.404+0200] {subprocess.py:93} INFO - 25/07/10 12:14:16 INFO TaskSchedulerImpl: Adding task set 12.0 with 2 tasks resource profile 0
[2025-07-10T12:14:16.405+0200] {subprocess.py:93} INFO - 25/07/10 12:14:16 INFO DAGScheduler: Registering RDD 29 (collect at StringIndexer.scala:204) as input to shuffle 5
[2025-07-10T12:14:16.405+0200] {subprocess.py:93} INFO - 25/07/10 12:14:16 INFO DAGScheduler: Got map stage job 9 (collect at StringIndexer.scala:204) with 13 output partitions
[2025-07-10T12:14:16.405+0200] {subprocess.py:93} INFO - 25/07/10 12:14:16 INFO DAGScheduler: Final stage: ShuffleMapStage 13 (collect at StringIndexer.scala:204)
[2025-07-10T12:14:16.405+0200] {subprocess.py:93} INFO - 25/07/10 12:14:16 INFO DAGScheduler: Parents of final stage: List()
[2025-07-10T12:14:16.405+0200] {subprocess.py:93} INFO - 25/07/10 12:14:16 INFO DAGScheduler: Missing parents: List()
[2025-07-10T12:14:16.406+0200] {subprocess.py:93} INFO - 25/07/10 12:14:16 INFO TaskSetManager: Starting task 0.0 in stage 12.0 (TID 21) (138.4.31.17, executor driver, partition 0, PROCESS_LOCAL, 13595 bytes)
[2025-07-10T12:14:16.407+0200] {subprocess.py:93} INFO - 25/07/10 12:14:16 INFO TaskSetManager: Starting task 1.0 in stage 12.0 (TID 22) (138.4.31.17, executor driver, partition 1, PROCESS_LOCAL, 13595 bytes)
[2025-07-10T12:14:16.407+0200] {subprocess.py:93} INFO - 25/07/10 12:14:16 INFO Executor: Running task 0.0 in stage 12.0 (TID 21)
[2025-07-10T12:14:16.407+0200] {subprocess.py:93} INFO - 25/07/10 12:14:16 INFO Executor: Running task 1.0 in stage 12.0 (TID 22)
[2025-07-10T12:14:16.410+0200] {subprocess.py:93} INFO - 25/07/10 12:14:16 INFO DAGScheduler: Submitting ShuffleMapStage 13 (MapPartitionsRDD[29] at collect at StringIndexer.scala:204), which has no missing parents
[2025-07-10T12:14:16.417+0200] {subprocess.py:93} INFO - 25/07/10 12:14:16 INFO MemoryStore: Block broadcast_11 stored as values in memory (estimated size 18.2 KiB, free 433.8 MiB)
[2025-07-10T12:14:16.429+0200] {subprocess.py:93} INFO - 25/07/10 12:14:16 INFO MemoryStore: Block broadcast_11_piece0 stored as bytes in memory (estimated size 9.1 KiB, free 433.8 MiB)
[2025-07-10T12:14:16.430+0200] {subprocess.py:93} INFO - 25/07/10 12:14:16 INFO BlockManagerInfo: Added broadcast_11_piece0 in memory on 138.4.31.17:32855 (size: 9.1 KiB, free: 434.3 MiB)
[2025-07-10T12:14:16.430+0200] {subprocess.py:93} INFO - 25/07/10 12:14:16 INFO SparkContext: Created broadcast 11 from broadcast at DAGScheduler.scala:1585
[2025-07-10T12:14:16.430+0200] {subprocess.py:93} INFO - 25/07/10 12:14:16 INFO DAGScheduler: Submitting 13 missing tasks from ShuffleMapStage 13 (MapPartitionsRDD[29] at collect at StringIndexer.scala:204) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12))
[2025-07-10T12:14:16.430+0200] {subprocess.py:93} INFO - 25/07/10 12:14:16 INFO TaskSchedulerImpl: Adding task set 13.0 with 13 tasks resource profile 0
[2025-07-10T12:14:16.432+0200] {subprocess.py:93} INFO - 25/07/10 12:14:16 INFO TaskSetManager: Starting task 0.0 in stage 13.0 (TID 23) (138.4.31.17, executor driver, partition 0, ANY, 14843 bytes)
[2025-07-10T12:14:16.432+0200] {subprocess.py:93} INFO - 25/07/10 12:14:16 INFO TaskSetManager: Starting task 1.0 in stage 13.0 (TID 24) (138.4.31.17, executor driver, partition 1, ANY, 14843 bytes)
[2025-07-10T12:14:16.437+0200] {subprocess.py:93} INFO - 25/07/10 12:14:16 INFO Executor: Running task 0.0 in stage 13.0 (TID 23)
[2025-07-10T12:14:16.437+0200] {subprocess.py:93} INFO - 25/07/10 12:14:16 INFO Executor: Running task 1.0 in stage 13.0 (TID 24)
[2025-07-10T12:14:16.443+0200] {subprocess.py:93} INFO - 25/07/10 12:14:16 INFO CodeGenerator: Code generated in 27.282634 ms
[2025-07-10T12:14:16.444+0200] {subprocess.py:93} INFO - 25/07/10 12:14:16 INFO FileScanRDD: Reading File path: file:///home/monica.fernandez/practica_creativa/data/simple_flight_delay_features.jsonl.bz2, range: 4194304-4676447, partition values: [empty row]
[2025-07-10T12:14:16.453+0200] {subprocess.py:93} INFO - 25/07/10 12:14:16 INFO CodeGenerator: Code generated in 7.703249 ms
[2025-07-10T12:14:16.455+0200] {subprocess.py:93} INFO - 25/07/10 12:14:16 INFO Executor: Finished task 1.0 in stage 13.0 (TID 24). 1816 bytes result sent to driver
[2025-07-10T12:14:16.458+0200] {subprocess.py:93} INFO - 25/07/10 12:14:16 INFO TaskSetManager: Starting task 2.0 in stage 13.0 (TID 25) (138.4.31.17, executor driver, partition 2, ANY, 14963 bytes)
[2025-07-10T12:14:16.458+0200] {subprocess.py:93} INFO - 25/07/10 12:14:16 INFO FileScanRDD: Reading File path: file:///home/monica.fernandez/practica_creativa/data/simple_flight_delay_features.jsonl.bz2, range: 0-4194304, partition values: [empty row]
[2025-07-10T12:14:16.458+0200] {subprocess.py:93} INFO - 25/07/10 12:14:16 INFO TaskSetManager: Finished task 1.0 in stage 13.0 (TID 24) in 26 ms on 138.4.31.17 (executor driver) (1/13)
[2025-07-10T12:14:16.458+0200] {subprocess.py:93} INFO - 25/07/10 12:14:16 INFO Executor: Running task 2.0 in stage 13.0 (TID 25)
[2025-07-10T12:14:16.470+0200] {subprocess.py:93} INFO - 25/07/10 12:14:16 INFO Executor: Finished task 0.0 in stage 13.0 (TID 23). 1816 bytes result sent to driver
[2025-07-10T12:14:16.472+0200] {subprocess.py:93} INFO - 25/07/10 12:14:16 INFO TaskSetManager: Starting task 3.0 in stage 13.0 (TID 26) (138.4.31.17, executor driver, partition 3, ANY, 14839 bytes)
[2025-07-10T12:14:16.472+0200] {subprocess.py:93} INFO - 25/07/10 12:14:16 INFO TaskSetManager: Finished task 0.0 in stage 13.0 (TID 23) in 41 ms on 138.4.31.17 (executor driver) (2/13)
[2025-07-10T12:14:16.472+0200] {subprocess.py:93} INFO - 25/07/10 12:14:16 INFO Executor: Running task 3.0 in stage 13.0 (TID 26)
[2025-07-10T12:14:16.506+0200] {subprocess.py:93} INFO - 25/07/10 12:14:16 INFO Executor: Finished task 2.0 in stage 13.0 (TID 25). 1902 bytes result sent to driver
[2025-07-10T12:14:16.508+0200] {subprocess.py:93} INFO - 25/07/10 12:14:16 INFO TaskSetManager: Starting task 4.0 in stage 13.0 (TID 27) (138.4.31.17, executor driver, partition 4, ANY, 14963 bytes)
[2025-07-10T12:14:16.509+0200] {subprocess.py:93} INFO - 25/07/10 12:14:16 INFO TaskSetManager: Finished task 2.0 in stage 13.0 (TID 25) in 51 ms on 138.4.31.17 (executor driver) (3/13)
[2025-07-10T12:14:16.509+0200] {subprocess.py:93} INFO - 25/07/10 12:14:16 INFO Executor: Running task 4.0 in stage 13.0 (TID 27)
[2025-07-10T12:14:16.562+0200] {subprocess.py:93} INFO - 25/07/10 12:14:16 INFO Executor: Finished task 3.0 in stage 13.0 (TID 26). 1902 bytes result sent to driver
[2025-07-10T12:14:16.563+0200] {subprocess.py:93} INFO - 25/07/10 12:14:16 INFO TaskSetManager: Starting task 5.0 in stage 13.0 (TID 28) (138.4.31.17, executor driver, partition 5, ANY, 14843 bytes)
[2025-07-10T12:14:16.564+0200] {subprocess.py:93} INFO - 25/07/10 12:14:16 INFO Executor: Running task 5.0 in stage 13.0 (TID 28)
[2025-07-10T12:14:16.564+0200] {subprocess.py:93} INFO - 25/07/10 12:14:16 INFO TaskSetManager: Finished task 3.0 in stage 13.0 (TID 26) in 93 ms on 138.4.31.17 (executor driver) (4/13)
[2025-07-10T12:14:16.580+0200] {subprocess.py:93} INFO - 25/07/10 12:14:16 INFO Executor: Finished task 4.0 in stage 13.0 (TID 27). 1859 bytes result sent to driver
[2025-07-10T12:14:16.585+0200] {subprocess.py:93} INFO - 25/07/10 12:14:16 INFO TaskSetManager: Starting task 6.0 in stage 13.0 (TID 29) (138.4.31.17, executor driver, partition 6, ANY, 14843 bytes)
[2025-07-10T12:14:16.585+0200] {subprocess.py:93} INFO - 25/07/10 12:14:16 INFO TaskSetManager: Finished task 4.0 in stage 13.0 (TID 27) in 78 ms on 138.4.31.17 (executor driver) (5/13)
[2025-07-10T12:14:16.586+0200] {subprocess.py:93} INFO - 25/07/10 12:14:16 INFO Executor: Running task 6.0 in stage 13.0 (TID 29)
[2025-07-10T12:14:16.595+0200] {subprocess.py:93} INFO - 25/07/10 12:14:16 INFO Executor: Finished task 5.0 in stage 13.0 (TID 28). 1859 bytes result sent to driver
[2025-07-10T12:14:16.597+0200] {subprocess.py:93} INFO - 25/07/10 12:14:16 INFO TaskSetManager: Starting task 7.0 in stage 13.0 (TID 30) (138.4.31.17, executor driver, partition 7, ANY, 14843 bytes)
[2025-07-10T12:14:16.597+0200] {subprocess.py:93} INFO - 25/07/10 12:14:16 INFO TaskSetManager: Finished task 5.0 in stage 13.0 (TID 28) in 35 ms on 138.4.31.17 (executor driver) (6/13)
[2025-07-10T12:14:16.598+0200] {subprocess.py:93} INFO - 25/07/10 12:14:16 INFO Executor: Running task 7.0 in stage 13.0 (TID 30)
[2025-07-10T12:14:16.609+0200] {subprocess.py:93} INFO - 25/07/10 12:14:16 INFO Executor: Finished task 6.0 in stage 13.0 (TID 29). 1816 bytes result sent to driver
[2025-07-10T12:14:16.612+0200] {subprocess.py:93} INFO - 25/07/10 12:14:16 INFO TaskSetManager: Starting task 8.0 in stage 13.0 (TID 31) (138.4.31.17, executor driver, partition 8, ANY, 14843 bytes)
[2025-07-10T12:14:16.613+0200] {subprocess.py:93} INFO - 25/07/10 12:14:16 INFO Executor: Running task 8.0 in stage 13.0 (TID 31)
[2025-07-10T12:14:16.613+0200] {subprocess.py:93} INFO - 25/07/10 12:14:16 INFO TaskSetManager: Finished task 6.0 in stage 13.0 (TID 29) in 28 ms on 138.4.31.17 (executor driver) (7/13)
[2025-07-10T12:14:16.630+0200] {subprocess.py:93} INFO - 25/07/10 12:14:16 INFO Executor: Finished task 7.0 in stage 13.0 (TID 30). 1859 bytes result sent to driver
[2025-07-10T12:14:16.631+0200] {subprocess.py:93} INFO - 25/07/10 12:14:16 INFO TaskSetManager: Starting task 9.0 in stage 13.0 (TID 32) (138.4.31.17, executor driver, partition 9, ANY, 14963 bytes)
[2025-07-10T12:14:16.631+0200] {subprocess.py:93} INFO - 25/07/10 12:14:16 INFO TaskSetManager: Finished task 7.0 in stage 13.0 (TID 30) in 35 ms on 138.4.31.17 (executor driver) (8/13)
[2025-07-10T12:14:16.632+0200] {subprocess.py:93} INFO - 25/07/10 12:14:16 INFO Executor: Running task 9.0 in stage 13.0 (TID 32)
[2025-07-10T12:14:16.634+0200] {subprocess.py:93} INFO - 25/07/10 12:14:16 INFO Executor: Finished task 8.0 in stage 13.0 (TID 31). 1859 bytes result sent to driver
[2025-07-10T12:14:16.635+0200] {subprocess.py:93} INFO - 25/07/10 12:14:16 INFO TaskSetManager: Finished task 8.0 in stage 13.0 (TID 31) in 24 ms on 138.4.31.17 (executor driver) (9/13)
[2025-07-10T12:14:16.636+0200] {subprocess.py:93} INFO - 25/07/10 12:14:16 INFO TaskSetManager: Starting task 10.0 in stage 13.0 (TID 33) (138.4.31.17, executor driver, partition 10, ANY, 14843 bytes)
[2025-07-10T12:14:16.641+0200] {subprocess.py:93} INFO - 25/07/10 12:14:16 INFO Executor: Running task 10.0 in stage 13.0 (TID 33)
[2025-07-10T12:14:16.667+0200] {subprocess.py:93} INFO - 25/07/10 12:14:16 INFO Executor: Finished task 9.0 in stage 13.0 (TID 32). 1859 bytes result sent to driver
[2025-07-10T12:14:16.671+0200] {subprocess.py:93} INFO - 25/07/10 12:14:16 INFO TaskSetManager: Starting task 11.0 in stage 13.0 (TID 34) (138.4.31.17, executor driver, partition 11, ANY, 14843 bytes)
[2025-07-10T12:14:16.671+0200] {subprocess.py:93} INFO - 25/07/10 12:14:16 INFO TaskSetManager: Finished task 9.0 in stage 13.0 (TID 32) in 41 ms on 138.4.31.17 (executor driver) (10/13)
[2025-07-10T12:14:16.672+0200] {subprocess.py:93} INFO - 25/07/10 12:14:16 INFO Executor: Running task 11.0 in stage 13.0 (TID 34)
[2025-07-10T12:14:16.675+0200] {subprocess.py:93} INFO - 25/07/10 12:14:16 INFO Executor: Finished task 10.0 in stage 13.0 (TID 33). 1859 bytes result sent to driver
[2025-07-10T12:14:16.676+0200] {subprocess.py:93} INFO - 25/07/10 12:14:16 INFO TaskSetManager: Starting task 12.0 in stage 13.0 (TID 35) (138.4.31.17, executor driver, partition 12, ANY, 14843 bytes)
[2025-07-10T12:14:16.677+0200] {subprocess.py:93} INFO - 25/07/10 12:14:16 INFO TaskSetManager: Finished task 10.0 in stage 13.0 (TID 33) in 42 ms on 138.4.31.17 (executor driver) (11/13)
[2025-07-10T12:14:16.689+0200] {subprocess.py:93} INFO - 25/07/10 12:14:16 INFO Executor: Running task 12.0 in stage 13.0 (TID 35)
[2025-07-10T12:14:16.700+0200] {subprocess.py:93} INFO - 25/07/10 12:14:16 INFO Executor: Finished task 11.0 in stage 13.0 (TID 34). 1859 bytes result sent to driver
[2025-07-10T12:14:16.700+0200] {subprocess.py:93} INFO - 25/07/10 12:14:16 INFO TaskSetManager: Finished task 11.0 in stage 13.0 (TID 34) in 31 ms on 138.4.31.17 (executor driver) (12/13)
[2025-07-10T12:14:16.742+0200] {subprocess.py:93} INFO - 25/07/10 12:14:16 INFO Executor: Finished task 12.0 in stage 13.0 (TID 35). 1859 bytes result sent to driver
[2025-07-10T12:14:16.744+0200] {subprocess.py:93} INFO - 25/07/10 12:14:16 INFO TaskSetManager: Finished task 12.0 in stage 13.0 (TID 35) in 68 ms on 138.4.31.17 (executor driver) (13/13)
[2025-07-10T12:14:16.744+0200] {subprocess.py:93} INFO - 25/07/10 12:14:16 INFO TaskSchedulerImpl: Removed TaskSet 13.0, whose tasks have all completed, from pool
[2025-07-10T12:14:16.744+0200] {subprocess.py:93} INFO - 25/07/10 12:14:16 INFO DAGScheduler: ShuffleMapStage 13 (collect at StringIndexer.scala:204) finished in 0,332 s
[2025-07-10T12:14:16.744+0200] {subprocess.py:93} INFO - 25/07/10 12:14:16 INFO DAGScheduler: looking for newly runnable stages
[2025-07-10T12:14:16.744+0200] {subprocess.py:93} INFO - 25/07/10 12:14:16 INFO DAGScheduler: running: Set(ShuffleMapStage 12)
[2025-07-10T12:14:16.744+0200] {subprocess.py:93} INFO - 25/07/10 12:14:16 INFO DAGScheduler: waiting: Set()
[2025-07-10T12:14:16.744+0200] {subprocess.py:93} INFO - 25/07/10 12:14:16 INFO DAGScheduler: failed: Set()
[2025-07-10T12:14:18.046+0200] {subprocess.py:93} INFO - 25/07/10 12:14:18 INFO Executor: Finished task 1.0 in stage 12.0 (TID 22). 1959 bytes result sent to driver
[2025-07-10T12:14:18.047+0200] {subprocess.py:93} INFO - 25/07/10 12:14:18 INFO TaskSetManager: Finished task 1.0 in stage 12.0 (TID 22) in 1641 ms on 138.4.31.17 (executor driver) (1/2)
[2025-07-10T12:14:18.877+0200] {subprocess.py:93} INFO - 25/07/10 12:14:18 INFO BlockManagerInfo: Removed broadcast_11_piece0 on 138.4.31.17:32855 in memory (size: 9.1 KiB, free: 434.3 MiB)
[2025-07-10T12:14:23.424+0200] {subprocess.py:93} INFO - 25/07/10 12:14:23 INFO Executor: Finished task 0.0 in stage 12.0 (TID 21). 1959 bytes result sent to driver
[2025-07-10T12:14:23.426+0200] {subprocess.py:93} INFO - 25/07/10 12:14:23 INFO TaskSetManager: Finished task 0.0 in stage 12.0 (TID 21) in 7020 ms on 138.4.31.17 (executor driver) (2/2)
[2025-07-10T12:14:23.426+0200] {subprocess.py:93} INFO - 25/07/10 12:14:23 INFO TaskSchedulerImpl: Removed TaskSet 12.0, whose tasks have all completed, from pool
[2025-07-10T12:14:23.426+0200] {subprocess.py:93} INFO - 25/07/10 12:14:23 INFO DAGScheduler: ShuffleMapStage 12 (collect at StringIndexer.scala:204) finished in 7,044 s
[2025-07-10T12:14:23.427+0200] {subprocess.py:93} INFO - 25/07/10 12:14:23 INFO DAGScheduler: looking for newly runnable stages
[2025-07-10T12:14:23.427+0200] {subprocess.py:93} INFO - 25/07/10 12:14:23 INFO DAGScheduler: running: Set()
[2025-07-10T12:14:23.427+0200] {subprocess.py:93} INFO - 25/07/10 12:14:23 INFO DAGScheduler: waiting: Set()
[2025-07-10T12:14:23.427+0200] {subprocess.py:93} INFO - 25/07/10 12:14:23 INFO DAGScheduler: failed: Set()
[2025-07-10T12:14:23.432+0200] {subprocess.py:93} INFO - 25/07/10 12:14:23 INFO ShufflePartitionsUtil: For shuffle(4), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576
[2025-07-10T12:14:23.448+0200] {subprocess.py:93} INFO - 25/07/10 12:14:23 INFO CodeGenerator: Code generated in 5.236555 ms
[2025-07-10T12:14:23.450+0200] {subprocess.py:93} INFO - 25/07/10 12:14:23 INFO DAGScheduler: Registering RDD 33 (collect at StringIndexer.scala:204) as input to shuffle 6
[2025-07-10T12:14:23.450+0200] {subprocess.py:93} INFO - 25/07/10 12:14:23 INFO DAGScheduler: Got map stage job 10 (collect at StringIndexer.scala:204) with 1 output partitions
[2025-07-10T12:14:23.451+0200] {subprocess.py:93} INFO - 25/07/10 12:14:23 INFO DAGScheduler: Final stage: ShuffleMapStage 15 (collect at StringIndexer.scala:204)
[2025-07-10T12:14:23.451+0200] {subprocess.py:93} INFO - 25/07/10 12:14:23 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 14)
[2025-07-10T12:14:23.451+0200] {subprocess.py:93} INFO - 25/07/10 12:14:23 INFO DAGScheduler: Missing parents: List()
[2025-07-10T12:14:23.451+0200] {subprocess.py:93} INFO - 25/07/10 12:14:23 INFO DAGScheduler: Submitting ShuffleMapStage 15 (MapPartitionsRDD[33] at collect at StringIndexer.scala:204), which has no missing parents
[2025-07-10T12:14:23.457+0200] {subprocess.py:93} INFO - 25/07/10 12:14:23 INFO MemoryStore: Block broadcast_12 stored as values in memory (estimated size 37.6 KiB, free 433.8 MiB)
[2025-07-10T12:14:23.461+0200] {subprocess.py:93} INFO - 25/07/10 12:14:23 INFO MemoryStore: Block broadcast_12_piece0 stored as bytes in memory (estimated size 18.1 KiB, free 433.8 MiB)
[2025-07-10T12:14:23.461+0200] {subprocess.py:93} INFO - 25/07/10 12:14:23 INFO BlockManagerInfo: Added broadcast_12_piece0 in memory on 138.4.31.17:32855 (size: 18.1 KiB, free: 434.3 MiB)
[2025-07-10T12:14:23.462+0200] {subprocess.py:93} INFO - 25/07/10 12:14:23 INFO SparkContext: Created broadcast 12 from broadcast at DAGScheduler.scala:1585
[2025-07-10T12:14:23.462+0200] {subprocess.py:93} INFO - 25/07/10 12:14:23 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 15 (MapPartitionsRDD[33] at collect at StringIndexer.scala:204) (first 15 tasks are for partitions Vector(0))
[2025-07-10T12:14:23.462+0200] {subprocess.py:93} INFO - 25/07/10 12:14:23 INFO TaskSchedulerImpl: Adding task set 15.0 with 1 tasks resource profile 0
[2025-07-10T12:14:23.463+0200] {subprocess.py:93} INFO - 25/07/10 12:14:23 INFO TaskSetManager: Starting task 0.0 in stage 15.0 (TID 36) (138.4.31.17, executor driver, partition 0, NODE_LOCAL, 12941 bytes)
[2025-07-10T12:14:23.463+0200] {subprocess.py:93} INFO - 25/07/10 12:14:23 INFO Executor: Running task 0.0 in stage 15.0 (TID 36)
[2025-07-10T12:14:23.470+0200] {subprocess.py:93} INFO - 25/07/10 12:14:23 INFO ShuffleBlockFetcherIterator: Getting 2 (887.5 KiB) non-empty blocks including 2 (887.5 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2025-07-10T12:14:23.470+0200] {subprocess.py:93} INFO - 25/07/10 12:14:23 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
[2025-07-10T12:14:23.475+0200] {subprocess.py:93} INFO - 25/07/10 12:14:23 INFO CodeGenerator: Code generated in 4.797483 ms
[2025-07-10T12:14:23.663+0200] {subprocess.py:93} INFO - 25/07/10 12:14:23 INFO Executor: Finished task 0.0 in stage 15.0 (TID 36). 4707 bytes result sent to driver
[2025-07-10T12:14:23.664+0200] {subprocess.py:93} INFO - 25/07/10 12:14:23 INFO TaskSetManager: Finished task 0.0 in stage 15.0 (TID 36) in 201 ms on 138.4.31.17 (executor driver) (1/1)
[2025-07-10T12:14:23.664+0200] {subprocess.py:93} INFO - 25/07/10 12:14:23 INFO TaskSchedulerImpl: Removed TaskSet 15.0, whose tasks have all completed, from pool
[2025-07-10T12:14:23.666+0200] {subprocess.py:93} INFO - 25/07/10 12:14:23 INFO DAGScheduler: ShuffleMapStage 15 (collect at StringIndexer.scala:204) finished in 0,210 s
[2025-07-10T12:14:23.667+0200] {subprocess.py:93} INFO - 25/07/10 12:14:23 INFO DAGScheduler: looking for newly runnable stages
[2025-07-10T12:14:23.667+0200] {subprocess.py:93} INFO - 25/07/10 12:14:23 INFO DAGScheduler: running: Set()
[2025-07-10T12:14:23.667+0200] {subprocess.py:93} INFO - 25/07/10 12:14:23 INFO DAGScheduler: waiting: Set()
[2025-07-10T12:14:23.667+0200] {subprocess.py:93} INFO - 25/07/10 12:14:23 INFO DAGScheduler: failed: Set()
[2025-07-10T12:14:23.680+0200] {subprocess.py:93} INFO - 25/07/10 12:14:23 INFO SparkContext: Starting job: collect at StringIndexer.scala:204
[2025-07-10T12:14:23.681+0200] {subprocess.py:93} INFO - 25/07/10 12:14:23 INFO DAGScheduler: Got job 11 (collect at StringIndexer.scala:204) with 1 output partitions
[2025-07-10T12:14:23.681+0200] {subprocess.py:93} INFO - 25/07/10 12:14:23 INFO DAGScheduler: Final stage: ResultStage 18 (collect at StringIndexer.scala:204)
[2025-07-10T12:14:23.682+0200] {subprocess.py:93} INFO - 25/07/10 12:14:23 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 17)
[2025-07-10T12:14:23.682+0200] {subprocess.py:93} INFO - 25/07/10 12:14:23 INFO DAGScheduler: Missing parents: List()
[2025-07-10T12:14:23.682+0200] {subprocess.py:93} INFO - 25/07/10 12:14:23 INFO DAGScheduler: Submitting ResultStage 18 (MapPartitionsRDD[36] at collect at StringIndexer.scala:204), which has no missing parents
[2025-07-10T12:14:23.683+0200] {subprocess.py:93} INFO - 25/07/10 12:14:23 INFO MemoryStore: Block broadcast_13 stored as values in memory (estimated size 38.4 KiB, free 433.8 MiB)
[2025-07-10T12:14:23.684+0200] {subprocess.py:93} INFO - 25/07/10 12:14:23 INFO MemoryStore: Block broadcast_13_piece0 stored as bytes in memory (estimated size 18.3 KiB, free 433.7 MiB)
[2025-07-10T12:14:23.684+0200] {subprocess.py:93} INFO - 25/07/10 12:14:23 INFO BlockManagerInfo: Added broadcast_13_piece0 in memory on 138.4.31.17:32855 (size: 18.3 KiB, free: 434.3 MiB)
[2025-07-10T12:14:23.685+0200] {subprocess.py:93} INFO - 25/07/10 12:14:23 INFO SparkContext: Created broadcast 13 from broadcast at DAGScheduler.scala:1585
[2025-07-10T12:14:23.685+0200] {subprocess.py:93} INFO - 25/07/10 12:14:23 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 18 (MapPartitionsRDD[36] at collect at StringIndexer.scala:204) (first 15 tasks are for partitions Vector(0))
[2025-07-10T12:14:23.685+0200] {subprocess.py:93} INFO - 25/07/10 12:14:23 INFO TaskSchedulerImpl: Adding task set 18.0 with 1 tasks resource profile 0
[2025-07-10T12:14:23.686+0200] {subprocess.py:93} INFO - 25/07/10 12:14:23 INFO TaskSetManager: Starting task 0.0 in stage 18.0 (TID 37) (138.4.31.17, executor driver, partition 0, NODE_LOCAL, 12952 bytes)
[2025-07-10T12:14:23.686+0200] {subprocess.py:93} INFO - 25/07/10 12:14:23 INFO Executor: Running task 0.0 in stage 18.0 (TID 37)
[2025-07-10T12:14:23.691+0200] {subprocess.py:93} INFO - 25/07/10 12:14:23 INFO ShuffleBlockFetcherIterator: Getting 1 (3.2 KiB) non-empty blocks including 1 (3.2 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2025-07-10T12:14:23.692+0200] {subprocess.py:93} INFO - 25/07/10 12:14:23 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2025-07-10T12:14:23.710+0200] {subprocess.py:93} INFO - 25/07/10 12:14:23 INFO Executor: Finished task 0.0 in stage 18.0 (TID 37). 9143 bytes result sent to driver
[2025-07-10T12:14:23.712+0200] {subprocess.py:93} INFO - 25/07/10 12:14:23 INFO TaskSetManager: Finished task 0.0 in stage 18.0 (TID 37) in 26 ms on 138.4.31.17 (executor driver) (1/1)
[2025-07-10T12:14:23.712+0200] {subprocess.py:93} INFO - 25/07/10 12:14:23 INFO TaskSchedulerImpl: Removed TaskSet 18.0, whose tasks have all completed, from pool
[2025-07-10T12:14:23.713+0200] {subprocess.py:93} INFO - 25/07/10 12:14:23 INFO DAGScheduler: ResultStage 18 (collect at StringIndexer.scala:204) finished in 0,032 s
[2025-07-10T12:14:23.713+0200] {subprocess.py:93} INFO - 25/07/10 12:14:23 INFO DAGScheduler: Job 11 is finished. Cancelling potential speculative or zombie tasks for this job
[2025-07-10T12:14:23.713+0200] {subprocess.py:93} INFO - 25/07/10 12:14:23 INFO TaskSchedulerImpl: Killing all running tasks in stage 18: Stage finished
[2025-07-10T12:14:23.715+0200] {subprocess.py:93} INFO - 25/07/10 12:14:23 INFO DAGScheduler: Job 11 finished: collect at StringIndexer.scala:204, took 0,034731 s
[2025-07-10T12:14:23.778+0200] {subprocess.py:93} INFO - 25/07/10 12:14:23 INFO FileSystemOverwrite: Path /home/monica.fernandez/practica_creativa/models/string_indexer_model_Origin.bin already exists. It will be overwritten.
[2025-07-10T12:14:23.793+0200] {subprocess.py:93} INFO - 25/07/10 12:14:23 INFO HadoopMapRedCommitProtocol: Using output committer class org.apache.hadoop.mapred.FileOutputCommitter
[2025-07-10T12:14:23.793+0200] {subprocess.py:93} INFO - 25/07/10 12:14:23 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2025-07-10T12:14:23.793+0200] {subprocess.py:93} INFO - 25/07/10 12:14:23 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2025-07-10T12:14:23.819+0200] {subprocess.py:93} INFO - 25/07/10 12:14:23 INFO SparkContext: Starting job: runJob at SparkHadoopWriter.scala:83
[2025-07-10T12:14:23.819+0200] {subprocess.py:93} INFO - 25/07/10 12:14:23 INFO DAGScheduler: Got job 12 (runJob at SparkHadoopWriter.scala:83) with 1 output partitions
[2025-07-10T12:14:23.819+0200] {subprocess.py:93} INFO - 25/07/10 12:14:23 INFO DAGScheduler: Final stage: ResultStage 19 (runJob at SparkHadoopWriter.scala:83)
[2025-07-10T12:14:23.819+0200] {subprocess.py:93} INFO - 25/07/10 12:14:23 INFO DAGScheduler: Parents of final stage: List()
[2025-07-10T12:14:23.819+0200] {subprocess.py:93} INFO - 25/07/10 12:14:23 INFO DAGScheduler: Missing parents: List()
[2025-07-10T12:14:23.819+0200] {subprocess.py:93} INFO - 25/07/10 12:14:23 INFO DAGScheduler: Submitting ResultStage 19 (MapPartitionsRDD[38] at saveAsTextFile at ReadWrite.scala:413), which has no missing parents
[2025-07-10T12:14:23.830+0200] {subprocess.py:93} INFO - 25/07/10 12:14:23 INFO MemoryStore: Block broadcast_14 stored as values in memory (estimated size 101.3 KiB, free 433.6 MiB)
[2025-07-10T12:14:23.831+0200] {subprocess.py:93} INFO - 25/07/10 12:14:23 INFO MemoryStore: Block broadcast_14_piece0 stored as bytes in memory (estimated size 36.5 KiB, free 433.6 MiB)
[2025-07-10T12:14:23.831+0200] {subprocess.py:93} INFO - 25/07/10 12:14:23 INFO BlockManagerInfo: Added broadcast_14_piece0 in memory on 138.4.31.17:32855 (size: 36.5 KiB, free: 434.2 MiB)
[2025-07-10T12:14:23.832+0200] {subprocess.py:93} INFO - 25/07/10 12:14:23 INFO SparkContext: Created broadcast 14 from broadcast at DAGScheduler.scala:1585
[2025-07-10T12:14:23.832+0200] {subprocess.py:93} INFO - 25/07/10 12:14:23 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 19 (MapPartitionsRDD[38] at saveAsTextFile at ReadWrite.scala:413) (first 15 tasks are for partitions Vector(0))
[2025-07-10T12:14:23.832+0200] {subprocess.py:93} INFO - 25/07/10 12:14:23 INFO TaskSchedulerImpl: Adding task set 19.0 with 1 tasks resource profile 0
[2025-07-10T12:14:23.833+0200] {subprocess.py:93} INFO - 25/07/10 12:14:23 INFO TaskSetManager: Starting task 0.0 in stage 19.0 (TID 38) (138.4.31.17, executor driver, partition 0, PROCESS_LOCAL, 13280 bytes)
[2025-07-10T12:14:23.833+0200] {subprocess.py:93} INFO - 25/07/10 12:14:23 INFO Executor: Running task 0.0 in stage 19.0 (TID 38)
[2025-07-10T12:14:23.840+0200] {subprocess.py:93} INFO - 25/07/10 12:14:23 INFO HadoopMapRedCommitProtocol: Using output committer class org.apache.hadoop.mapred.FileOutputCommitter
[2025-07-10T12:14:23.841+0200] {subprocess.py:93} INFO - 25/07/10 12:14:23 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2025-07-10T12:14:23.841+0200] {subprocess.py:93} INFO - 25/07/10 12:14:23 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2025-07-10T12:14:23.863+0200] {subprocess.py:93} INFO - 25/07/10 12:14:23 INFO FileOutputCommitter: Saved output of task 'attempt_202507101214234700152730714742993_0038_m_000000_0' to file:/home/monica.fernandez/practica_creativa/models/string_indexer_model_Origin.bin/metadata/_temporary/0/task_202507101214234700152730714742993_0038_m_000000
[2025-07-10T12:14:23.864+0200] {subprocess.py:93} INFO - 25/07/10 12:14:23 INFO SparkHadoopMapRedUtil: attempt_202507101214234700152730714742993_0038_m_000000_0: Committed. Elapsed time: 1 ms.
[2025-07-10T12:14:23.865+0200] {subprocess.py:93} INFO - 25/07/10 12:14:23 INFO Executor: Finished task 0.0 in stage 19.0 (TID 38). 1170 bytes result sent to driver
[2025-07-10T12:14:23.865+0200] {subprocess.py:93} INFO - 25/07/10 12:14:23 INFO TaskSetManager: Finished task 0.0 in stage 19.0 (TID 38) in 32 ms on 138.4.31.17 (executor driver) (1/1)
[2025-07-10T12:14:23.865+0200] {subprocess.py:93} INFO - 25/07/10 12:14:23 INFO TaskSchedulerImpl: Removed TaskSet 19.0, whose tasks have all completed, from pool
[2025-07-10T12:14:23.866+0200] {subprocess.py:93} INFO - 25/07/10 12:14:23 INFO DAGScheduler: ResultStage 19 (runJob at SparkHadoopWriter.scala:83) finished in 0,046 s
[2025-07-10T12:14:23.866+0200] {subprocess.py:93} INFO - 25/07/10 12:14:23 INFO DAGScheduler: Job 12 is finished. Cancelling potential speculative or zombie tasks for this job
[2025-07-10T12:14:23.866+0200] {subprocess.py:93} INFO - 25/07/10 12:14:23 INFO TaskSchedulerImpl: Killing all running tasks in stage 19: Stage finished
[2025-07-10T12:14:23.867+0200] {subprocess.py:93} INFO - 25/07/10 12:14:23 INFO DAGScheduler: Job 12 finished: runJob at SparkHadoopWriter.scala:83, took 0,048197 s
[2025-07-10T12:14:23.867+0200] {subprocess.py:93} INFO - 25/07/10 12:14:23 INFO SparkHadoopWriter: Start to commit write Job job_202507101214234700152730714742993_0038.
[2025-07-10T12:14:23.884+0200] {subprocess.py:93} INFO - 25/07/10 12:14:23 INFO SparkHadoopWriter: Write Job job_202507101214234700152730714742993_0038 committed. Elapsed time: 17 ms.
[2025-07-10T12:14:23.926+0200] {subprocess.py:93} INFO - 25/07/10 12:14:23 INFO DAGScheduler: Registering RDD 41 (parquet at StringIndexer.scala:499) as input to shuffle 7
[2025-07-10T12:14:23.926+0200] {subprocess.py:93} INFO - 25/07/10 12:14:23 INFO DAGScheduler: Got map stage job 13 (parquet at StringIndexer.scala:499) with 1 output partitions
[2025-07-10T12:14:23.926+0200] {subprocess.py:93} INFO - 25/07/10 12:14:23 INFO DAGScheduler: Final stage: ShuffleMapStage 20 (parquet at StringIndexer.scala:499)
[2025-07-10T12:14:23.926+0200] {subprocess.py:93} INFO - 25/07/10 12:14:23 INFO DAGScheduler: Parents of final stage: List()
[2025-07-10T12:14:23.927+0200] {subprocess.py:93} INFO - 25/07/10 12:14:23 INFO DAGScheduler: Missing parents: List()
[2025-07-10T12:14:23.927+0200] {subprocess.py:93} INFO - 25/07/10 12:14:23 INFO DAGScheduler: Submitting ShuffleMapStage 20 (MapPartitionsRDD[41] at parquet at StringIndexer.scala:499), which has no missing parents
[2025-07-10T12:14:23.929+0200] {subprocess.py:93} INFO - 25/07/10 12:14:23 INFO MemoryStore: Block broadcast_15 stored as values in memory (estimated size 8.1 KiB, free 433.6 MiB)
[2025-07-10T12:14:23.930+0200] {subprocess.py:93} INFO - 25/07/10 12:14:23 INFO MemoryStore: Block broadcast_15_piece0 stored as bytes in memory (estimated size 4.4 KiB, free 433.6 MiB)
[2025-07-10T12:14:23.930+0200] {subprocess.py:93} INFO - 25/07/10 12:14:23 INFO BlockManagerInfo: Added broadcast_15_piece0 in memory on 138.4.31.17:32855 (size: 4.4 KiB, free: 434.2 MiB)
[2025-07-10T12:14:23.932+0200] {subprocess.py:93} INFO - 25/07/10 12:14:23 INFO SparkContext: Created broadcast 15 from broadcast at DAGScheduler.scala:1585
[2025-07-10T12:14:23.932+0200] {subprocess.py:93} INFO - 25/07/10 12:14:23 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 20 (MapPartitionsRDD[41] at parquet at StringIndexer.scala:499) (first 15 tasks are for partitions Vector(0))
[2025-07-10T12:14:23.932+0200] {subprocess.py:93} INFO - 25/07/10 12:14:23 INFO TaskSchedulerImpl: Adding task set 20.0 with 1 tasks resource profile 0
[2025-07-10T12:14:23.932+0200] {subprocess.py:93} INFO - 25/07/10 12:14:23 INFO TaskSetManager: Starting task 0.0 in stage 20.0 (TID 39) (138.4.31.17, executor driver, partition 0, PROCESS_LOCAL, 18200 bytes)
[2025-07-10T12:14:23.933+0200] {subprocess.py:93} INFO - 25/07/10 12:14:23 INFO Executor: Running task 0.0 in stage 20.0 (TID 39)
[2025-07-10T12:14:23.938+0200] {subprocess.py:93} INFO - 25/07/10 12:14:23 INFO Executor: Finished task 0.0 in stage 20.0 (TID 39). 1628 bytes result sent to driver
[2025-07-10T12:14:23.939+0200] {subprocess.py:93} INFO - 25/07/10 12:14:23 INFO TaskSetManager: Finished task 0.0 in stage 20.0 (TID 39) in 7 ms on 138.4.31.17 (executor driver) (1/1)
[2025-07-10T12:14:23.939+0200] {subprocess.py:93} INFO - 25/07/10 12:14:23 INFO TaskSchedulerImpl: Removed TaskSet 20.0, whose tasks have all completed, from pool
[2025-07-10T12:14:23.940+0200] {subprocess.py:93} INFO - 25/07/10 12:14:23 INFO DAGScheduler: ShuffleMapStage 20 (parquet at StringIndexer.scala:499) finished in 0,013 s
[2025-07-10T12:14:23.940+0200] {subprocess.py:93} INFO - 25/07/10 12:14:23 INFO DAGScheduler: looking for newly runnable stages
[2025-07-10T12:14:23.940+0200] {subprocess.py:93} INFO - 25/07/10 12:14:23 INFO DAGScheduler: running: Set()
[2025-07-10T12:14:23.940+0200] {subprocess.py:93} INFO - 25/07/10 12:14:23 INFO DAGScheduler: waiting: Set()
[2025-07-10T12:14:23.940+0200] {subprocess.py:93} INFO - 25/07/10 12:14:23 INFO DAGScheduler: failed: Set()
[2025-07-10T12:14:23.955+0200] {subprocess.py:93} INFO - 25/07/10 12:14:23 INFO ParquetUtils: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
[2025-07-10T12:14:23.956+0200] {subprocess.py:93} INFO - 25/07/10 12:14:23 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2025-07-10T12:14:23.957+0200] {subprocess.py:93} INFO - 25/07/10 12:14:23 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2025-07-10T12:14:23.957+0200] {subprocess.py:93} INFO - 25/07/10 12:14:23 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[2025-07-10T12:14:23.957+0200] {subprocess.py:93} INFO - 25/07/10 12:14:23 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2025-07-10T12:14:23.957+0200] {subprocess.py:93} INFO - 25/07/10 12:14:23 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2025-07-10T12:14:23.959+0200] {subprocess.py:93} INFO - 25/07/10 12:14:23 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[2025-07-10T12:14:23.984+0200] {subprocess.py:93} INFO - 25/07/10 12:14:23 INFO SparkContext: Starting job: parquet at StringIndexer.scala:499
[2025-07-10T12:14:23.997+0200] {subprocess.py:93} INFO - 25/07/10 12:14:23 INFO DAGScheduler: Got job 14 (parquet at StringIndexer.scala:499) with 1 output partitions
[2025-07-10T12:14:23.997+0200] {subprocess.py:93} INFO - 25/07/10 12:14:23 INFO DAGScheduler: Final stage: ResultStage 22 (parquet at StringIndexer.scala:499)
[2025-07-10T12:14:23.997+0200] {subprocess.py:93} INFO - 25/07/10 12:14:23 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 21)
[2025-07-10T12:14:23.997+0200] {subprocess.py:93} INFO - 25/07/10 12:14:23 INFO DAGScheduler: Missing parents: List()
[2025-07-10T12:14:23.997+0200] {subprocess.py:93} INFO - 25/07/10 12:14:23 INFO DAGScheduler: Submitting ResultStage 22 (MapPartitionsRDD[43] at parquet at StringIndexer.scala:499), which has no missing parents
[2025-07-10T12:14:24.015+0200] {subprocess.py:93} INFO - 25/07/10 12:14:24 INFO MemoryStore: Block broadcast_16 stored as values in memory (estimated size 230.2 KiB, free 433.4 MiB)
[2025-07-10T12:14:24.015+0200] {subprocess.py:93} INFO - 25/07/10 12:14:24 INFO MemoryStore: Block broadcast_16_piece0 stored as bytes in memory (estimated size 81.0 KiB, free 433.3 MiB)
[2025-07-10T12:14:24.015+0200] {subprocess.py:93} INFO - 25/07/10 12:14:24 INFO BlockManagerInfo: Added broadcast_16_piece0 in memory on 138.4.31.17:32855 (size: 81.0 KiB, free: 434.1 MiB)
[2025-07-10T12:14:24.015+0200] {subprocess.py:93} INFO - 25/07/10 12:14:24 INFO SparkContext: Created broadcast 16 from broadcast at DAGScheduler.scala:1585
[2025-07-10T12:14:24.018+0200] {subprocess.py:93} INFO - 25/07/10 12:14:24 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 22 (MapPartitionsRDD[43] at parquet at StringIndexer.scala:499) (first 15 tasks are for partitions Vector(0))
[2025-07-10T12:14:24.024+0200] {subprocess.py:93} INFO - 25/07/10 12:14:24 INFO TaskSchedulerImpl: Adding task set 22.0 with 1 tasks resource profile 0
[2025-07-10T12:14:24.025+0200] {subprocess.py:93} INFO - 25/07/10 12:14:24 INFO TaskSetManager: Starting task 0.0 in stage 22.0 (TID 40) (138.4.31.17, executor driver, partition 0, NODE_LOCAL, 12952 bytes)
[2025-07-10T12:14:24.025+0200] {subprocess.py:93} INFO - 25/07/10 12:14:24 INFO Executor: Running task 0.0 in stage 22.0 (TID 40)
[2025-07-10T12:14:24.037+0200] {subprocess.py:93} INFO - 25/07/10 12:14:24 INFO ShuffleBlockFetcherIterator: Getting 1 (2.9 KiB) non-empty blocks including 1 (2.9 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2025-07-10T12:14:24.037+0200] {subprocess.py:93} INFO - 25/07/10 12:14:24 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2025-07-10T12:14:24.038+0200] {subprocess.py:93} INFO - 25/07/10 12:14:24 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2025-07-10T12:14:24.038+0200] {subprocess.py:93} INFO - 25/07/10 12:14:24 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2025-07-10T12:14:24.038+0200] {subprocess.py:93} INFO - 25/07/10 12:14:24 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[2025-07-10T12:14:24.038+0200] {subprocess.py:93} INFO - 25/07/10 12:14:24 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2025-07-10T12:14:24.038+0200] {subprocess.py:93} INFO - 25/07/10 12:14:24 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2025-07-10T12:14:24.038+0200] {subprocess.py:93} INFO - 25/07/10 12:14:24 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[2025-07-10T12:14:24.039+0200] {subprocess.py:93} INFO - 25/07/10 12:14:24 INFO CodecConfig: Compression: SNAPPY
[2025-07-10T12:14:24.039+0200] {subprocess.py:93} INFO - 25/07/10 12:14:24 INFO CodecConfig: Compression: SNAPPY
[2025-07-10T12:14:24.041+0200] {subprocess.py:93} INFO - 25/07/10 12:14:24 INFO ParquetOutputFormat: ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]
[2025-07-10T12:14:24.041+0200] {subprocess.py:93} INFO - 25/07/10 12:14:24 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
[2025-07-10T12:14:24.041+0200] {subprocess.py:93} INFO - {
[2025-07-10T12:14:24.041+0200] {subprocess.py:93} INFO -   "type" : "struct",
[2025-07-10T12:14:24.041+0200] {subprocess.py:93} INFO -   "fields" : [ {
[2025-07-10T12:14:24.041+0200] {subprocess.py:93} INFO -     "name" : "labelsArray",
[2025-07-10T12:14:24.041+0200] {subprocess.py:93} INFO -     "type" : {
[2025-07-10T12:14:24.041+0200] {subprocess.py:93} INFO -       "type" : "array",
[2025-07-10T12:14:24.041+0200] {subprocess.py:93} INFO -       "elementType" : {
[2025-07-10T12:14:24.041+0200] {subprocess.py:93} INFO -         "type" : "array",
[2025-07-10T12:14:24.041+0200] {subprocess.py:93} INFO -         "elementType" : "string",
[2025-07-10T12:14:24.042+0200] {subprocess.py:93} INFO -         "containsNull" : true
[2025-07-10T12:14:24.042+0200] {subprocess.py:93} INFO -       },
[2025-07-10T12:14:24.042+0200] {subprocess.py:93} INFO -       "containsNull" : true
[2025-07-10T12:14:24.042+0200] {subprocess.py:93} INFO -     },
[2025-07-10T12:14:24.042+0200] {subprocess.py:93} INFO -     "nullable" : true,
[2025-07-10T12:14:24.042+0200] {subprocess.py:93} INFO -     "metadata" : { }
[2025-07-10T12:14:24.042+0200] {subprocess.py:93} INFO -   } ]
[2025-07-10T12:14:24.042+0200] {subprocess.py:93} INFO - }
[2025-07-10T12:14:24.042+0200] {subprocess.py:93} INFO - and corresponding Parquet message type:
[2025-07-10T12:14:24.042+0200] {subprocess.py:93} INFO - message spark_schema {
[2025-07-10T12:14:24.042+0200] {subprocess.py:93} INFO -   optional group labelsArray (LIST) {
[2025-07-10T12:14:24.042+0200] {subprocess.py:93} INFO -     repeated group list {
[2025-07-10T12:14:24.042+0200] {subprocess.py:93} INFO -       optional group element (LIST) {
[2025-07-10T12:14:24.042+0200] {subprocess.py:93} INFO -         repeated group list {
[2025-07-10T12:14:24.043+0200] {subprocess.py:93} INFO -           optional binary element (STRING);
[2025-07-10T12:14:24.043+0200] {subprocess.py:93} INFO -         }
[2025-07-10T12:14:24.043+0200] {subprocess.py:93} INFO -       }
[2025-07-10T12:14:24.043+0200] {subprocess.py:93} INFO -     }
[2025-07-10T12:14:24.043+0200] {subprocess.py:93} INFO -   }
[2025-07-10T12:14:24.043+0200] {subprocess.py:93} INFO - }
[2025-07-10T12:14:24.043+0200] {subprocess.py:93} INFO - 
[2025-07-10T12:14:24.043+0200] {subprocess.py:93} INFO - 
[2025-07-10T12:14:24.088+0200] {subprocess.py:93} INFO - 25/07/10 12:14:24 INFO FileOutputCommitter: Saved output of task 'attempt_202507101214231085429216986200085_0022_m_000000_40' to file:/home/monica.fernandez/practica_creativa/models/string_indexer_model_Origin.bin/data/_temporary/0/task_202507101214231085429216986200085_0022_m_000000
[2025-07-10T12:14:24.088+0200] {subprocess.py:93} INFO - 25/07/10 12:14:24 INFO SparkHadoopMapRedUtil: attempt_202507101214231085429216986200085_0022_m_000000_40: Committed. Elapsed time: 1 ms.
[2025-07-10T12:14:24.089+0200] {subprocess.py:93} INFO - 25/07/10 12:14:24 INFO Executor: Finished task 0.0 in stage 22.0 (TID 40). 4740 bytes result sent to driver
[2025-07-10T12:14:24.092+0200] {subprocess.py:93} INFO - 25/07/10 12:14:24 INFO TaskSetManager: Finished task 0.0 in stage 22.0 (TID 40) in 71 ms on 138.4.31.17 (executor driver) (1/1)
[2025-07-10T12:14:24.092+0200] {subprocess.py:93} INFO - 25/07/10 12:14:24 INFO TaskSchedulerImpl: Removed TaskSet 22.0, whose tasks have all completed, from pool
[2025-07-10T12:14:24.092+0200] {subprocess.py:93} INFO - 25/07/10 12:14:24 INFO DAGScheduler: ResultStage 22 (parquet at StringIndexer.scala:499) finished in 0,103 s
[2025-07-10T12:14:24.092+0200] {subprocess.py:93} INFO - 25/07/10 12:14:24 INFO DAGScheduler: Job 14 is finished. Cancelling potential speculative or zombie tasks for this job
[2025-07-10T12:14:24.092+0200] {subprocess.py:93} INFO - 25/07/10 12:14:24 INFO TaskSchedulerImpl: Killing all running tasks in stage 22: Stage finished
[2025-07-10T12:14:24.092+0200] {subprocess.py:93} INFO - 25/07/10 12:14:24 INFO DAGScheduler: Job 14 finished: parquet at StringIndexer.scala:499, took 0,107350 s
[2025-07-10T12:14:24.092+0200] {subprocess.py:93} INFO - 25/07/10 12:14:24 INFO FileFormatWriter: Start to commit write Job 52ed5c25-164a-41bf-9c96-eb1b0fed4d08.
[2025-07-10T12:14:24.114+0200] {subprocess.py:93} INFO - 25/07/10 12:14:24 INFO FileFormatWriter: Write Job 52ed5c25-164a-41bf-9c96-eb1b0fed4d08 committed. Elapsed time: 21 ms.
[2025-07-10T12:14:24.114+0200] {subprocess.py:93} INFO - 25/07/10 12:14:24 INFO FileFormatWriter: Finished processing stats for write job 52ed5c25-164a-41bf-9c96-eb1b0fed4d08.
[2025-07-10T12:14:24.158+0200] {subprocess.py:93} INFO - 25/07/10 12:14:24 INFO V2ScanRelationPushDown:
[2025-07-10T12:14:24.158+0200] {subprocess.py:93} INFO - Pushing operators to origin_dest_distance
[2025-07-10T12:14:24.158+0200] {subprocess.py:93} INFO - Pushed Filters:
[2025-07-10T12:14:24.158+0200] {subprocess.py:93} INFO - Post-Scan Filters:
[2025-07-10T12:14:24.158+0200] {subprocess.py:93} INFO - 
[2025-07-10T12:14:24.165+0200] {subprocess.py:93} INFO - 25/07/10 12:14:24 INFO V2ScanRelationPushDown:
[2025-07-10T12:14:24.168+0200] {subprocess.py:93} INFO - Output: origin#42, dest#43
[2025-07-10T12:14:24.168+0200] {subprocess.py:93} INFO - 
[2025-07-10T12:14:24.180+0200] {subprocess.py:93} INFO - 25/07/10 12:14:24 INFO FileSourceStrategy: Pushed Filters:
[2025-07-10T12:14:24.180+0200] {subprocess.py:93} INFO - 25/07/10 12:14:24 INFO FileSourceStrategy: Post-Scan Filters:
[2025-07-10T12:14:24.195+0200] {subprocess.py:93} INFO - 25/07/10 12:14:24 INFO MemoryStore: Block broadcast_17 stored as values in memory (estimated size 209.9 KiB, free 433.1 MiB)
[2025-07-10T12:14:24.203+0200] {subprocess.py:93} INFO - 25/07/10 12:14:24 INFO MemoryStore: Block broadcast_17_piece0 stored as bytes in memory (estimated size 37.7 KiB, free 433.0 MiB)
[2025-07-10T12:14:24.203+0200] {subprocess.py:93} INFO - 25/07/10 12:14:24 INFO BlockManagerInfo: Added broadcast_17_piece0 in memory on 138.4.31.17:32855 (size: 37.7 KiB, free: 434.1 MiB)
[2025-07-10T12:14:24.203+0200] {subprocess.py:93} INFO - 25/07/10 12:14:24 INFO SparkContext: Created broadcast 17 from collect at StringIndexer.scala:204
[2025-07-10T12:14:24.204+0200] {subprocess.py:93} INFO - 25/07/10 12:14:24 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
[2025-07-10T12:14:24.206+0200] {subprocess.py:93} INFO - 25/07/10 12:14:24 INFO DAGScheduler: Registering RDD 46 (collect at StringIndexer.scala:204) as input to shuffle 8
[2025-07-10T12:14:24.207+0200] {subprocess.py:93} INFO - 25/07/10 12:14:24 INFO DAGScheduler: Got map stage job 15 (collect at StringIndexer.scala:204) with 2 output partitions
[2025-07-10T12:14:24.207+0200] {subprocess.py:93} INFO - 25/07/10 12:14:24 INFO DAGScheduler: Final stage: ShuffleMapStage 23 (collect at StringIndexer.scala:204)
[2025-07-10T12:14:24.207+0200] {subprocess.py:93} INFO - 25/07/10 12:14:24 INFO DAGScheduler: Parents of final stage: List()
[2025-07-10T12:14:24.207+0200] {subprocess.py:93} INFO - 25/07/10 12:14:24 INFO DAGScheduler: Missing parents: List()
[2025-07-10T12:14:24.207+0200] {subprocess.py:93} INFO - 25/07/10 12:14:24 INFO DAGScheduler: Submitting ShuffleMapStage 23 (MapPartitionsRDD[46] at collect at StringIndexer.scala:204), which has no missing parents
[2025-07-10T12:14:24.208+0200] {subprocess.py:93} INFO - 25/07/10 12:14:24 INFO MemoryStore: Block broadcast_18 stored as values in memory (estimated size 13.7 KiB, free 433.0 MiB)
[2025-07-10T12:14:24.209+0200] {subprocess.py:93} INFO - 25/07/10 12:14:24 INFO MemoryStore: Block broadcast_18_piece0 stored as bytes in memory (estimated size 7.0 KiB, free 433.0 MiB)
[2025-07-10T12:14:24.209+0200] {subprocess.py:93} INFO - 25/07/10 12:14:24 INFO BlockManagerInfo: Added broadcast_18_piece0 in memory on 138.4.31.17:32855 (size: 7.0 KiB, free: 434.1 MiB)
[2025-07-10T12:14:24.210+0200] {subprocess.py:93} INFO - 25/07/10 12:14:24 INFO SparkContext: Created broadcast 18 from broadcast at DAGScheduler.scala:1585
[2025-07-10T12:14:24.210+0200] {subprocess.py:93} INFO - 25/07/10 12:14:24 INFO DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 23 (MapPartitionsRDD[46] at collect at StringIndexer.scala:204) (first 15 tasks are for partitions Vector(0, 1))
[2025-07-10T12:14:24.210+0200] {subprocess.py:93} INFO - 25/07/10 12:14:24 INFO TaskSchedulerImpl: Adding task set 23.0 with 2 tasks resource profile 0
[2025-07-10T12:14:24.211+0200] {subprocess.py:93} INFO - 25/07/10 12:14:24 INFO TaskSetManager: Starting task 0.0 in stage 23.0 (TID 41) (138.4.31.17, executor driver, partition 0, PROCESS_LOCAL, 13595 bytes)
[2025-07-10T12:14:24.217+0200] {subprocess.py:93} INFO - 25/07/10 12:14:24 INFO TaskSetManager: Starting task 1.0 in stage 23.0 (TID 42) (138.4.31.17, executor driver, partition 1, PROCESS_LOCAL, 13595 bytes)
[2025-07-10T12:14:24.217+0200] {subprocess.py:93} INFO - 25/07/10 12:14:24 INFO Executor: Running task 1.0 in stage 23.0 (TID 42)
[2025-07-10T12:14:24.217+0200] {subprocess.py:93} INFO - 25/07/10 12:14:24 INFO Executor: Running task 0.0 in stage 23.0 (TID 41)
[2025-07-10T12:14:24.217+0200] {subprocess.py:93} INFO - 25/07/10 12:14:24 INFO FileScanRDD: Reading File path: file:///home/monica.fernandez/practica_creativa/data/simple_flight_delay_features.jsonl.bz2, range: 4194304-4676447, partition values: [empty row]
[2025-07-10T12:14:24.220+0200] {subprocess.py:93} INFO - 25/07/10 12:14:24 INFO FileScanRDD: Reading File path: file:///home/monica.fernandez/practica_creativa/data/simple_flight_delay_features.jsonl.bz2, range: 0-4194304, partition values: [empty row]
[2025-07-10T12:14:24.223+0200] {subprocess.py:93} INFO - 25/07/10 12:14:24 INFO DAGScheduler: Registering RDD 50 (collect at StringIndexer.scala:204) as input to shuffle 9
[2025-07-10T12:14:24.223+0200] {subprocess.py:93} INFO - 25/07/10 12:14:24 INFO DAGScheduler: Got map stage job 16 (collect at StringIndexer.scala:204) with 13 output partitions
[2025-07-10T12:14:24.224+0200] {subprocess.py:93} INFO - 25/07/10 12:14:24 INFO DAGScheduler: Final stage: ShuffleMapStage 24 (collect at StringIndexer.scala:204)
[2025-07-10T12:14:24.224+0200] {subprocess.py:93} INFO - 25/07/10 12:14:24 INFO DAGScheduler: Parents of final stage: List()
[2025-07-10T12:14:24.224+0200] {subprocess.py:93} INFO - 25/07/10 12:14:24 INFO DAGScheduler: Missing parents: List()
[2025-07-10T12:14:24.224+0200] {subprocess.py:93} INFO - 25/07/10 12:14:24 INFO DAGScheduler: Submitting ShuffleMapStage 24 (MapPartitionsRDD[50] at collect at StringIndexer.scala:204), which has no missing parents
[2025-07-10T12:14:24.225+0200] {subprocess.py:93} INFO - 25/07/10 12:14:24 INFO MemoryStore: Block broadcast_19 stored as values in memory (estimated size 18.2 KiB, free 433.0 MiB)
[2025-07-10T12:14:24.225+0200] {subprocess.py:93} INFO - 25/07/10 12:14:24 INFO MemoryStore: Block broadcast_19_piece0 stored as bytes in memory (estimated size 9.1 KiB, free 433.0 MiB)
[2025-07-10T12:14:24.226+0200] {subprocess.py:93} INFO - 25/07/10 12:14:24 INFO BlockManagerInfo: Added broadcast_19_piece0 in memory on 138.4.31.17:32855 (size: 9.1 KiB, free: 434.1 MiB)
[2025-07-10T12:14:24.227+0200] {subprocess.py:93} INFO - 25/07/10 12:14:24 INFO SparkContext: Created broadcast 19 from broadcast at DAGScheduler.scala:1585
[2025-07-10T12:14:24.227+0200] {subprocess.py:93} INFO - 25/07/10 12:14:24 INFO DAGScheduler: Submitting 13 missing tasks from ShuffleMapStage 24 (MapPartitionsRDD[50] at collect at StringIndexer.scala:204) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12))
[2025-07-10T12:14:24.227+0200] {subprocess.py:93} INFO - 25/07/10 12:14:24 INFO TaskSchedulerImpl: Adding task set 24.0 with 13 tasks resource profile 0
[2025-07-10T12:14:24.227+0200] {subprocess.py:93} INFO - 25/07/10 12:14:24 INFO TaskSetManager: Starting task 0.0 in stage 24.0 (TID 43) (138.4.31.17, executor driver, partition 0, ANY, 14843 bytes)
[2025-07-10T12:14:24.228+0200] {subprocess.py:93} INFO - 25/07/10 12:14:24 INFO TaskSetManager: Starting task 1.0 in stage 24.0 (TID 44) (138.4.31.17, executor driver, partition 1, ANY, 14963 bytes)
[2025-07-10T12:14:24.228+0200] {subprocess.py:93} INFO - 25/07/10 12:14:24 INFO Executor: Running task 1.0 in stage 24.0 (TID 44)
[2025-07-10T12:14:24.228+0200] {subprocess.py:93} INFO - 25/07/10 12:14:24 INFO Executor: Running task 0.0 in stage 24.0 (TID 43)
[2025-07-10T12:14:24.252+0200] {subprocess.py:93} INFO - 25/07/10 12:14:24 INFO Executor: Finished task 0.0 in stage 24.0 (TID 43). 1816 bytes result sent to driver
[2025-07-10T12:14:24.254+0200] {subprocess.py:93} INFO - 25/07/10 12:14:24 INFO TaskSetManager: Starting task 2.0 in stage 24.0 (TID 45) (138.4.31.17, executor driver, partition 2, ANY, 14843 bytes)
[2025-07-10T12:14:24.254+0200] {subprocess.py:93} INFO - 25/07/10 12:14:24 INFO TaskSetManager: Finished task 0.0 in stage 24.0 (TID 43) in 27 ms on 138.4.31.17 (executor driver) (1/13)
[2025-07-10T12:14:24.255+0200] {subprocess.py:93} INFO - 25/07/10 12:14:24 INFO Executor: Running task 2.0 in stage 24.0 (TID 45)
[2025-07-10T12:14:24.257+0200] {subprocess.py:93} INFO - 25/07/10 12:14:24 INFO Executor: Finished task 1.0 in stage 24.0 (TID 44). 1816 bytes result sent to driver
[2025-07-10T12:14:24.258+0200] {subprocess.py:93} INFO - 25/07/10 12:14:24 INFO TaskSetManager: Starting task 3.0 in stage 24.0 (TID 46) (138.4.31.17, executor driver, partition 3, ANY, 14839 bytes)
[2025-07-10T12:14:24.258+0200] {subprocess.py:93} INFO - 25/07/10 12:14:24 INFO TaskSetManager: Finished task 1.0 in stage 24.0 (TID 44) in 31 ms on 138.4.31.17 (executor driver) (2/13)
[2025-07-10T12:14:24.259+0200] {subprocess.py:93} INFO - 25/07/10 12:14:24 INFO Executor: Running task 3.0 in stage 24.0 (TID 46)
[2025-07-10T12:14:24.287+0200] {subprocess.py:93} INFO - 25/07/10 12:14:24 INFO Executor: Finished task 3.0 in stage 24.0 (TID 46). 1816 bytes result sent to driver
[2025-07-10T12:14:24.288+0200] {subprocess.py:93} INFO - 25/07/10 12:14:24 INFO TaskSetManager: Starting task 4.0 in stage 24.0 (TID 47) (138.4.31.17, executor driver, partition 4, ANY, 14963 bytes)
[2025-07-10T12:14:24.289+0200] {subprocess.py:93} INFO - 25/07/10 12:14:24 INFO Executor: Running task 4.0 in stage 24.0 (TID 47)
[2025-07-10T12:14:24.289+0200] {subprocess.py:93} INFO - 25/07/10 12:14:24 INFO TaskSetManager: Finished task 3.0 in stage 24.0 (TID 46) in 31 ms on 138.4.31.17 (executor driver) (3/13)
[2025-07-10T12:14:24.312+0200] {subprocess.py:93} INFO - 25/07/10 12:14:24 INFO Executor: Finished task 4.0 in stage 24.0 (TID 47). 1816 bytes result sent to driver
[2025-07-10T12:14:24.313+0200] {subprocess.py:93} INFO - 25/07/10 12:14:24 INFO Executor: Finished task 2.0 in stage 24.0 (TID 45). 1816 bytes result sent to driver
[2025-07-10T12:14:24.313+0200] {subprocess.py:93} INFO - 25/07/10 12:14:24 INFO TaskSetManager: Starting task 5.0 in stage 24.0 (TID 48) (138.4.31.17, executor driver, partition 5, ANY, 14843 bytes)
[2025-07-10T12:14:24.315+0200] {subprocess.py:93} INFO - 25/07/10 12:14:24 INFO TaskSetManager: Finished task 4.0 in stage 24.0 (TID 47) in 27 ms on 138.4.31.17 (executor driver) (4/13)
[2025-07-10T12:14:24.315+0200] {subprocess.py:93} INFO - 25/07/10 12:14:24 INFO Executor: Running task 5.0 in stage 24.0 (TID 48)
[2025-07-10T12:14:24.315+0200] {subprocess.py:93} INFO - 25/07/10 12:14:24 INFO TaskSetManager: Finished task 2.0 in stage 24.0 (TID 45) in 61 ms on 138.4.31.17 (executor driver) (5/13)
[2025-07-10T12:14:24.316+0200] {subprocess.py:93} INFO - 25/07/10 12:14:24 INFO TaskSetManager: Starting task 6.0 in stage 24.0 (TID 49) (138.4.31.17, executor driver, partition 6, ANY, 14843 bytes)
[2025-07-10T12:14:24.317+0200] {subprocess.py:93} INFO - 25/07/10 12:14:24 INFO Executor: Running task 6.0 in stage 24.0 (TID 49)
[2025-07-10T12:14:24.327+0200] {subprocess.py:93} INFO - 25/07/10 12:14:24 INFO Executor: Finished task 5.0 in stage 24.0 (TID 48). 1816 bytes result sent to driver
[2025-07-10T12:14:24.327+0200] {subprocess.py:93} INFO - 25/07/10 12:14:24 INFO TaskSetManager: Finished task 5.0 in stage 24.0 (TID 48) in 14 ms on 138.4.31.17 (executor driver) (6/13)
[2025-07-10T12:14:24.329+0200] {subprocess.py:93} INFO - 25/07/10 12:14:24 INFO TaskSetManager: Starting task 7.0 in stage 24.0 (TID 50) (138.4.31.17, executor driver, partition 7, ANY, 14843 bytes)
[2025-07-10T12:14:24.330+0200] {subprocess.py:93} INFO - 25/07/10 12:14:24 INFO Executor: Running task 7.0 in stage 24.0 (TID 50)
[2025-07-10T12:14:24.334+0200] {subprocess.py:93} INFO - 25/07/10 12:14:24 INFO Executor: Finished task 6.0 in stage 24.0 (TID 49). 1816 bytes result sent to driver
[2025-07-10T12:14:24.336+0200] {subprocess.py:93} INFO - 25/07/10 12:14:24 INFO TaskSetManager: Starting task 8.0 in stage 24.0 (TID 51) (138.4.31.17, executor driver, partition 8, ANY, 14963 bytes)
[2025-07-10T12:14:24.336+0200] {subprocess.py:93} INFO - 25/07/10 12:14:24 INFO TaskSetManager: Finished task 6.0 in stage 24.0 (TID 49) in 19 ms on 138.4.31.17 (executor driver) (7/13)
[2025-07-10T12:14:24.336+0200] {subprocess.py:93} INFO - 25/07/10 12:14:24 INFO Executor: Running task 8.0 in stage 24.0 (TID 51)
[2025-07-10T12:14:24.350+0200] {subprocess.py:93} INFO - 25/07/10 12:14:24 INFO Executor: Finished task 8.0 in stage 24.0 (TID 51). 1816 bytes result sent to driver
[2025-07-10T12:14:24.351+0200] {subprocess.py:93} INFO - 25/07/10 12:14:24 INFO TaskSetManager: Starting task 9.0 in stage 24.0 (TID 52) (138.4.31.17, executor driver, partition 9, ANY, 14843 bytes)
[2025-07-10T12:14:24.352+0200] {subprocess.py:93} INFO - 25/07/10 12:14:24 INFO TaskSetManager: Finished task 8.0 in stage 24.0 (TID 51) in 18 ms on 138.4.31.17 (executor driver) (8/13)
[2025-07-10T12:14:24.353+0200] {subprocess.py:93} INFO - 25/07/10 12:14:24 INFO Executor: Running task 9.0 in stage 24.0 (TID 52)
[2025-07-10T12:14:24.357+0200] {subprocess.py:93} INFO - 25/07/10 12:14:24 INFO Executor: Finished task 7.0 in stage 24.0 (TID 50). 1816 bytes result sent to driver
[2025-07-10T12:14:24.359+0200] {subprocess.py:93} INFO - 25/07/10 12:14:24 INFO TaskSetManager: Starting task 10.0 in stage 24.0 (TID 53) (138.4.31.17, executor driver, partition 10, ANY, 14843 bytes)
[2025-07-10T12:14:24.360+0200] {subprocess.py:93} INFO - 25/07/10 12:14:24 INFO TaskSetManager: Finished task 7.0 in stage 24.0 (TID 50) in 30 ms on 138.4.31.17 (executor driver) (9/13)
[2025-07-10T12:14:24.360+0200] {subprocess.py:93} INFO - 25/07/10 12:14:24 INFO Executor: Running task 10.0 in stage 24.0 (TID 53)
[2025-07-10T12:14:24.370+0200] {subprocess.py:93} INFO - 25/07/10 12:14:24 INFO Executor: Finished task 9.0 in stage 24.0 (TID 52). 1816 bytes result sent to driver
[2025-07-10T12:14:24.371+0200] {subprocess.py:93} INFO - 25/07/10 12:14:24 INFO TaskSetManager: Starting task 11.0 in stage 24.0 (TID 54) (138.4.31.17, executor driver, partition 11, ANY, 14843 bytes)
[2025-07-10T12:14:24.371+0200] {subprocess.py:93} INFO - 25/07/10 12:14:24 INFO Executor: Running task 11.0 in stage 24.0 (TID 54)
[2025-07-10T12:14:24.371+0200] {subprocess.py:93} INFO - 25/07/10 12:14:24 INFO TaskSetManager: Finished task 9.0 in stage 24.0 (TID 52) in 20 ms on 138.4.31.17 (executor driver) (10/13)
[2025-07-10T12:14:24.395+0200] {subprocess.py:93} INFO - 25/07/10 12:14:24 INFO Executor: Finished task 10.0 in stage 24.0 (TID 53). 1902 bytes result sent to driver
[2025-07-10T12:14:24.396+0200] {subprocess.py:93} INFO - 25/07/10 12:14:24 INFO TaskSetManager: Starting task 12.0 in stage 24.0 (TID 55) (138.4.31.17, executor driver, partition 12, ANY, 14843 bytes)
[2025-07-10T12:14:24.396+0200] {subprocess.py:93} INFO - 25/07/10 12:14:24 INFO Executor: Running task 12.0 in stage 24.0 (TID 55)
[2025-07-10T12:14:24.396+0200] {subprocess.py:93} INFO - 25/07/10 12:14:24 INFO TaskSetManager: Finished task 10.0 in stage 24.0 (TID 53) in 38 ms on 138.4.31.17 (executor driver) (11/13)
[2025-07-10T12:14:24.402+0200] {subprocess.py:93} INFO - 25/07/10 12:14:24 INFO BlockManagerInfo: Removed broadcast_14_piece0 on 138.4.31.17:32855 in memory (size: 36.5 KiB, free: 434.1 MiB)
[2025-07-10T12:14:24.410+0200] {subprocess.py:93} INFO - 25/07/10 12:14:24 INFO Executor: Finished task 11.0 in stage 24.0 (TID 54). 1859 bytes result sent to driver
[2025-07-10T12:14:24.411+0200] {subprocess.py:93} INFO - 25/07/10 12:14:24 INFO Executor: Finished task 12.0 in stage 24.0 (TID 55). 1816 bytes result sent to driver
[2025-07-10T12:14:24.412+0200] {subprocess.py:93} INFO - 25/07/10 12:14:24 INFO TaskSetManager: Finished task 12.0 in stage 24.0 (TID 55) in 17 ms on 138.4.31.17 (executor driver) (12/13)
[2025-07-10T12:14:24.413+0200] {subprocess.py:93} INFO - 25/07/10 12:14:24 INFO TaskSetManager: Finished task 11.0 in stage 24.0 (TID 54) in 42 ms on 138.4.31.17 (executor driver) (13/13)
[2025-07-10T12:14:24.413+0200] {subprocess.py:93} INFO - 25/07/10 12:14:24 INFO TaskSchedulerImpl: Removed TaskSet 24.0, whose tasks have all completed, from pool
[2025-07-10T12:14:24.414+0200] {subprocess.py:93} INFO - 25/07/10 12:14:24 INFO DAGScheduler: ShuffleMapStage 24 (collect at StringIndexer.scala:204) finished in 0,191 s
[2025-07-10T12:14:24.415+0200] {subprocess.py:93} INFO - 25/07/10 12:14:24 INFO DAGScheduler: looking for newly runnable stages
[2025-07-10T12:14:24.415+0200] {subprocess.py:93} INFO - 25/07/10 12:14:24 INFO DAGScheduler: running: Set(ShuffleMapStage 23)
[2025-07-10T12:14:24.415+0200] {subprocess.py:93} INFO - 25/07/10 12:14:24 INFO DAGScheduler: waiting: Set()
[2025-07-10T12:14:24.415+0200] {subprocess.py:93} INFO - 25/07/10 12:14:24 INFO DAGScheduler: failed: Set()
[2025-07-10T12:14:24.431+0200] {subprocess.py:93} INFO - 25/07/10 12:14:24 INFO BlockManagerInfo: Removed broadcast_12_piece0 on 138.4.31.17:32855 in memory (size: 18.1 KiB, free: 434.2 MiB)
[2025-07-10T12:14:24.441+0200] {subprocess.py:93} INFO - 25/07/10 12:14:24 INFO BlockManagerInfo: Removed broadcast_13_piece0 on 138.4.31.17:32855 in memory (size: 18.3 KiB, free: 434.2 MiB)
[2025-07-10T12:14:24.450+0200] {subprocess.py:93} INFO - 25/07/10 12:14:24 INFO BlockManagerInfo: Removed broadcast_16_piece0 on 138.4.31.17:32855 in memory (size: 81.0 KiB, free: 434.2 MiB)
[2025-07-10T12:14:24.454+0200] {subprocess.py:93} INFO - 25/07/10 12:14:24 INFO BlockManagerInfo: Removed broadcast_15_piece0 on 138.4.31.17:32855 in memory (size: 4.4 KiB, free: 434.3 MiB)
[2025-07-10T12:14:25.090+0200] {subprocess.py:93} INFO - 25/07/10 12:14:25 INFO Executor: Finished task 1.0 in stage 23.0 (TID 42). 1959 bytes result sent to driver
[2025-07-10T12:14:25.090+0200] {subprocess.py:93} INFO - 25/07/10 12:14:25 INFO TaskSetManager: Finished task 1.0 in stage 23.0 (TID 42) in 878 ms on 138.4.31.17 (executor driver) (1/2)
[2025-07-10T12:14:31.936+0200] {subprocess.py:93} INFO - 25/07/10 12:14:31 INFO Executor: Finished task 0.0 in stage 23.0 (TID 41). 1959 bytes result sent to driver
[2025-07-10T12:14:31.947+0200] {subprocess.py:93} INFO - 25/07/10 12:14:31 INFO TaskSetManager: Finished task 0.0 in stage 23.0 (TID 41) in 7730 ms on 138.4.31.17 (executor driver) (2/2)
[2025-07-10T12:14:31.948+0200] {subprocess.py:93} INFO - 25/07/10 12:14:31 INFO TaskSchedulerImpl: Removed TaskSet 23.0, whose tasks have all completed, from pool
[2025-07-10T12:14:31.948+0200] {subprocess.py:93} INFO - 25/07/10 12:14:31 INFO DAGScheduler: ShuffleMapStage 23 (collect at StringIndexer.scala:204) finished in 7,734 s
[2025-07-10T12:14:31.948+0200] {subprocess.py:93} INFO - 25/07/10 12:14:31 INFO DAGScheduler: looking for newly runnable stages
[2025-07-10T12:14:31.948+0200] {subprocess.py:93} INFO - 25/07/10 12:14:31 INFO DAGScheduler: running: Set()
[2025-07-10T12:14:31.948+0200] {subprocess.py:93} INFO - 25/07/10 12:14:31 INFO DAGScheduler: waiting: Set()
[2025-07-10T12:14:31.948+0200] {subprocess.py:93} INFO - 25/07/10 12:14:31 INFO DAGScheduler: failed: Set()
[2025-07-10T12:14:31.948+0200] {subprocess.py:93} INFO - 25/07/10 12:14:31 INFO ShufflePartitionsUtil: For shuffle(8), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576
[2025-07-10T12:14:31.983+0200] {subprocess.py:93} INFO - 25/07/10 12:14:31 INFO CodeGenerator: Code generated in 6.899783 ms
[2025-07-10T12:14:31.988+0200] {subprocess.py:93} INFO - 25/07/10 12:14:31 INFO DAGScheduler: Registering RDD 54 (collect at StringIndexer.scala:204) as input to shuffle 10
[2025-07-10T12:14:31.988+0200] {subprocess.py:93} INFO - 25/07/10 12:14:31 INFO DAGScheduler: Got map stage job 17 (collect at StringIndexer.scala:204) with 1 output partitions
[2025-07-10T12:14:31.988+0200] {subprocess.py:93} INFO - 25/07/10 12:14:31 INFO DAGScheduler: Final stage: ShuffleMapStage 26 (collect at StringIndexer.scala:204)
[2025-07-10T12:14:31.988+0200] {subprocess.py:93} INFO - 25/07/10 12:14:31 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 25)
[2025-07-10T12:14:31.988+0200] {subprocess.py:93} INFO - 25/07/10 12:14:31 INFO DAGScheduler: Missing parents: List()
[2025-07-10T12:14:31.989+0200] {subprocess.py:93} INFO - 25/07/10 12:14:31 INFO DAGScheduler: Submitting ShuffleMapStage 26 (MapPartitionsRDD[54] at collect at StringIndexer.scala:204), which has no missing parents
[2025-07-10T12:14:31.997+0200] {subprocess.py:93} INFO - 25/07/10 12:14:31 INFO MemoryStore: Block broadcast_20 stored as values in memory (estimated size 37.6 KiB, free 433.5 MiB)
[2025-07-10T12:14:31.998+0200] {subprocess.py:93} INFO - 25/07/10 12:14:31 INFO MemoryStore: Block broadcast_20_piece0 stored as bytes in memory (estimated size 18.0 KiB, free 433.5 MiB)
[2025-07-10T12:14:31.999+0200] {subprocess.py:93} INFO - 25/07/10 12:14:31 INFO BlockManagerInfo: Added broadcast_20_piece0 in memory on 138.4.31.17:32855 (size: 18.0 KiB, free: 434.2 MiB)
[2025-07-10T12:14:31.999+0200] {subprocess.py:93} INFO - 25/07/10 12:14:31 INFO SparkContext: Created broadcast 20 from broadcast at DAGScheduler.scala:1585
[2025-07-10T12:14:32.007+0200] {subprocess.py:93} INFO - 25/07/10 12:14:32 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 26 (MapPartitionsRDD[54] at collect at StringIndexer.scala:204) (first 15 tasks are for partitions Vector(0))
[2025-07-10T12:14:32.007+0200] {subprocess.py:93} INFO - 25/07/10 12:14:32 INFO TaskSchedulerImpl: Adding task set 26.0 with 1 tasks resource profile 0
[2025-07-10T12:14:32.008+0200] {subprocess.py:93} INFO - 25/07/10 12:14:32 INFO TaskSetManager: Starting task 0.0 in stage 26.0 (TID 56) (138.4.31.17, executor driver, partition 0, NODE_LOCAL, 12941 bytes)
[2025-07-10T12:14:32.009+0200] {subprocess.py:93} INFO - 25/07/10 12:14:32 INFO Executor: Running task 0.0 in stage 26.0 (TID 56)
[2025-07-10T12:14:32.014+0200] {subprocess.py:93} INFO - 25/07/10 12:14:32 INFO ShuffleBlockFetcherIterator: Getting 2 (887.5 KiB) non-empty blocks including 2 (887.5 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2025-07-10T12:14:32.015+0200] {subprocess.py:93} INFO - 25/07/10 12:14:32 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2025-07-10T12:14:32.027+0200] {subprocess.py:93} INFO - 25/07/10 12:14:32 INFO CodeGenerator: Code generated in 11.622603 ms
[2025-07-10T12:14:32.286+0200] {subprocess.py:93} INFO - 25/07/10 12:14:32 INFO Executor: Finished task 0.0 in stage 26.0 (TID 56). 4664 bytes result sent to driver
[2025-07-10T12:14:32.286+0200] {subprocess.py:93} INFO - 25/07/10 12:14:32 INFO TaskSetManager: Finished task 0.0 in stage 26.0 (TID 56) in 277 ms on 138.4.31.17 (executor driver) (1/1)
[2025-07-10T12:14:32.286+0200] {subprocess.py:93} INFO - 25/07/10 12:14:32 INFO TaskSchedulerImpl: Removed TaskSet 26.0, whose tasks have all completed, from pool
[2025-07-10T12:14:32.286+0200] {subprocess.py:93} INFO - 25/07/10 12:14:32 INFO DAGScheduler: ShuffleMapStage 26 (collect at StringIndexer.scala:204) finished in 0,293 s
[2025-07-10T12:14:32.286+0200] {subprocess.py:93} INFO - 25/07/10 12:14:32 INFO DAGScheduler: looking for newly runnable stages
[2025-07-10T12:14:32.288+0200] {subprocess.py:93} INFO - 25/07/10 12:14:32 INFO DAGScheduler: running: Set()
[2025-07-10T12:14:32.288+0200] {subprocess.py:93} INFO - 25/07/10 12:14:32 INFO DAGScheduler: waiting: Set()
[2025-07-10T12:14:32.288+0200] {subprocess.py:93} INFO - 25/07/10 12:14:32 INFO DAGScheduler: failed: Set()
[2025-07-10T12:14:32.315+0200] {subprocess.py:93} INFO - 25/07/10 12:14:32 INFO SparkContext: Starting job: collect at StringIndexer.scala:204
[2025-07-10T12:14:32.316+0200] {subprocess.py:93} INFO - 25/07/10 12:14:32 INFO DAGScheduler: Got job 18 (collect at StringIndexer.scala:204) with 1 output partitions
[2025-07-10T12:14:32.316+0200] {subprocess.py:93} INFO - 25/07/10 12:14:32 INFO DAGScheduler: Final stage: ResultStage 29 (collect at StringIndexer.scala:204)
[2025-07-10T12:14:32.316+0200] {subprocess.py:93} INFO - 25/07/10 12:14:32 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 28)
[2025-07-10T12:14:32.316+0200] {subprocess.py:93} INFO - 25/07/10 12:14:32 INFO DAGScheduler: Missing parents: List()
[2025-07-10T12:14:32.316+0200] {subprocess.py:93} INFO - 25/07/10 12:14:32 INFO DAGScheduler: Submitting ResultStage 29 (MapPartitionsRDD[57] at collect at StringIndexer.scala:204), which has no missing parents
[2025-07-10T12:14:32.318+0200] {subprocess.py:93} INFO - 25/07/10 12:14:32 INFO MemoryStore: Block broadcast_21 stored as values in memory (estimated size 38.4 KiB, free 433.5 MiB)
[2025-07-10T12:14:32.319+0200] {subprocess.py:93} INFO - 25/07/10 12:14:32 INFO MemoryStore: Block broadcast_21_piece0 stored as bytes in memory (estimated size 18.3 KiB, free 433.5 MiB)
[2025-07-10T12:14:32.319+0200] {subprocess.py:93} INFO - 25/07/10 12:14:32 INFO BlockManagerInfo: Added broadcast_21_piece0 in memory on 138.4.31.17:32855 (size: 18.3 KiB, free: 434.2 MiB)
[2025-07-10T12:14:32.320+0200] {subprocess.py:93} INFO - 25/07/10 12:14:32 INFO SparkContext: Created broadcast 21 from broadcast at DAGScheduler.scala:1585
[2025-07-10T12:14:32.320+0200] {subprocess.py:93} INFO - 25/07/10 12:14:32 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 29 (MapPartitionsRDD[57] at collect at StringIndexer.scala:204) (first 15 tasks are for partitions Vector(0))
[2025-07-10T12:14:32.320+0200] {subprocess.py:93} INFO - 25/07/10 12:14:32 INFO TaskSchedulerImpl: Adding task set 29.0 with 1 tasks resource profile 0
[2025-07-10T12:14:32.322+0200] {subprocess.py:93} INFO - 25/07/10 12:14:32 INFO TaskSetManager: Starting task 0.0 in stage 29.0 (TID 57) (138.4.31.17, executor driver, partition 0, NODE_LOCAL, 12952 bytes)
[2025-07-10T12:14:32.322+0200] {subprocess.py:93} INFO - 25/07/10 12:14:32 INFO Executor: Running task 0.0 in stage 29.0 (TID 57)
[2025-07-10T12:14:32.326+0200] {subprocess.py:93} INFO - 25/07/10 12:14:32 INFO ShuffleBlockFetcherIterator: Getting 1 (3.2 KiB) non-empty blocks including 1 (3.2 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2025-07-10T12:14:32.327+0200] {subprocess.py:93} INFO - 25/07/10 12:14:32 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2025-07-10T12:14:32.344+0200] {subprocess.py:93} INFO - 25/07/10 12:14:32 INFO Executor: Finished task 0.0 in stage 29.0 (TID 57). 9170 bytes result sent to driver
[2025-07-10T12:14:32.346+0200] {subprocess.py:93} INFO - 25/07/10 12:14:32 INFO TaskSetManager: Finished task 0.0 in stage 29.0 (TID 57) in 24 ms on 138.4.31.17 (executor driver) (1/1)
[2025-07-10T12:14:32.346+0200] {subprocess.py:93} INFO - 25/07/10 12:14:32 INFO TaskSchedulerImpl: Removed TaskSet 29.0, whose tasks have all completed, from pool
[2025-07-10T12:14:32.346+0200] {subprocess.py:93} INFO - 25/07/10 12:14:32 INFO DAGScheduler: ResultStage 29 (collect at StringIndexer.scala:204) finished in 0,028 s
[2025-07-10T12:14:32.347+0200] {subprocess.py:93} INFO - 25/07/10 12:14:32 INFO DAGScheduler: Job 18 is finished. Cancelling potential speculative or zombie tasks for this job
[2025-07-10T12:14:32.347+0200] {subprocess.py:93} INFO - 25/07/10 12:14:32 INFO TaskSchedulerImpl: Killing all running tasks in stage 29: Stage finished
[2025-07-10T12:14:32.347+0200] {subprocess.py:93} INFO - 25/07/10 12:14:32 INFO DAGScheduler: Job 18 finished: collect at StringIndexer.scala:204, took 0,031257 s
[2025-07-10T12:14:32.399+0200] {subprocess.py:93} INFO - 25/07/10 12:14:32 INFO FileSystemOverwrite: Path /home/monica.fernandez/practica_creativa/models/string_indexer_model_Dest.bin already exists. It will be overwritten.
[2025-07-10T12:14:32.414+0200] {subprocess.py:93} INFO - 25/07/10 12:14:32 INFO HadoopMapRedCommitProtocol: Using output committer class org.apache.hadoop.mapred.FileOutputCommitter
[2025-07-10T12:14:32.415+0200] {subprocess.py:93} INFO - 25/07/10 12:14:32 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2025-07-10T12:14:32.415+0200] {subprocess.py:93} INFO - 25/07/10 12:14:32 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2025-07-10T12:14:32.457+0200] {subprocess.py:93} INFO - 25/07/10 12:14:32 INFO SparkContext: Starting job: runJob at SparkHadoopWriter.scala:83
[2025-07-10T12:14:32.459+0200] {subprocess.py:93} INFO - 25/07/10 12:14:32 INFO DAGScheduler: Got job 19 (runJob at SparkHadoopWriter.scala:83) with 1 output partitions
[2025-07-10T12:14:32.460+0200] {subprocess.py:93} INFO - 25/07/10 12:14:32 INFO DAGScheduler: Final stage: ResultStage 30 (runJob at SparkHadoopWriter.scala:83)
[2025-07-10T12:14:32.460+0200] {subprocess.py:93} INFO - 25/07/10 12:14:32 INFO DAGScheduler: Parents of final stage: List()
[2025-07-10T12:14:32.460+0200] {subprocess.py:93} INFO - 25/07/10 12:14:32 INFO DAGScheduler: Missing parents: List()
[2025-07-10T12:14:32.460+0200] {subprocess.py:93} INFO - 25/07/10 12:14:32 INFO DAGScheduler: Submitting ResultStage 30 (MapPartitionsRDD[59] at saveAsTextFile at ReadWrite.scala:413), which has no missing parents
[2025-07-10T12:14:32.465+0200] {subprocess.py:93} INFO - 25/07/10 12:14:32 INFO MemoryStore: Block broadcast_22 stored as values in memory (estimated size 101.3 KiB, free 433.4 MiB)
[2025-07-10T12:14:32.466+0200] {subprocess.py:93} INFO - 25/07/10 12:14:32 INFO MemoryStore: Block broadcast_22_piece0 stored as bytes in memory (estimated size 36.5 KiB, free 433.3 MiB)
[2025-07-10T12:14:32.466+0200] {subprocess.py:93} INFO - 25/07/10 12:14:32 INFO BlockManagerInfo: Added broadcast_22_piece0 in memory on 138.4.31.17:32855 (size: 36.5 KiB, free: 434.2 MiB)
[2025-07-10T12:14:32.466+0200] {subprocess.py:93} INFO - 25/07/10 12:14:32 INFO SparkContext: Created broadcast 22 from broadcast at DAGScheduler.scala:1585
[2025-07-10T12:14:32.467+0200] {subprocess.py:93} INFO - 25/07/10 12:14:32 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 30 (MapPartitionsRDD[59] at saveAsTextFile at ReadWrite.scala:413) (first 15 tasks are for partitions Vector(0))
[2025-07-10T12:14:32.467+0200] {subprocess.py:93} INFO - 25/07/10 12:14:32 INFO TaskSchedulerImpl: Adding task set 30.0 with 1 tasks resource profile 0
[2025-07-10T12:14:32.468+0200] {subprocess.py:93} INFO - 25/07/10 12:14:32 INFO TaskSetManager: Starting task 0.0 in stage 30.0 (TID 58) (138.4.31.17, executor driver, partition 0, PROCESS_LOCAL, 13276 bytes)
[2025-07-10T12:14:32.471+0200] {subprocess.py:93} INFO - 25/07/10 12:14:32 INFO Executor: Running task 0.0 in stage 30.0 (TID 58)
[2025-07-10T12:14:32.474+0200] {subprocess.py:93} INFO - 25/07/10 12:14:32 INFO HadoopMapRedCommitProtocol: Using output committer class org.apache.hadoop.mapred.FileOutputCommitter
[2025-07-10T12:14:32.475+0200] {subprocess.py:93} INFO - 25/07/10 12:14:32 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2025-07-10T12:14:32.475+0200] {subprocess.py:93} INFO - 25/07/10 12:14:32 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2025-07-10T12:14:32.501+0200] {subprocess.py:93} INFO - 25/07/10 12:14:32 INFO FileOutputCommitter: Saved output of task 'attempt_20250710121432195746326855730488_0059_m_000000_0' to file:/home/monica.fernandez/practica_creativa/models/string_indexer_model_Dest.bin/metadata/_temporary/0/task_20250710121432195746326855730488_0059_m_000000
[2025-07-10T12:14:32.501+0200] {subprocess.py:93} INFO - 25/07/10 12:14:32 INFO SparkHadoopMapRedUtil: attempt_20250710121432195746326855730488_0059_m_000000_0: Committed. Elapsed time: 1 ms.
[2025-07-10T12:14:32.502+0200] {subprocess.py:93} INFO - 25/07/10 12:14:32 INFO Executor: Finished task 0.0 in stage 30.0 (TID 58). 1170 bytes result sent to driver
[2025-07-10T12:14:32.504+0200] {subprocess.py:93} INFO - 25/07/10 12:14:32 INFO TaskSetManager: Finished task 0.0 in stage 30.0 (TID 58) in 35 ms on 138.4.31.17 (executor driver) (1/1)
[2025-07-10T12:14:32.504+0200] {subprocess.py:93} INFO - 25/07/10 12:14:32 INFO TaskSchedulerImpl: Removed TaskSet 30.0, whose tasks have all completed, from pool
[2025-07-10T12:14:32.504+0200] {subprocess.py:93} INFO - 25/07/10 12:14:32 INFO DAGScheduler: ResultStage 30 (runJob at SparkHadoopWriter.scala:83) finished in 0,045 s
[2025-07-10T12:14:32.504+0200] {subprocess.py:93} INFO - 25/07/10 12:14:32 INFO DAGScheduler: Job 19 is finished. Cancelling potential speculative or zombie tasks for this job
[2025-07-10T12:14:32.504+0200] {subprocess.py:93} INFO - 25/07/10 12:14:32 INFO TaskSchedulerImpl: Killing all running tasks in stage 30: Stage finished
[2025-07-10T12:14:32.504+0200] {subprocess.py:93} INFO - 25/07/10 12:14:32 INFO DAGScheduler: Job 19 finished: runJob at SparkHadoopWriter.scala:83, took 0,047241 s
[2025-07-10T12:14:32.504+0200] {subprocess.py:93} INFO - 25/07/10 12:14:32 INFO SparkHadoopWriter: Start to commit write Job job_20250710121432195746326855730488_0059.
[2025-07-10T12:14:32.535+0200] {subprocess.py:93} INFO - 25/07/10 12:14:32 INFO SparkHadoopWriter: Write Job job_20250710121432195746326855730488_0059 committed. Elapsed time: 30 ms.
[2025-07-10T12:14:32.575+0200] {subprocess.py:93} INFO - 25/07/10 12:14:32 INFO DAGScheduler: Registering RDD 62 (parquet at StringIndexer.scala:499) as input to shuffle 11
[2025-07-10T12:14:32.575+0200] {subprocess.py:93} INFO - 25/07/10 12:14:32 INFO DAGScheduler: Got map stage job 20 (parquet at StringIndexer.scala:499) with 1 output partitions
[2025-07-10T12:14:32.575+0200] {subprocess.py:93} INFO - 25/07/10 12:14:32 INFO DAGScheduler: Final stage: ShuffleMapStage 31 (parquet at StringIndexer.scala:499)
[2025-07-10T12:14:32.575+0200] {subprocess.py:93} INFO - 25/07/10 12:14:32 INFO DAGScheduler: Parents of final stage: List()
[2025-07-10T12:14:32.575+0200] {subprocess.py:93} INFO - 25/07/10 12:14:32 INFO DAGScheduler: Missing parents: List()
[2025-07-10T12:14:32.575+0200] {subprocess.py:93} INFO - 25/07/10 12:14:32 INFO DAGScheduler: Submitting ShuffleMapStage 31 (MapPartitionsRDD[62] at parquet at StringIndexer.scala:499), which has no missing parents
[2025-07-10T12:14:32.575+0200] {subprocess.py:93} INFO - 25/07/10 12:14:32 INFO MemoryStore: Block broadcast_23 stored as values in memory (estimated size 8.1 KiB, free 433.3 MiB)
[2025-07-10T12:14:32.575+0200] {subprocess.py:93} INFO - 25/07/10 12:14:32 INFO MemoryStore: Block broadcast_23_piece0 stored as bytes in memory (estimated size 4.4 KiB, free 433.3 MiB)
[2025-07-10T12:14:32.577+0200] {subprocess.py:93} INFO - 25/07/10 12:14:32 INFO BlockManagerInfo: Added broadcast_23_piece0 in memory on 138.4.31.17:32855 (size: 4.4 KiB, free: 434.2 MiB)
[2025-07-10T12:14:32.578+0200] {subprocess.py:93} INFO - 25/07/10 12:14:32 INFO SparkContext: Created broadcast 23 from broadcast at DAGScheduler.scala:1585
[2025-07-10T12:14:32.578+0200] {subprocess.py:93} INFO - 25/07/10 12:14:32 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 31 (MapPartitionsRDD[62] at parquet at StringIndexer.scala:499) (first 15 tasks are for partitions Vector(0))
[2025-07-10T12:14:32.578+0200] {subprocess.py:93} INFO - 25/07/10 12:14:32 INFO TaskSchedulerImpl: Adding task set 31.0 with 1 tasks resource profile 0
[2025-07-10T12:14:32.579+0200] {subprocess.py:93} INFO - 25/07/10 12:14:32 INFO TaskSetManager: Starting task 0.0 in stage 31.0 (TID 59) (138.4.31.17, executor driver, partition 0, PROCESS_LOCAL, 18200 bytes)
[2025-07-10T12:14:32.581+0200] {subprocess.py:93} INFO - 25/07/10 12:14:32 INFO Executor: Running task 0.0 in stage 31.0 (TID 59)
[2025-07-10T12:14:32.584+0200] {subprocess.py:93} INFO - 25/07/10 12:14:32 INFO Executor: Finished task 0.0 in stage 31.0 (TID 59). 1628 bytes result sent to driver
[2025-07-10T12:14:32.585+0200] {subprocess.py:93} INFO - 25/07/10 12:14:32 INFO TaskSetManager: Finished task 0.0 in stage 31.0 (TID 59) in 6 ms on 138.4.31.17 (executor driver) (1/1)
[2025-07-10T12:14:32.585+0200] {subprocess.py:93} INFO - 25/07/10 12:14:32 INFO TaskSchedulerImpl: Removed TaskSet 31.0, whose tasks have all completed, from pool
[2025-07-10T12:14:32.585+0200] {subprocess.py:93} INFO - 25/07/10 12:14:32 INFO DAGScheduler: ShuffleMapStage 31 (parquet at StringIndexer.scala:499) finished in 0,012 s
[2025-07-10T12:14:32.586+0200] {subprocess.py:93} INFO - 25/07/10 12:14:32 INFO DAGScheduler: looking for newly runnable stages
[2025-07-10T12:14:32.586+0200] {subprocess.py:93} INFO - 25/07/10 12:14:32 INFO DAGScheduler: running: Set()
[2025-07-10T12:14:32.586+0200] {subprocess.py:93} INFO - 25/07/10 12:14:32 INFO DAGScheduler: waiting: Set()
[2025-07-10T12:14:32.586+0200] {subprocess.py:93} INFO - 25/07/10 12:14:32 INFO DAGScheduler: failed: Set()
[2025-07-10T12:14:32.597+0200] {subprocess.py:93} INFO - 25/07/10 12:14:32 INFO ParquetUtils: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
[2025-07-10T12:14:32.599+0200] {subprocess.py:93} INFO - 25/07/10 12:14:32 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2025-07-10T12:14:32.599+0200] {subprocess.py:93} INFO - 25/07/10 12:14:32 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2025-07-10T12:14:32.600+0200] {subprocess.py:93} INFO - 25/07/10 12:14:32 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[2025-07-10T12:14:32.600+0200] {subprocess.py:93} INFO - 25/07/10 12:14:32 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2025-07-10T12:14:32.600+0200] {subprocess.py:93} INFO - 25/07/10 12:14:32 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2025-07-10T12:14:32.600+0200] {subprocess.py:93} INFO - 25/07/10 12:14:32 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[2025-07-10T12:14:32.619+0200] {subprocess.py:93} INFO - 25/07/10 12:14:32 INFO SparkContext: Starting job: parquet at StringIndexer.scala:499
[2025-07-10T12:14:32.622+0200] {subprocess.py:93} INFO - 25/07/10 12:14:32 INFO DAGScheduler: Got job 21 (parquet at StringIndexer.scala:499) with 1 output partitions
[2025-07-10T12:14:32.623+0200] {subprocess.py:93} INFO - 25/07/10 12:14:32 INFO DAGScheduler: Final stage: ResultStage 33 (parquet at StringIndexer.scala:499)
[2025-07-10T12:14:32.623+0200] {subprocess.py:93} INFO - 25/07/10 12:14:32 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 32)
[2025-07-10T12:14:32.623+0200] {subprocess.py:93} INFO - 25/07/10 12:14:32 INFO DAGScheduler: Missing parents: List()
[2025-07-10T12:14:32.623+0200] {subprocess.py:93} INFO - 25/07/10 12:14:32 INFO DAGScheduler: Submitting ResultStage 33 (MapPartitionsRDD[64] at parquet at StringIndexer.scala:499), which has no missing parents
[2025-07-10T12:14:32.637+0200] {subprocess.py:93} INFO - 25/07/10 12:14:32 INFO MemoryStore: Block broadcast_24 stored as values in memory (estimated size 230.2 KiB, free 433.1 MiB)
[2025-07-10T12:14:32.638+0200] {subprocess.py:93} INFO - 25/07/10 12:14:32 INFO MemoryStore: Block broadcast_24_piece0 stored as bytes in memory (estimated size 81.0 KiB, free 433.0 MiB)
[2025-07-10T12:14:32.639+0200] {subprocess.py:93} INFO - 25/07/10 12:14:32 INFO BlockManagerInfo: Added broadcast_24_piece0 in memory on 138.4.31.17:32855 (size: 81.0 KiB, free: 434.1 MiB)
[2025-07-10T12:14:32.639+0200] {subprocess.py:93} INFO - 25/07/10 12:14:32 INFO SparkContext: Created broadcast 24 from broadcast at DAGScheduler.scala:1585
[2025-07-10T12:14:32.640+0200] {subprocess.py:93} INFO - 25/07/10 12:14:32 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 33 (MapPartitionsRDD[64] at parquet at StringIndexer.scala:499) (first 15 tasks are for partitions Vector(0))
[2025-07-10T12:14:32.640+0200] {subprocess.py:93} INFO - 25/07/10 12:14:32 INFO TaskSchedulerImpl: Adding task set 33.0 with 1 tasks resource profile 0
[2025-07-10T12:14:32.641+0200] {subprocess.py:93} INFO - 25/07/10 12:14:32 INFO TaskSetManager: Starting task 0.0 in stage 33.0 (TID 60) (138.4.31.17, executor driver, partition 0, NODE_LOCAL, 12952 bytes)
[2025-07-10T12:14:32.642+0200] {subprocess.py:93} INFO - 25/07/10 12:14:32 INFO Executor: Running task 0.0 in stage 33.0 (TID 60)
[2025-07-10T12:14:32.655+0200] {subprocess.py:93} INFO - 25/07/10 12:14:32 INFO ShuffleBlockFetcherIterator: Getting 1 (2.9 KiB) non-empty blocks including 1 (2.9 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2025-07-10T12:14:32.655+0200] {subprocess.py:93} INFO - 25/07/10 12:14:32 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 3 ms
[2025-07-10T12:14:32.656+0200] {subprocess.py:93} INFO - 25/07/10 12:14:32 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2025-07-10T12:14:32.656+0200] {subprocess.py:93} INFO - 25/07/10 12:14:32 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2025-07-10T12:14:32.656+0200] {subprocess.py:93} INFO - 25/07/10 12:14:32 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[2025-07-10T12:14:32.656+0200] {subprocess.py:93} INFO - 25/07/10 12:14:32 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2025-07-10T12:14:32.656+0200] {subprocess.py:93} INFO - 25/07/10 12:14:32 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2025-07-10T12:14:32.656+0200] {subprocess.py:93} INFO - 25/07/10 12:14:32 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[2025-07-10T12:14:32.656+0200] {subprocess.py:93} INFO - 25/07/10 12:14:32 INFO CodecConfig: Compression: SNAPPY
[2025-07-10T12:14:32.657+0200] {subprocess.py:93} INFO - 25/07/10 12:14:32 INFO CodecConfig: Compression: SNAPPY
[2025-07-10T12:14:32.659+0200] {subprocess.py:93} INFO - 25/07/10 12:14:32 INFO ParquetOutputFormat: ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]
[2025-07-10T12:14:32.663+0200] {subprocess.py:93} INFO - 25/07/10 12:14:32 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
[2025-07-10T12:14:32.663+0200] {subprocess.py:93} INFO - {
[2025-07-10T12:14:32.663+0200] {subprocess.py:93} INFO -   "type" : "struct",
[2025-07-10T12:14:32.663+0200] {subprocess.py:93} INFO -   "fields" : [ {
[2025-07-10T12:14:32.663+0200] {subprocess.py:93} INFO -     "name" : "labelsArray",
[2025-07-10T12:14:32.664+0200] {subprocess.py:93} INFO -     "type" : {
[2025-07-10T12:14:32.664+0200] {subprocess.py:93} INFO -       "type" : "array",
[2025-07-10T12:14:32.664+0200] {subprocess.py:93} INFO -       "elementType" : {
[2025-07-10T12:14:32.664+0200] {subprocess.py:93} INFO -         "type" : "array",
[2025-07-10T12:14:32.664+0200] {subprocess.py:93} INFO -         "elementType" : "string",
[2025-07-10T12:14:32.664+0200] {subprocess.py:93} INFO -         "containsNull" : true
[2025-07-10T12:14:32.664+0200] {subprocess.py:93} INFO -       },
[2025-07-10T12:14:32.664+0200] {subprocess.py:93} INFO -       "containsNull" : true
[2025-07-10T12:14:32.664+0200] {subprocess.py:93} INFO -     },
[2025-07-10T12:14:32.664+0200] {subprocess.py:93} INFO -     "nullable" : true,
[2025-07-10T12:14:32.664+0200] {subprocess.py:93} INFO -     "metadata" : { }
[2025-07-10T12:14:32.664+0200] {subprocess.py:93} INFO -   } ]
[2025-07-10T12:14:32.664+0200] {subprocess.py:93} INFO - }
[2025-07-10T12:14:32.664+0200] {subprocess.py:93} INFO - and corresponding Parquet message type:
[2025-07-10T12:14:32.664+0200] {subprocess.py:93} INFO - message spark_schema {
[2025-07-10T12:14:32.664+0200] {subprocess.py:93} INFO -   optional group labelsArray (LIST) {
[2025-07-10T12:14:32.664+0200] {subprocess.py:93} INFO -     repeated group list {
[2025-07-10T12:14:32.664+0200] {subprocess.py:93} INFO -       optional group element (LIST) {
[2025-07-10T12:14:32.665+0200] {subprocess.py:93} INFO -         repeated group list {
[2025-07-10T12:14:32.665+0200] {subprocess.py:93} INFO -           optional binary element (STRING);
[2025-07-10T12:14:32.665+0200] {subprocess.py:93} INFO -         }
[2025-07-10T12:14:32.665+0200] {subprocess.py:93} INFO -       }
[2025-07-10T12:14:32.665+0200] {subprocess.py:93} INFO -     }
[2025-07-10T12:14:32.665+0200] {subprocess.py:93} INFO -   }
[2025-07-10T12:14:32.665+0200] {subprocess.py:93} INFO - }
[2025-07-10T12:14:32.665+0200] {subprocess.py:93} INFO - 
[2025-07-10T12:14:32.665+0200] {subprocess.py:93} INFO - 
[2025-07-10T12:14:32.696+0200] {subprocess.py:93} INFO - 25/07/10 12:14:32 INFO FileOutputCommitter: Saved output of task 'attempt_202507101214322645327086318476471_0033_m_000000_60' to file:/home/monica.fernandez/practica_creativa/models/string_indexer_model_Dest.bin/data/_temporary/0/task_202507101214322645327086318476471_0033_m_000000
[2025-07-10T12:14:32.696+0200] {subprocess.py:93} INFO - 25/07/10 12:14:32 INFO SparkHadoopMapRedUtil: attempt_202507101214322645327086318476471_0033_m_000000_60: Committed. Elapsed time: 1 ms.
[2025-07-10T12:14:32.697+0200] {subprocess.py:93} INFO - 25/07/10 12:14:32 INFO Executor: Finished task 0.0 in stage 33.0 (TID 60). 4740 bytes result sent to driver
[2025-07-10T12:14:32.697+0200] {subprocess.py:93} INFO - 25/07/10 12:14:32 INFO TaskSetManager: Finished task 0.0 in stage 33.0 (TID 60) in 56 ms on 138.4.31.17 (executor driver) (1/1)
[2025-07-10T12:14:32.697+0200] {subprocess.py:93} INFO - 25/07/10 12:14:32 INFO TaskSchedulerImpl: Removed TaskSet 33.0, whose tasks have all completed, from pool
[2025-07-10T12:14:32.698+0200] {subprocess.py:93} INFO - 25/07/10 12:14:32 INFO DAGScheduler: ResultStage 33 (parquet at StringIndexer.scala:499) finished in 0,077 s
[2025-07-10T12:14:32.698+0200] {subprocess.py:93} INFO - 25/07/10 12:14:32 INFO DAGScheduler: Job 21 is finished. Cancelling potential speculative or zombie tasks for this job
[2025-07-10T12:14:32.699+0200] {subprocess.py:93} INFO - 25/07/10 12:14:32 INFO TaskSchedulerImpl: Killing all running tasks in stage 33: Stage finished
[2025-07-10T12:14:32.699+0200] {subprocess.py:93} INFO - 25/07/10 12:14:32 INFO DAGScheduler: Job 21 finished: parquet at StringIndexer.scala:499, took 0,078817 s
[2025-07-10T12:14:32.699+0200] {subprocess.py:93} INFO - 25/07/10 12:14:32 INFO FileFormatWriter: Start to commit write Job 64b8b0d8-20a5-45dd-9785-636b20cb2b96.
[2025-07-10T12:14:32.718+0200] {subprocess.py:93} INFO - 25/07/10 12:14:32 INFO FileFormatWriter: Write Job 64b8b0d8-20a5-45dd-9785-636b20cb2b96 committed. Elapsed time: 19 ms.
[2025-07-10T12:14:32.718+0200] {subprocess.py:93} INFO - 25/07/10 12:14:32 INFO FileFormatWriter: Finished processing stats for write job 64b8b0d8-20a5-45dd-9785-636b20cb2b96.
[2025-07-10T12:14:32.788+0200] {subprocess.py:93} INFO - 25/07/10 12:14:32 INFO V2ScanRelationPushDown:
[2025-07-10T12:14:32.788+0200] {subprocess.py:93} INFO - Pushing operators to origin_dest_distance
[2025-07-10T12:14:32.788+0200] {subprocess.py:93} INFO - Pushed Filters:
[2025-07-10T12:14:32.788+0200] {subprocess.py:93} INFO - Post-Scan Filters:
[2025-07-10T12:14:32.788+0200] {subprocess.py:93} INFO - 
[2025-07-10T12:14:32.796+0200] {subprocess.py:93} INFO - 25/07/10 12:14:32 INFO V2ScanRelationPushDown:
[2025-07-10T12:14:32.796+0200] {subprocess.py:93} INFO - Output: origin#42, dest#43
[2025-07-10T12:14:32.797+0200] {subprocess.py:93} INFO - 
[2025-07-10T12:14:32.852+0200] {subprocess.py:93} INFO - 25/07/10 12:14:32 INFO FileSourceStrategy: Pushed Filters:
[2025-07-10T12:14:32.852+0200] {subprocess.py:93} INFO - 25/07/10 12:14:32 INFO FileSourceStrategy: Post-Scan Filters:
[2025-07-10T12:14:32.913+0200] {subprocess.py:93} INFO - 25/07/10 12:14:32 INFO CodeGenerator: Code generated in 23.487693 ms
[2025-07-10T12:14:32.919+0200] {subprocess.py:93} INFO - 25/07/10 12:14:32 INFO MemoryStore: Block broadcast_25 stored as values in memory (estimated size 209.9 KiB, free 432.8 MiB)
[2025-07-10T12:14:32.932+0200] {subprocess.py:93} INFO - 25/07/10 12:14:32 INFO MemoryStore: Block broadcast_25_piece0 stored as bytes in memory (estimated size 37.7 KiB, free 432.8 MiB)
[2025-07-10T12:14:32.934+0200] {subprocess.py:93} INFO - 25/07/10 12:14:32 INFO BlockManagerInfo: Added broadcast_25_piece0 in memory on 138.4.31.17:32855 (size: 37.7 KiB, free: 434.1 MiB)
[2025-07-10T12:14:32.935+0200] {subprocess.py:93} INFO - 25/07/10 12:14:32 INFO SparkContext: Created broadcast 25 from collect at StringIndexer.scala:204
[2025-07-10T12:14:32.935+0200] {subprocess.py:93} INFO - 25/07/10 12:14:32 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
[2025-07-10T12:14:32.945+0200] {subprocess.py:93} INFO - 25/07/10 12:14:32 INFO DAGScheduler: Registering RDD 68 (collect at StringIndexer.scala:204) as input to shuffle 12
[2025-07-10T12:14:32.945+0200] {subprocess.py:93} INFO - 25/07/10 12:14:32 INFO DAGScheduler: Got map stage job 22 (collect at StringIndexer.scala:204) with 2 output partitions
[2025-07-10T12:14:32.945+0200] {subprocess.py:93} INFO - 25/07/10 12:14:32 INFO DAGScheduler: Final stage: ShuffleMapStage 34 (collect at StringIndexer.scala:204)
[2025-07-10T12:14:32.945+0200] {subprocess.py:93} INFO - 25/07/10 12:14:32 INFO DAGScheduler: Parents of final stage: List()
[2025-07-10T12:14:32.948+0200] {subprocess.py:93} INFO - 25/07/10 12:14:32 INFO DAGScheduler: Missing parents: List()
[2025-07-10T12:14:32.948+0200] {subprocess.py:93} INFO - 25/07/10 12:14:32 INFO DAGScheduler: Submitting ShuffleMapStage 34 (MapPartitionsRDD[68] at collect at StringIndexer.scala:204), which has no missing parents
[2025-07-10T12:14:32.949+0200] {subprocess.py:93} INFO - 25/07/10 12:14:32 INFO MemoryStore: Block broadcast_26 stored as values in memory (estimated size 18.0 KiB, free 432.7 MiB)
[2025-07-10T12:14:32.956+0200] {subprocess.py:93} INFO - 25/07/10 12:14:32 INFO MemoryStore: Block broadcast_26_piece0 stored as bytes in memory (estimated size 8.6 KiB, free 432.7 MiB)
[2025-07-10T12:14:32.957+0200] {subprocess.py:93} INFO - 25/07/10 12:14:32 INFO BlockManagerInfo: Added broadcast_26_piece0 in memory on 138.4.31.17:32855 (size: 8.6 KiB, free: 434.1 MiB)
[2025-07-10T12:14:32.957+0200] {subprocess.py:93} INFO - 25/07/10 12:14:32 INFO SparkContext: Created broadcast 26 from broadcast at DAGScheduler.scala:1585
[2025-07-10T12:14:32.958+0200] {subprocess.py:93} INFO - 25/07/10 12:14:32 INFO DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 34 (MapPartitionsRDD[68] at collect at StringIndexer.scala:204) (first 15 tasks are for partitions Vector(0, 1))
[2025-07-10T12:14:32.958+0200] {subprocess.py:93} INFO - 25/07/10 12:14:32 INFO TaskSchedulerImpl: Adding task set 34.0 with 2 tasks resource profile 0
[2025-07-10T12:14:32.960+0200] {subprocess.py:93} INFO - 25/07/10 12:14:32 INFO TaskSetManager: Starting task 0.0 in stage 34.0 (TID 61) (138.4.31.17, executor driver, partition 0, PROCESS_LOCAL, 13595 bytes)
[2025-07-10T12:14:32.960+0200] {subprocess.py:93} INFO - 25/07/10 12:14:32 INFO TaskSetManager: Starting task 1.0 in stage 34.0 (TID 62) (138.4.31.17, executor driver, partition 1, PROCESS_LOCAL, 13595 bytes)
[2025-07-10T12:14:32.964+0200] {subprocess.py:93} INFO - 25/07/10 12:14:32 INFO Executor: Running task 1.0 in stage 34.0 (TID 62)
[2025-07-10T12:14:32.964+0200] {subprocess.py:93} INFO - 25/07/10 12:14:32 INFO Executor: Running task 0.0 in stage 34.0 (TID 61)
[2025-07-10T12:14:32.972+0200] {subprocess.py:93} INFO - 25/07/10 12:14:32 INFO CodeGenerator: Code generated in 6.96182 ms
[2025-07-10T12:14:32.974+0200] {subprocess.py:93} INFO - 25/07/10 12:14:32 INFO BlockManagerInfo: Removed broadcast_23_piece0 on 138.4.31.17:32855 in memory (size: 4.4 KiB, free: 434.1 MiB)
[2025-07-10T12:14:32.980+0200] {subprocess.py:93} INFO - 25/07/10 12:14:32 INFO FileScanRDD: Reading File path: file:///home/monica.fernandez/practica_creativa/data/simple_flight_delay_features.jsonl.bz2, range: 4194304-4676447, partition values: [empty row]
[2025-07-10T12:14:32.982+0200] {subprocess.py:93} INFO - 25/07/10 12:14:32 INFO FileScanRDD: Reading File path: file:///home/monica.fernandez/practica_creativa/data/simple_flight_delay_features.jsonl.bz2, range: 0-4194304, partition values: [empty row]
[2025-07-10T12:14:32.987+0200] {subprocess.py:93} INFO - 25/07/10 12:14:32 INFO CodeGenerator: Code generated in 10.14029 ms
[2025-07-10T12:14:32.993+0200] {subprocess.py:93} INFO - 25/07/10 12:14:32 INFO DAGScheduler: Registering RDD 72 (collect at StringIndexer.scala:204) as input to shuffle 13
[2025-07-10T12:14:32.995+0200] {subprocess.py:93} INFO - 25/07/10 12:14:32 INFO DAGScheduler: Got map stage job 23 (collect at StringIndexer.scala:204) with 13 output partitions
[2025-07-10T12:14:32.996+0200] {subprocess.py:93} INFO - 25/07/10 12:14:32 INFO DAGScheduler: Final stage: ShuffleMapStage 35 (collect at StringIndexer.scala:204)
[2025-07-10T12:14:32.996+0200] {subprocess.py:93} INFO - 25/07/10 12:14:32 INFO DAGScheduler: Parents of final stage: List()
[2025-07-10T12:14:32.996+0200] {subprocess.py:93} INFO - 25/07/10 12:14:32 INFO DAGScheduler: Missing parents: List()
[2025-07-10T12:14:32.996+0200] {subprocess.py:93} INFO - 25/07/10 12:14:32 INFO DAGScheduler: Submitting ShuffleMapStage 35 (MapPartitionsRDD[72] at collect at StringIndexer.scala:204), which has no missing parents
[2025-07-10T12:14:32.997+0200] {subprocess.py:93} INFO - 25/07/10 12:14:32 INFO MemoryStore: Block broadcast_27 stored as values in memory (estimated size 18.2 KiB, free 432.7 MiB)
[2025-07-10T12:14:32.998+0200] {subprocess.py:93} INFO - 25/07/10 12:14:32 INFO MemoryStore: Block broadcast_27_piece0 stored as bytes in memory (estimated size 9.1 KiB, free 432.7 MiB)
[2025-07-10T12:14:32.998+0200] {subprocess.py:93} INFO - 25/07/10 12:14:32 INFO BlockManagerInfo: Added broadcast_27_piece0 in memory on 138.4.31.17:32855 (size: 9.1 KiB, free: 434.0 MiB)
[2025-07-10T12:14:32.999+0200] {subprocess.py:93} INFO - 25/07/10 12:14:32 INFO SparkContext: Created broadcast 27 from broadcast at DAGScheduler.scala:1585
[2025-07-10T12:14:32.999+0200] {subprocess.py:93} INFO - 25/07/10 12:14:32 INFO DAGScheduler: Submitting 13 missing tasks from ShuffleMapStage 35 (MapPartitionsRDD[72] at collect at StringIndexer.scala:204) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12))
[2025-07-10T12:14:32.999+0200] {subprocess.py:93} INFO - 25/07/10 12:14:32 INFO TaskSchedulerImpl: Adding task set 35.0 with 13 tasks resource profile 0
[2025-07-10T12:14:33.007+0200] {subprocess.py:93} INFO - 25/07/10 12:14:33 INFO TaskSetManager: Starting task 0.0 in stage 35.0 (TID 63) (138.4.31.17, executor driver, partition 0, ANY, 14843 bytes)
[2025-07-10T12:14:33.008+0200] {subprocess.py:93} INFO - 25/07/10 12:14:33 INFO TaskSetManager: Starting task 1.0 in stage 35.0 (TID 64) (138.4.31.17, executor driver, partition 1, ANY, 14839 bytes)
[2025-07-10T12:14:33.008+0200] {subprocess.py:93} INFO - 25/07/10 12:14:33 INFO Executor: Running task 1.0 in stage 35.0 (TID 64)
[2025-07-10T12:14:33.008+0200] {subprocess.py:93} INFO - 25/07/10 12:14:33 INFO Executor: Running task 0.0 in stage 35.0 (TID 63)
[2025-07-10T12:14:33.014+0200] {subprocess.py:93} INFO - 25/07/10 12:14:33 INFO BlockManagerInfo: Removed broadcast_22_piece0 on 138.4.31.17:32855 in memory (size: 36.5 KiB, free: 434.1 MiB)
[2025-07-10T12:14:33.015+0200] {subprocess.py:93} INFO - 25/07/10 12:14:33 INFO CodeGenerator: Code generated in 7.64708 ms
[2025-07-10T12:14:33.029+0200] {subprocess.py:93} INFO - 25/07/10 12:14:33 INFO Executor: Finished task 1.0 in stage 35.0 (TID 64). 1816 bytes result sent to driver
[2025-07-10T12:14:33.035+0200] {subprocess.py:93} INFO - 25/07/10 12:14:33 INFO TaskSetManager: Starting task 2.0 in stage 35.0 (TID 65) (138.4.31.17, executor driver, partition 2, ANY, 14963 bytes)
[2025-07-10T12:14:33.036+0200] {subprocess.py:93} INFO - 25/07/10 12:14:33 INFO TaskSetManager: Finished task 1.0 in stage 35.0 (TID 64) in 33 ms on 138.4.31.17 (executor driver) (1/13)
[2025-07-10T12:14:33.036+0200] {subprocess.py:93} INFO - 25/07/10 12:14:33 INFO Executor: Running task 2.0 in stage 35.0 (TID 65)
[2025-07-10T12:14:33.041+0200] {subprocess.py:93} INFO - 25/07/10 12:14:33 INFO Executor: Finished task 0.0 in stage 35.0 (TID 63). 1816 bytes result sent to driver
[2025-07-10T12:14:33.042+0200] {subprocess.py:93} INFO - 25/07/10 12:14:33 INFO TaskSetManager: Starting task 3.0 in stage 35.0 (TID 66) (138.4.31.17, executor driver, partition 3, ANY, 14843 bytes)
[2025-07-10T12:14:33.042+0200] {subprocess.py:93} INFO - 25/07/10 12:14:33 INFO TaskSetManager: Finished task 0.0 in stage 35.0 (TID 63) in 40 ms on 138.4.31.17 (executor driver) (2/13)
[2025-07-10T12:14:33.042+0200] {subprocess.py:93} INFO - 25/07/10 12:14:33 INFO Executor: Running task 3.0 in stage 35.0 (TID 66)
[2025-07-10T12:14:33.054+0200] {subprocess.py:93} INFO - 25/07/10 12:14:33 INFO Executor: Finished task 3.0 in stage 35.0 (TID 66). 1816 bytes result sent to driver
[2025-07-10T12:14:33.060+0200] {subprocess.py:93} INFO - 25/07/10 12:14:33 INFO TaskSetManager: Starting task 4.0 in stage 35.0 (TID 67) (138.4.31.17, executor driver, partition 4, ANY, 14843 bytes)
[2025-07-10T12:14:33.061+0200] {subprocess.py:93} INFO - 25/07/10 12:14:33 INFO TaskSetManager: Finished task 3.0 in stage 35.0 (TID 66) in 19 ms on 138.4.31.17 (executor driver) (3/13)
[2025-07-10T12:14:33.061+0200] {subprocess.py:93} INFO - 25/07/10 12:14:33 INFO Executor: Running task 4.0 in stage 35.0 (TID 67)
[2025-07-10T12:14:33.074+0200] {subprocess.py:93} INFO - 25/07/10 12:14:33 INFO Executor: Finished task 4.0 in stage 35.0 (TID 67). 1816 bytes result sent to driver
[2025-07-10T12:14:33.077+0200] {subprocess.py:93} INFO - 25/07/10 12:14:33 INFO Executor: Finished task 2.0 in stage 35.0 (TID 65). 1816 bytes result sent to driver
[2025-07-10T12:14:33.079+0200] {subprocess.py:93} INFO - 25/07/10 12:14:33 INFO TaskSetManager: Starting task 5.0 in stage 35.0 (TID 68) (138.4.31.17, executor driver, partition 5, ANY, 14843 bytes)
[2025-07-10T12:14:33.080+0200] {subprocess.py:93} INFO - 25/07/10 12:14:33 INFO TaskSetManager: Finished task 4.0 in stage 35.0 (TID 67) in 19 ms on 138.4.31.17 (executor driver) (4/13)
[2025-07-10T12:14:33.080+0200] {subprocess.py:93} INFO - 25/07/10 12:14:33 INFO Executor: Running task 5.0 in stage 35.0 (TID 68)
[2025-07-10T12:14:33.083+0200] {subprocess.py:93} INFO - 25/07/10 12:14:33 INFO TaskSetManager: Starting task 6.0 in stage 35.0 (TID 69) (138.4.31.17, executor driver, partition 6, ANY, 14843 bytes)
[2025-07-10T12:14:33.083+0200] {subprocess.py:93} INFO - 25/07/10 12:14:33 INFO TaskSetManager: Finished task 2.0 in stage 35.0 (TID 65) in 48 ms on 138.4.31.17 (executor driver) (5/13)
[2025-07-10T12:14:33.084+0200] {subprocess.py:93} INFO - 25/07/10 12:14:33 INFO Executor: Running task 6.0 in stage 35.0 (TID 69)
[2025-07-10T12:14:33.086+0200] {subprocess.py:93} INFO - 25/07/10 12:14:33 INFO BlockManagerInfo: Removed broadcast_24_piece0 on 138.4.31.17:32855 in memory (size: 81.0 KiB, free: 434.2 MiB)
[2025-07-10T12:14:33.105+0200] {subprocess.py:93} INFO - 25/07/10 12:14:33 INFO Executor: Finished task 5.0 in stage 35.0 (TID 68). 1816 bytes result sent to driver
[2025-07-10T12:14:33.106+0200] {subprocess.py:93} INFO - 25/07/10 12:14:33 INFO TaskSetManager: Starting task 7.0 in stage 35.0 (TID 70) (138.4.31.17, executor driver, partition 7, ANY, 14963 bytes)
[2025-07-10T12:14:33.107+0200] {subprocess.py:93} INFO - 25/07/10 12:14:33 INFO Executor: Running task 7.0 in stage 35.0 (TID 70)
[2025-07-10T12:14:33.108+0200] {subprocess.py:93} INFO - 25/07/10 12:14:33 INFO TaskSetManager: Finished task 5.0 in stage 35.0 (TID 68) in 30 ms on 138.4.31.17 (executor driver) (6/13)
[2025-07-10T12:14:33.122+0200] {subprocess.py:93} INFO - 25/07/10 12:14:33 INFO Executor: Finished task 7.0 in stage 35.0 (TID 70). 1816 bytes result sent to driver
[2025-07-10T12:14:33.123+0200] {subprocess.py:93} INFO - 25/07/10 12:14:33 INFO TaskSetManager: Starting task 8.0 in stage 35.0 (TID 71) (138.4.31.17, executor driver, partition 8, ANY, 14843 bytes)
[2025-07-10T12:14:33.123+0200] {subprocess.py:93} INFO - 25/07/10 12:14:33 INFO TaskSetManager: Finished task 7.0 in stage 35.0 (TID 70) in 17 ms on 138.4.31.17 (executor driver) (7/13)
[2025-07-10T12:14:33.124+0200] {subprocess.py:93} INFO - 25/07/10 12:14:33 INFO Executor: Running task 8.0 in stage 35.0 (TID 71)
[2025-07-10T12:14:33.126+0200] {subprocess.py:93} INFO - 25/07/10 12:14:33 INFO Executor: Finished task 6.0 in stage 35.0 (TID 69). 1859 bytes result sent to driver
[2025-07-10T12:14:33.127+0200] {subprocess.py:93} INFO - 25/07/10 12:14:33 INFO TaskSetManager: Starting task 9.0 in stage 35.0 (TID 72) (138.4.31.17, executor driver, partition 9, ANY, 14843 bytes)
[2025-07-10T12:14:33.129+0200] {subprocess.py:93} INFO - 25/07/10 12:14:33 INFO TaskSetManager: Finished task 6.0 in stage 35.0 (TID 69) in 46 ms on 138.4.31.17 (executor driver) (8/13)
[2025-07-10T12:14:33.129+0200] {subprocess.py:93} INFO - 25/07/10 12:14:33 INFO Executor: Running task 9.0 in stage 35.0 (TID 72)
[2025-07-10T12:14:33.142+0200] {subprocess.py:93} INFO - 25/07/10 12:14:33 INFO BlockManagerInfo: Removed broadcast_21_piece0 on 138.4.31.17:32855 in memory (size: 18.3 KiB, free: 434.2 MiB)
[2025-07-10T12:14:33.154+0200] {subprocess.py:93} INFO - 25/07/10 12:14:33 INFO Executor: Finished task 8.0 in stage 35.0 (TID 71). 1816 bytes result sent to driver
[2025-07-10T12:14:33.155+0200] {subprocess.py:93} INFO - 25/07/10 12:14:33 INFO TaskSetManager: Starting task 10.0 in stage 35.0 (TID 73) (138.4.31.17, executor driver, partition 10, ANY, 14843 bytes)
[2025-07-10T12:14:33.156+0200] {subprocess.py:93} INFO - 25/07/10 12:14:33 INFO TaskSetManager: Finished task 8.0 in stage 35.0 (TID 71) in 33 ms on 138.4.31.17 (executor driver) (9/13)
[2025-07-10T12:14:33.156+0200] {subprocess.py:93} INFO - 25/07/10 12:14:33 INFO Executor: Running task 10.0 in stage 35.0 (TID 73)
[2025-07-10T12:14:33.159+0200] {subprocess.py:93} INFO - 25/07/10 12:14:33 INFO Executor: Finished task 9.0 in stage 35.0 (TID 72). 1816 bytes result sent to driver
[2025-07-10T12:14:33.160+0200] {subprocess.py:93} INFO - 25/07/10 12:14:33 INFO TaskSetManager: Starting task 11.0 in stage 35.0 (TID 74) (138.4.31.17, executor driver, partition 11, ANY, 14843 bytes)
[2025-07-10T12:14:33.160+0200] {subprocess.py:93} INFO - 25/07/10 12:14:33 INFO TaskSetManager: Finished task 9.0 in stage 35.0 (TID 72) in 33 ms on 138.4.31.17 (executor driver) (10/13)
[2025-07-10T12:14:33.161+0200] {subprocess.py:93} INFO - 25/07/10 12:14:33 INFO Executor: Running task 11.0 in stage 35.0 (TID 74)
[2025-07-10T12:14:33.170+0200] {subprocess.py:93} INFO - 25/07/10 12:14:33 INFO Executor: Finished task 11.0 in stage 35.0 (TID 74). 1816 bytes result sent to driver
[2025-07-10T12:14:33.171+0200] {subprocess.py:93} INFO - 25/07/10 12:14:33 INFO TaskSetManager: Starting task 12.0 in stage 35.0 (TID 75) (138.4.31.17, executor driver, partition 12, ANY, 14963 bytes)
[2025-07-10T12:14:33.172+0200] {subprocess.py:93} INFO - 25/07/10 12:14:33 INFO TaskSetManager: Finished task 11.0 in stage 35.0 (TID 74) in 12 ms on 138.4.31.17 (executor driver) (11/13)
[2025-07-10T12:14:33.172+0200] {subprocess.py:93} INFO - 25/07/10 12:14:33 INFO Executor: Running task 12.0 in stage 35.0 (TID 75)
[2025-07-10T12:14:33.188+0200] {subprocess.py:93} INFO - 25/07/10 12:14:33 INFO BlockManagerInfo: Removed broadcast_20_piece0 on 138.4.31.17:32855 in memory (size: 18.0 KiB, free: 434.2 MiB)
[2025-07-10T12:14:33.189+0200] {subprocess.py:93} INFO - 25/07/10 12:14:33 INFO Executor: Finished task 10.0 in stage 35.0 (TID 73). 1816 bytes result sent to driver
[2025-07-10T12:14:33.190+0200] {subprocess.py:93} INFO - 25/07/10 12:14:33 INFO TaskSetManager: Finished task 10.0 in stage 35.0 (TID 73) in 34 ms on 138.4.31.17 (executor driver) (12/13)
[2025-07-10T12:14:33.197+0200] {subprocess.py:93} INFO - 25/07/10 12:14:33 INFO Executor: Finished task 12.0 in stage 35.0 (TID 75). 1816 bytes result sent to driver
[2025-07-10T12:14:33.198+0200] {subprocess.py:93} INFO - 25/07/10 12:14:33 INFO TaskSetManager: Finished task 12.0 in stage 35.0 (TID 75) in 26 ms on 138.4.31.17 (executor driver) (13/13)
[2025-07-10T12:14:33.199+0200] {subprocess.py:93} INFO - 25/07/10 12:14:33 INFO TaskSchedulerImpl: Removed TaskSet 35.0, whose tasks have all completed, from pool
[2025-07-10T12:14:33.199+0200] {subprocess.py:93} INFO - 25/07/10 12:14:33 INFO DAGScheduler: ShuffleMapStage 35 (collect at StringIndexer.scala:204) finished in 0,204 s
[2025-07-10T12:14:33.199+0200] {subprocess.py:93} INFO - 25/07/10 12:14:33 INFO DAGScheduler: looking for newly runnable stages
[2025-07-10T12:14:33.199+0200] {subprocess.py:93} INFO - 25/07/10 12:14:33 INFO DAGScheduler: running: Set(ShuffleMapStage 34)
[2025-07-10T12:14:33.199+0200] {subprocess.py:93} INFO - 25/07/10 12:14:33 INFO DAGScheduler: waiting: Set()
[2025-07-10T12:14:33.199+0200] {subprocess.py:93} INFO - 25/07/10 12:14:33 INFO DAGScheduler: failed: Set()
[2025-07-10T12:14:34.077+0200] {subprocess.py:93} INFO - 25/07/10 12:14:34 INFO Executor: Finished task 1.0 in stage 34.0 (TID 62). 2074 bytes result sent to driver
[2025-07-10T12:14:34.078+0200] {subprocess.py:93} INFO - 25/07/10 12:14:34 INFO BlockManagerInfo: Removed broadcast_27_piece0 on 138.4.31.17:32855 in memory (size: 9.1 KiB, free: 434.2 MiB)
[2025-07-10T12:14:34.078+0200] {subprocess.py:93} INFO - 25/07/10 12:14:34 INFO TaskSetManager: Finished task 1.0 in stage 34.0 (TID 62) in 1118 ms on 138.4.31.17 (executor driver) (1/2)
[2025-07-10T12:14:41.340+0200] {subprocess.py:93} INFO - 25/07/10 12:14:41 INFO Executor: Finished task 0.0 in stage 34.0 (TID 61). 2031 bytes result sent to driver
[2025-07-10T12:14:41.342+0200] {subprocess.py:93} INFO - 25/07/10 12:14:41 INFO TaskSetManager: Finished task 0.0 in stage 34.0 (TID 61) in 8381 ms on 138.4.31.17 (executor driver) (2/2)
[2025-07-10T12:14:41.342+0200] {subprocess.py:93} INFO - 25/07/10 12:14:41 INFO TaskSchedulerImpl: Removed TaskSet 34.0, whose tasks have all completed, from pool
[2025-07-10T12:14:41.343+0200] {subprocess.py:93} INFO - 25/07/10 12:14:41 INFO DAGScheduler: ShuffleMapStage 34 (collect at StringIndexer.scala:204) finished in 8,396 s
[2025-07-10T12:14:41.343+0200] {subprocess.py:93} INFO - 25/07/10 12:14:41 INFO DAGScheduler: looking for newly runnable stages
[2025-07-10T12:14:41.343+0200] {subprocess.py:93} INFO - 25/07/10 12:14:41 INFO DAGScheduler: running: Set()
[2025-07-10T12:14:41.343+0200] {subprocess.py:93} INFO - 25/07/10 12:14:41 INFO DAGScheduler: waiting: Set()
[2025-07-10T12:14:41.343+0200] {subprocess.py:93} INFO - 25/07/10 12:14:41 INFO DAGScheduler: failed: Set()
[2025-07-10T12:14:41.347+0200] {subprocess.py:93} INFO - 25/07/10 12:14:41 INFO ShufflePartitionsUtil: For shuffle(12), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576
[2025-07-10T12:14:41.368+0200] {subprocess.py:93} INFO - 25/07/10 12:14:41 INFO CodeGenerator: Code generated in 6.023631 ms
[2025-07-10T12:14:41.378+0200] {subprocess.py:93} INFO - 25/07/10 12:14:41 INFO DAGScheduler: Registering RDD 76 (collect at StringIndexer.scala:204) as input to shuffle 14
[2025-07-10T12:14:41.379+0200] {subprocess.py:93} INFO - 25/07/10 12:14:41 INFO DAGScheduler: Got map stage job 24 (collect at StringIndexer.scala:204) with 1 output partitions
[2025-07-10T12:14:41.379+0200] {subprocess.py:93} INFO - 25/07/10 12:14:41 INFO DAGScheduler: Final stage: ShuffleMapStage 37 (collect at StringIndexer.scala:204)
[2025-07-10T12:14:41.379+0200] {subprocess.py:93} INFO - 25/07/10 12:14:41 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 36)
[2025-07-10T12:14:41.379+0200] {subprocess.py:93} INFO - 25/07/10 12:14:41 INFO DAGScheduler: Missing parents: List()
[2025-07-10T12:14:41.379+0200] {subprocess.py:93} INFO - 25/07/10 12:14:41 INFO DAGScheduler: Submitting ShuffleMapStage 37 (MapPartitionsRDD[76] at collect at StringIndexer.scala:204), which has no missing parents
[2025-07-10T12:14:41.385+0200] {subprocess.py:93} INFO - 25/07/10 12:14:41 INFO MemoryStore: Block broadcast_28 stored as values in memory (estimated size 38.8 KiB, free 433.3 MiB)
[2025-07-10T12:14:41.386+0200] {subprocess.py:93} INFO - 25/07/10 12:14:41 INFO MemoryStore: Block broadcast_28_piece0 stored as bytes in memory (estimated size 18.7 KiB, free 433.2 MiB)
[2025-07-10T12:14:41.387+0200] {subprocess.py:93} INFO - 25/07/10 12:14:41 INFO BlockManagerInfo: Added broadcast_28_piece0 in memory on 138.4.31.17:32855 (size: 18.7 KiB, free: 434.2 MiB)
[2025-07-10T12:14:41.388+0200] {subprocess.py:93} INFO - 25/07/10 12:14:41 INFO SparkContext: Created broadcast 28 from broadcast at DAGScheduler.scala:1585
[2025-07-10T12:14:41.389+0200] {subprocess.py:93} INFO - 25/07/10 12:14:41 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 37 (MapPartitionsRDD[76] at collect at StringIndexer.scala:204) (first 15 tasks are for partitions Vector(0))
[2025-07-10T12:14:41.389+0200] {subprocess.py:93} INFO - 25/07/10 12:14:41 INFO TaskSchedulerImpl: Adding task set 37.0 with 1 tasks resource profile 0
[2025-07-10T12:14:41.390+0200] {subprocess.py:93} INFO - 25/07/10 12:14:41 INFO TaskSetManager: Starting task 0.0 in stage 37.0 (TID 76) (138.4.31.17, executor driver, partition 0, NODE_LOCAL, 12941 bytes)
[2025-07-10T12:14:41.391+0200] {subprocess.py:93} INFO - 25/07/10 12:14:41 INFO Executor: Running task 0.0 in stage 37.0 (TID 76)
[2025-07-10T12:14:41.398+0200] {subprocess.py:93} INFO - 25/07/10 12:14:41 INFO ShuffleBlockFetcherIterator: Getting 2 (1220.4 KiB) non-empty blocks including 2 (1220.4 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2025-07-10T12:14:41.398+0200] {subprocess.py:93} INFO - 25/07/10 12:14:41 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2025-07-10T12:14:41.405+0200] {subprocess.py:93} INFO - 25/07/10 12:14:41 INFO CodeGenerator: Code generated in 8.07058 ms
[2025-07-10T12:14:41.695+0200] {subprocess.py:93} INFO - 25/07/10 12:14:41 INFO Executor: Finished task 0.0 in stage 37.0 (TID 76). 4763 bytes result sent to driver
[2025-07-10T12:14:41.696+0200] {subprocess.py:93} INFO - 25/07/10 12:14:41 INFO TaskSetManager: Finished task 0.0 in stage 37.0 (TID 76) in 297 ms on 138.4.31.17 (executor driver) (1/1)
[2025-07-10T12:14:41.696+0200] {subprocess.py:93} INFO - 25/07/10 12:14:41 INFO TaskSchedulerImpl: Removed TaskSet 37.0, whose tasks have all completed, from pool
[2025-07-10T12:14:41.696+0200] {subprocess.py:93} INFO - 25/07/10 12:14:41 INFO DAGScheduler: ShuffleMapStage 37 (collect at StringIndexer.scala:204) finished in 0,305 s
[2025-07-10T12:14:41.696+0200] {subprocess.py:93} INFO - 25/07/10 12:14:41 INFO DAGScheduler: looking for newly runnable stages
[2025-07-10T12:14:41.696+0200] {subprocess.py:93} INFO - 25/07/10 12:14:41 INFO DAGScheduler: running: Set()
[2025-07-10T12:14:41.696+0200] {subprocess.py:93} INFO - 25/07/10 12:14:41 INFO DAGScheduler: waiting: Set()
[2025-07-10T12:14:41.696+0200] {subprocess.py:93} INFO - 25/07/10 12:14:41 INFO DAGScheduler: failed: Set()
[2025-07-10T12:14:41.713+0200] {subprocess.py:93} INFO - 25/07/10 12:14:41 INFO SparkContext: Starting job: collect at StringIndexer.scala:204
[2025-07-10T12:14:41.713+0200] {subprocess.py:93} INFO - 25/07/10 12:14:41 INFO DAGScheduler: Got job 25 (collect at StringIndexer.scala:204) with 1 output partitions
[2025-07-10T12:14:41.713+0200] {subprocess.py:93} INFO - 25/07/10 12:14:41 INFO DAGScheduler: Final stage: ResultStage 40 (collect at StringIndexer.scala:204)
[2025-07-10T12:14:41.713+0200] {subprocess.py:93} INFO - 25/07/10 12:14:41 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 39)
[2025-07-10T12:14:41.713+0200] {subprocess.py:93} INFO - 25/07/10 12:14:41 INFO DAGScheduler: Missing parents: List()
[2025-07-10T12:14:41.713+0200] {subprocess.py:93} INFO - 25/07/10 12:14:41 INFO DAGScheduler: Submitting ResultStage 40 (MapPartitionsRDD[79] at collect at StringIndexer.scala:204), which has no missing parents
[2025-07-10T12:14:41.714+0200] {subprocess.py:93} INFO - 25/07/10 12:14:41 INFO MemoryStore: Block broadcast_29 stored as values in memory (estimated size 39.6 KiB, free 433.2 MiB)
[2025-07-10T12:14:41.715+0200] {subprocess.py:93} INFO - 25/07/10 12:14:41 INFO MemoryStore: Block broadcast_29_piece0 stored as bytes in memory (estimated size 18.8 KiB, free 433.2 MiB)
[2025-07-10T12:14:41.716+0200] {subprocess.py:93} INFO - 25/07/10 12:14:41 INFO BlockManagerInfo: Added broadcast_29_piece0 in memory on 138.4.31.17:32855 (size: 18.8 KiB, free: 434.2 MiB)
[2025-07-10T12:14:41.728+0200] {subprocess.py:93} INFO - 25/07/10 12:14:41 INFO SparkContext: Created broadcast 29 from broadcast at DAGScheduler.scala:1585
[2025-07-10T12:14:41.728+0200] {subprocess.py:93} INFO - 25/07/10 12:14:41 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 40 (MapPartitionsRDD[79] at collect at StringIndexer.scala:204) (first 15 tasks are for partitions Vector(0))
[2025-07-10T12:14:41.728+0200] {subprocess.py:93} INFO - 25/07/10 12:14:41 INFO TaskSchedulerImpl: Adding task set 40.0 with 1 tasks resource profile 0
[2025-07-10T12:14:41.730+0200] {subprocess.py:93} INFO - 25/07/10 12:14:41 INFO TaskSetManager: Starting task 0.0 in stage 40.0 (TID 77) (138.4.31.17, executor driver, partition 0, NODE_LOCAL, 12952 bytes)
[2025-07-10T12:14:41.730+0200] {subprocess.py:93} INFO - 25/07/10 12:14:41 INFO Executor: Running task 0.0 in stage 40.0 (TID 77)
[2025-07-10T12:14:41.737+0200] {subprocess.py:93} INFO - 25/07/10 12:14:41 INFO ShuffleBlockFetcherIterator: Getting 1 (46.5 KiB) non-empty blocks including 1 (46.5 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2025-07-10T12:14:41.737+0200] {subprocess.py:93} INFO - 25/07/10 12:14:41 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2025-07-10T12:14:41.802+0200] {subprocess.py:93} INFO - 25/07/10 12:14:41 INFO Executor: Finished task 0.0 in stage 40.0 (TID 77). 51642 bytes result sent to driver
[2025-07-10T12:14:41.805+0200] {subprocess.py:93} INFO - 25/07/10 12:14:41 INFO TaskSetManager: Finished task 0.0 in stage 40.0 (TID 77) in 73 ms on 138.4.31.17 (executor driver) (1/1)
[2025-07-10T12:14:41.806+0200] {subprocess.py:93} INFO - 25/07/10 12:14:41 INFO TaskSchedulerImpl: Removed TaskSet 40.0, whose tasks have all completed, from pool
[2025-07-10T12:14:41.806+0200] {subprocess.py:93} INFO - 25/07/10 12:14:41 INFO DAGScheduler: ResultStage 40 (collect at StringIndexer.scala:204) finished in 0,091 s
[2025-07-10T12:14:41.806+0200] {subprocess.py:93} INFO - 25/07/10 12:14:41 INFO DAGScheduler: Job 25 is finished. Cancelling potential speculative or zombie tasks for this job
[2025-07-10T12:14:41.806+0200] {subprocess.py:93} INFO - 25/07/10 12:14:41 INFO TaskSchedulerImpl: Killing all running tasks in stage 40: Stage finished
[2025-07-10T12:14:41.806+0200] {subprocess.py:93} INFO - 25/07/10 12:14:41 INFO DAGScheduler: Job 25 finished: collect at StringIndexer.scala:204, took 0,093748 s
[2025-07-10T12:14:41.909+0200] {subprocess.py:93} INFO - 25/07/10 12:14:41 INFO FileSystemOverwrite: Path /home/monica.fernandez/practica_creativa/models/string_indexer_model_Route.bin already exists. It will be overwritten.
[2025-07-10T12:14:41.941+0200] {subprocess.py:93} INFO - 25/07/10 12:14:41 INFO HadoopMapRedCommitProtocol: Using output committer class org.apache.hadoop.mapred.FileOutputCommitter
[2025-07-10T12:14:41.942+0200] {subprocess.py:93} INFO - 25/07/10 12:14:41 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2025-07-10T12:14:41.942+0200] {subprocess.py:93} INFO - 25/07/10 12:14:41 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2025-07-10T12:14:42.074+0200] {subprocess.py:93} INFO - 25/07/10 12:14:42 INFO SparkContext: Starting job: runJob at SparkHadoopWriter.scala:83
[2025-07-10T12:14:42.076+0200] {subprocess.py:93} INFO - 25/07/10 12:14:42 INFO DAGScheduler: Got job 26 (runJob at SparkHadoopWriter.scala:83) with 1 output partitions
[2025-07-10T12:14:42.078+0200] {subprocess.py:93} INFO - 25/07/10 12:14:42 INFO DAGScheduler: Final stage: ResultStage 41 (runJob at SparkHadoopWriter.scala:83)
[2025-07-10T12:14:42.078+0200] {subprocess.py:93} INFO - 25/07/10 12:14:42 INFO DAGScheduler: Parents of final stage: List()
[2025-07-10T12:14:42.078+0200] {subprocess.py:93} INFO - 25/07/10 12:14:42 INFO DAGScheduler: Missing parents: List()
[2025-07-10T12:14:42.078+0200] {subprocess.py:93} INFO - 25/07/10 12:14:42 INFO DAGScheduler: Submitting ResultStage 41 (MapPartitionsRDD[81] at saveAsTextFile at ReadWrite.scala:413), which has no missing parents
[2025-07-10T12:14:42.089+0200] {subprocess.py:93} INFO - 25/07/10 12:14:42 INFO MemoryStore: Block broadcast_30 stored as values in memory (estimated size 101.3 KiB, free 433.1 MiB)
[2025-07-10T12:14:42.093+0200] {subprocess.py:93} INFO - 25/07/10 12:14:42 INFO MemoryStore: Block broadcast_30_piece0 stored as bytes in memory (estimated size 36.5 KiB, free 433.0 MiB)
[2025-07-10T12:14:42.094+0200] {subprocess.py:93} INFO - 25/07/10 12:14:42 INFO BlockManagerInfo: Added broadcast_30_piece0 in memory on 138.4.31.17:32855 (size: 36.5 KiB, free: 434.1 MiB)
[2025-07-10T12:14:42.095+0200] {subprocess.py:93} INFO - 25/07/10 12:14:42 INFO SparkContext: Created broadcast 30 from broadcast at DAGScheduler.scala:1585
[2025-07-10T12:14:42.096+0200] {subprocess.py:93} INFO - 25/07/10 12:14:42 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 41 (MapPartitionsRDD[81] at saveAsTextFile at ReadWrite.scala:413) (first 15 tasks are for partitions Vector(0))
[2025-07-10T12:14:42.096+0200] {subprocess.py:93} INFO - 25/07/10 12:14:42 INFO TaskSchedulerImpl: Adding task set 41.0 with 1 tasks resource profile 0
[2025-07-10T12:14:42.097+0200] {subprocess.py:93} INFO - 25/07/10 12:14:42 INFO TaskSetManager: Starting task 0.0 in stage 41.0 (TID 78) (138.4.31.17, executor driver, partition 0, PROCESS_LOCAL, 13278 bytes)
[2025-07-10T12:14:42.101+0200] {subprocess.py:93} INFO - 25/07/10 12:14:42 INFO Executor: Running task 0.0 in stage 41.0 (TID 78)
[2025-07-10T12:14:42.106+0200] {subprocess.py:93} INFO - 25/07/10 12:14:42 INFO HadoopMapRedCommitProtocol: Using output committer class org.apache.hadoop.mapred.FileOutputCommitter
[2025-07-10T12:14:42.106+0200] {subprocess.py:93} INFO - 25/07/10 12:14:42 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2025-07-10T12:14:42.106+0200] {subprocess.py:93} INFO - 25/07/10 12:14:42 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2025-07-10T12:14:42.189+0200] {subprocess.py:93} INFO - 25/07/10 12:14:42 INFO FileOutputCommitter: Saved output of task 'attempt_202507101214417849374628307048009_0081_m_000000_0' to file:/home/monica.fernandez/practica_creativa/models/string_indexer_model_Route.bin/metadata/_temporary/0/task_202507101214417849374628307048009_0081_m_000000
[2025-07-10T12:14:42.190+0200] {subprocess.py:93} INFO - 25/07/10 12:14:42 INFO SparkHadoopMapRedUtil: attempt_202507101214417849374628307048009_0081_m_000000_0: Committed. Elapsed time: 3 ms.
[2025-07-10T12:14:42.190+0200] {subprocess.py:93} INFO - 25/07/10 12:14:42 INFO Executor: Finished task 0.0 in stage 41.0 (TID 78). 1170 bytes result sent to driver
[2025-07-10T12:14:42.191+0200] {subprocess.py:93} INFO - 25/07/10 12:14:42 INFO TaskSetManager: Finished task 0.0 in stage 41.0 (TID 78) in 95 ms on 138.4.31.17 (executor driver) (1/1)
[2025-07-10T12:14:42.191+0200] {subprocess.py:93} INFO - 25/07/10 12:14:42 INFO TaskSchedulerImpl: Removed TaskSet 41.0, whose tasks have all completed, from pool
[2025-07-10T12:14:42.191+0200] {subprocess.py:93} INFO - 25/07/10 12:14:42 INFO DAGScheduler: ResultStage 41 (runJob at SparkHadoopWriter.scala:83) finished in 0,116 s
[2025-07-10T12:14:42.192+0200] {subprocess.py:93} INFO - 25/07/10 12:14:42 INFO DAGScheduler: Job 26 is finished. Cancelling potential speculative or zombie tasks for this job
[2025-07-10T12:14:42.192+0200] {subprocess.py:93} INFO - 25/07/10 12:14:42 INFO TaskSchedulerImpl: Killing all running tasks in stage 41: Stage finished
[2025-07-10T12:14:42.192+0200] {subprocess.py:93} INFO - 25/07/10 12:14:42 INFO DAGScheduler: Job 26 finished: runJob at SparkHadoopWriter.scala:83, took 0,117295 s
[2025-07-10T12:14:42.192+0200] {subprocess.py:93} INFO - 25/07/10 12:14:42 INFO SparkHadoopWriter: Start to commit write Job job_202507101214417849374628307048009_0081.
[2025-07-10T12:14:42.247+0200] {subprocess.py:93} INFO - 25/07/10 12:14:42 INFO SparkHadoopWriter: Write Job job_202507101214417849374628307048009_0081 committed. Elapsed time: 54 ms.
[2025-07-10T12:14:42.329+0200] {subprocess.py:93} INFO - 25/07/10 12:14:42 INFO DAGScheduler: Registering RDD 84 (parquet at StringIndexer.scala:499) as input to shuffle 15
[2025-07-10T12:14:42.329+0200] {subprocess.py:93} INFO - 25/07/10 12:14:42 INFO DAGScheduler: Got map stage job 27 (parquet at StringIndexer.scala:499) with 1 output partitions
[2025-07-10T12:14:42.329+0200] {subprocess.py:93} INFO - 25/07/10 12:14:42 INFO DAGScheduler: Final stage: ShuffleMapStage 42 (parquet at StringIndexer.scala:499)
[2025-07-10T12:14:42.330+0200] {subprocess.py:93} INFO - 25/07/10 12:14:42 INFO DAGScheduler: Parents of final stage: List()
[2025-07-10T12:14:42.330+0200] {subprocess.py:93} INFO - 25/07/10 12:14:42 INFO DAGScheduler: Missing parents: List()
[2025-07-10T12:14:42.330+0200] {subprocess.py:93} INFO - 25/07/10 12:14:42 INFO DAGScheduler: Submitting ShuffleMapStage 42 (MapPartitionsRDD[84] at parquet at StringIndexer.scala:499), which has no missing parents
[2025-07-10T12:14:42.331+0200] {subprocess.py:93} INFO - 25/07/10 12:14:42 INFO MemoryStore: Block broadcast_31 stored as values in memory (estimated size 8.1 KiB, free 433.0 MiB)
[2025-07-10T12:14:42.341+0200] {subprocess.py:93} INFO - 25/07/10 12:14:42 INFO BlockManagerInfo: Removed broadcast_30_piece0 on 138.4.31.17:32855 in memory (size: 36.5 KiB, free: 434.2 MiB)
[2025-07-10T12:14:42.342+0200] {subprocess.py:93} INFO - 25/07/10 12:14:42 INFO MemoryStore: Block broadcast_31_piece0 stored as bytes in memory (estimated size 4.4 KiB, free 433.1 MiB)
[2025-07-10T12:14:42.343+0200] {subprocess.py:93} INFO - 25/07/10 12:14:42 INFO BlockManagerInfo: Added broadcast_31_piece0 in memory on 138.4.31.17:32855 (size: 4.4 KiB, free: 434.2 MiB)
[2025-07-10T12:14:42.343+0200] {subprocess.py:93} INFO - 25/07/10 12:14:42 INFO SparkContext: Created broadcast 31 from broadcast at DAGScheduler.scala:1585
[2025-07-10T12:14:42.344+0200] {subprocess.py:93} INFO - 25/07/10 12:14:42 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 42 (MapPartitionsRDD[84] at parquet at StringIndexer.scala:499) (first 15 tasks are for partitions Vector(0))
[2025-07-10T12:14:42.344+0200] {subprocess.py:93} INFO - 25/07/10 12:14:42 INFO TaskSchedulerImpl: Adding task set 42.0 with 1 tasks resource profile 0
[2025-07-10T12:14:42.345+0200] {subprocess.py:93} INFO - 25/07/10 12:14:42 INFO TaskSetManager: Starting task 0.0 in stage 42.0 (TID 79) (138.4.31.17, executor driver, partition 0, PROCESS_LOCAL, 80641 bytes)
[2025-07-10T12:14:42.347+0200] {subprocess.py:93} INFO - 25/07/10 12:14:42 INFO Executor: Running task 0.0 in stage 42.0 (TID 79)
[2025-07-10T12:14:42.351+0200] {subprocess.py:93} INFO - 25/07/10 12:14:42 INFO Executor: Finished task 0.0 in stage 42.0 (TID 79). 1628 bytes result sent to driver
[2025-07-10T12:14:42.352+0200] {subprocess.py:93} INFO - 25/07/10 12:14:42 INFO TaskSetManager: Finished task 0.0 in stage 42.0 (TID 79) in 7 ms on 138.4.31.17 (executor driver) (1/1)
[2025-07-10T12:14:42.353+0200] {subprocess.py:93} INFO - 25/07/10 12:14:42 INFO TaskSchedulerImpl: Removed TaskSet 42.0, whose tasks have all completed, from pool
[2025-07-10T12:14:42.353+0200] {subprocess.py:93} INFO - 25/07/10 12:14:42 INFO DAGScheduler: ShuffleMapStage 42 (parquet at StringIndexer.scala:499) finished in 0,022 s
[2025-07-10T12:14:42.353+0200] {subprocess.py:93} INFO - 25/07/10 12:14:42 INFO DAGScheduler: looking for newly runnable stages
[2025-07-10T12:14:42.353+0200] {subprocess.py:93} INFO - 25/07/10 12:14:42 INFO DAGScheduler: running: Set()
[2025-07-10T12:14:42.353+0200] {subprocess.py:93} INFO - 25/07/10 12:14:42 INFO DAGScheduler: waiting: Set()
[2025-07-10T12:14:42.353+0200] {subprocess.py:93} INFO - 25/07/10 12:14:42 INFO DAGScheduler: failed: Set()
[2025-07-10T12:14:42.361+0200] {subprocess.py:93} INFO - 25/07/10 12:14:42 INFO ParquetUtils: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
[2025-07-10T12:14:42.362+0200] {subprocess.py:93} INFO - 25/07/10 12:14:42 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2025-07-10T12:14:42.363+0200] {subprocess.py:93} INFO - 25/07/10 12:14:42 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2025-07-10T12:14:42.363+0200] {subprocess.py:93} INFO - 25/07/10 12:14:42 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[2025-07-10T12:14:42.363+0200] {subprocess.py:93} INFO - 25/07/10 12:14:42 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2025-07-10T12:14:42.364+0200] {subprocess.py:93} INFO - 25/07/10 12:14:42 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2025-07-10T12:14:42.364+0200] {subprocess.py:93} INFO - 25/07/10 12:14:42 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[2025-07-10T12:14:42.365+0200] {subprocess.py:93} INFO - 25/07/10 12:14:42 INFO BlockManagerInfo: Removed broadcast_28_piece0 on 138.4.31.17:32855 in memory (size: 18.7 KiB, free: 434.2 MiB)
[2025-07-10T12:14:42.374+0200] {subprocess.py:93} INFO - 25/07/10 12:14:42 INFO BlockManagerInfo: Removed broadcast_29_piece0 on 138.4.31.17:32855 in memory (size: 18.8 KiB, free: 434.2 MiB)
[2025-07-10T12:14:42.391+0200] {subprocess.py:93} INFO - 25/07/10 12:14:42 INFO SparkContext: Starting job: parquet at StringIndexer.scala:499
[2025-07-10T12:14:42.396+0200] {subprocess.py:93} INFO - 25/07/10 12:14:42 INFO DAGScheduler: Got job 28 (parquet at StringIndexer.scala:499) with 1 output partitions
[2025-07-10T12:14:42.396+0200] {subprocess.py:93} INFO - 25/07/10 12:14:42 INFO DAGScheduler: Final stage: ResultStage 44 (parquet at StringIndexer.scala:499)
[2025-07-10T12:14:42.396+0200] {subprocess.py:93} INFO - 25/07/10 12:14:42 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 43)
[2025-07-10T12:14:42.396+0200] {subprocess.py:93} INFO - 25/07/10 12:14:42 INFO DAGScheduler: Missing parents: List()
[2025-07-10T12:14:42.396+0200] {subprocess.py:93} INFO - 25/07/10 12:14:42 INFO DAGScheduler: Submitting ResultStage 44 (MapPartitionsRDD[86] at parquet at StringIndexer.scala:499), which has no missing parents
[2025-07-10T12:14:42.412+0200] {subprocess.py:93} INFO - 25/07/10 12:14:42 INFO MemoryStore: Block broadcast_32 stored as values in memory (estimated size 230.2 KiB, free 433.1 MiB)
[2025-07-10T12:14:42.414+0200] {subprocess.py:93} INFO - 25/07/10 12:14:42 INFO MemoryStore: Block broadcast_32_piece0 stored as bytes in memory (estimated size 81.0 KiB, free 433.0 MiB)
[2025-07-10T12:14:42.415+0200] {subprocess.py:93} INFO - 25/07/10 12:14:42 INFO BlockManagerInfo: Added broadcast_32_piece0 in memory on 138.4.31.17:32855 (size: 81.0 KiB, free: 434.1 MiB)
[2025-07-10T12:14:42.416+0200] {subprocess.py:93} INFO - 25/07/10 12:14:42 INFO SparkContext: Created broadcast 32 from broadcast at DAGScheduler.scala:1585
[2025-07-10T12:14:42.417+0200] {subprocess.py:93} INFO - 25/07/10 12:14:42 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 44 (MapPartitionsRDD[86] at parquet at StringIndexer.scala:499) (first 15 tasks are for partitions Vector(0))
[2025-07-10T12:14:42.417+0200] {subprocess.py:93} INFO - 25/07/10 12:14:42 INFO TaskSchedulerImpl: Adding task set 44.0 with 1 tasks resource profile 0
[2025-07-10T12:14:42.419+0200] {subprocess.py:93} INFO - 25/07/10 12:14:42 INFO TaskSetManager: Starting task 0.0 in stage 44.0 (TID 80) (138.4.31.17, executor driver, partition 0, NODE_LOCAL, 12952 bytes)
[2025-07-10T12:14:42.420+0200] {subprocess.py:93} INFO - 25/07/10 12:14:42 INFO Executor: Running task 0.0 in stage 44.0 (TID 80)
[2025-07-10T12:14:42.434+0200] {subprocess.py:93} INFO - 25/07/10 12:14:42 INFO ShuffleBlockFetcherIterator: Getting 1 (42.2 KiB) non-empty blocks including 1 (42.2 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2025-07-10T12:14:42.434+0200] {subprocess.py:93} INFO - 25/07/10 12:14:42 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2025-07-10T12:14:42.435+0200] {subprocess.py:93} INFO - 25/07/10 12:14:42 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2025-07-10T12:14:42.435+0200] {subprocess.py:93} INFO - 25/07/10 12:14:42 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2025-07-10T12:14:42.435+0200] {subprocess.py:93} INFO - 25/07/10 12:14:42 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[2025-07-10T12:14:42.435+0200] {subprocess.py:93} INFO - 25/07/10 12:14:42 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2025-07-10T12:14:42.435+0200] {subprocess.py:93} INFO - 25/07/10 12:14:42 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2025-07-10T12:14:42.435+0200] {subprocess.py:93} INFO - 25/07/10 12:14:42 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[2025-07-10T12:14:42.435+0200] {subprocess.py:93} INFO - 25/07/10 12:14:42 INFO CodecConfig: Compression: SNAPPY
[2025-07-10T12:14:42.436+0200] {subprocess.py:93} INFO - 25/07/10 12:14:42 INFO CodecConfig: Compression: SNAPPY
[2025-07-10T12:14:42.437+0200] {subprocess.py:93} INFO - 25/07/10 12:14:42 INFO ParquetOutputFormat: ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]
[2025-07-10T12:14:42.438+0200] {subprocess.py:93} INFO - 25/07/10 12:14:42 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
[2025-07-10T12:14:42.439+0200] {subprocess.py:93} INFO - {
[2025-07-10T12:14:42.439+0200] {subprocess.py:93} INFO -   "type" : "struct",
[2025-07-10T12:14:42.439+0200] {subprocess.py:93} INFO -   "fields" : [ {
[2025-07-10T12:14:42.439+0200] {subprocess.py:93} INFO -     "name" : "labelsArray",
[2025-07-10T12:14:42.439+0200] {subprocess.py:93} INFO -     "type" : {
[2025-07-10T12:14:42.439+0200] {subprocess.py:93} INFO -       "type" : "array",
[2025-07-10T12:14:42.439+0200] {subprocess.py:93} INFO -       "elementType" : {
[2025-07-10T12:14:42.439+0200] {subprocess.py:93} INFO -         "type" : "array",
[2025-07-10T12:14:42.439+0200] {subprocess.py:93} INFO -         "elementType" : "string",
[2025-07-10T12:14:42.439+0200] {subprocess.py:93} INFO -         "containsNull" : true
[2025-07-10T12:14:42.439+0200] {subprocess.py:93} INFO -       },
[2025-07-10T12:14:42.440+0200] {subprocess.py:93} INFO -       "containsNull" : true
[2025-07-10T12:14:42.440+0200] {subprocess.py:93} INFO -     },
[2025-07-10T12:14:42.440+0200] {subprocess.py:93} INFO -     "nullable" : true,
[2025-07-10T12:14:42.440+0200] {subprocess.py:93} INFO -     "metadata" : { }
[2025-07-10T12:14:42.440+0200] {subprocess.py:93} INFO -   } ]
[2025-07-10T12:14:42.440+0200] {subprocess.py:93} INFO - }
[2025-07-10T12:14:42.440+0200] {subprocess.py:93} INFO - and corresponding Parquet message type:
[2025-07-10T12:14:42.440+0200] {subprocess.py:93} INFO - message spark_schema {
[2025-07-10T12:14:42.441+0200] {subprocess.py:93} INFO -   optional group labelsArray (LIST) {
[2025-07-10T12:14:42.441+0200] {subprocess.py:93} INFO -     repeated group list {
[2025-07-10T12:14:42.441+0200] {subprocess.py:93} INFO -       optional group element (LIST) {
[2025-07-10T12:14:42.441+0200] {subprocess.py:93} INFO -         repeated group list {
[2025-07-10T12:14:42.441+0200] {subprocess.py:93} INFO -           optional binary element (STRING);
[2025-07-10T12:14:42.441+0200] {subprocess.py:93} INFO -         }
[2025-07-10T12:14:42.441+0200] {subprocess.py:93} INFO -       }
[2025-07-10T12:14:42.441+0200] {subprocess.py:93} INFO -     }
[2025-07-10T12:14:42.441+0200] {subprocess.py:93} INFO -   }
[2025-07-10T12:14:42.441+0200] {subprocess.py:93} INFO - }
[2025-07-10T12:14:42.441+0200] {subprocess.py:93} INFO - 
[2025-07-10T12:14:42.441+0200] {subprocess.py:93} INFO - 
[2025-07-10T12:14:42.548+0200] {subprocess.py:93} INFO - 25/07/10 12:14:42 INFO FileOutputCommitter: Saved output of task 'attempt_202507101214424431645786344511243_0044_m_000000_80' to file:/home/monica.fernandez/practica_creativa/models/string_indexer_model_Route.bin/data/_temporary/0/task_202507101214424431645786344511243_0044_m_000000
[2025-07-10T12:14:42.548+0200] {subprocess.py:93} INFO - 25/07/10 12:14:42 INFO SparkHadoopMapRedUtil: attempt_202507101214424431645786344511243_0044_m_000000_80: Committed. Elapsed time: 1 ms.
[2025-07-10T12:14:42.549+0200] {subprocess.py:93} INFO - 25/07/10 12:14:42 INFO Executor: Finished task 0.0 in stage 44.0 (TID 80). 4740 bytes result sent to driver
[2025-07-10T12:14:42.551+0200] {subprocess.py:93} INFO - 25/07/10 12:14:42 INFO TaskSetManager: Finished task 0.0 in stage 44.0 (TID 80) in 133 ms on 138.4.31.17 (executor driver) (1/1)
[2025-07-10T12:14:42.552+0200] {subprocess.py:93} INFO - 25/07/10 12:14:42 INFO TaskSchedulerImpl: Removed TaskSet 44.0, whose tasks have all completed, from pool
[2025-07-10T12:14:42.554+0200] {subprocess.py:93} INFO - 25/07/10 12:14:42 INFO DAGScheduler: ResultStage 44 (parquet at StringIndexer.scala:499) finished in 0,158 s
[2025-07-10T12:14:42.554+0200] {subprocess.py:93} INFO - 25/07/10 12:14:42 INFO DAGScheduler: Job 28 is finished. Cancelling potential speculative or zombie tasks for this job
[2025-07-10T12:14:42.554+0200] {subprocess.py:93} INFO - 25/07/10 12:14:42 INFO TaskSchedulerImpl: Killing all running tasks in stage 44: Stage finished
[2025-07-10T12:14:42.554+0200] {subprocess.py:93} INFO - 25/07/10 12:14:42 INFO DAGScheduler: Job 28 finished: parquet at StringIndexer.scala:499, took 0,162445 s
[2025-07-10T12:14:42.554+0200] {subprocess.py:93} INFO - 25/07/10 12:14:42 INFO FileFormatWriter: Start to commit write Job e34a9f97-fcbc-417e-b4b8-f024c2e4b7b6.
[2025-07-10T12:14:42.586+0200] {subprocess.py:93} INFO - 25/07/10 12:14:42 INFO FileFormatWriter: Write Job e34a9f97-fcbc-417e-b4b8-f024c2e4b7b6 committed. Elapsed time: 32 ms.
[2025-07-10T12:14:42.587+0200] {subprocess.py:93} INFO - 25/07/10 12:14:42 INFO FileFormatWriter: Finished processing stats for write job e34a9f97-fcbc-417e-b4b8-f024c2e4b7b6.
[2025-07-10T12:14:43.008+0200] {subprocess.py:93} INFO - Traceback (most recent call last):
[2025-07-10T12:14:43.008+0200] {subprocess.py:93} INFO -   File "/home/monica.fernandez/practica_creativa/resources/train_spark_mllib_model.py", line 147, in <module>
[2025-07-10T12:14:43.023+0200] {subprocess.py:93} INFO -     main(sys.argv[1] if len(sys.argv) > 1 else ".")
[2025-07-10T12:14:43.031+0200] {subprocess.py:93} INFO -   File "/home/monica.fernandez/practica_creativa/resources/train_spark_mllib_model.py", line 97, in main
[2025-07-10T12:14:43.032+0200] {subprocess.py:93} INFO -     final_vectorized_features = vector_assembler.transform(ml_bucketized_features)
[2025-07-10T12:14:43.033+0200] {subprocess.py:93} INFO -                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2025-07-10T12:14:43.033+0200] {subprocess.py:93} INFO -   File "/home/monica.fernandez/.sdkman/candidates/spark/current/python/lib/pyspark.zip/pyspark/ml/base.py", line 262, in transform
[2025-07-10T12:14:43.036+0200] {subprocess.py:93} INFO -   File "/home/monica.fernandez/.sdkman/candidates/spark/current/python/lib/pyspark.zip/pyspark/ml/wrapper.py", line 398, in _transform
[2025-07-10T12:14:43.041+0200] {subprocess.py:93} INFO -   File "/home/monica.fernandez/.sdkman/candidates/spark/current/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py", line 1322, in __call__
[2025-07-10T12:14:43.043+0200] {subprocess.py:93} INFO -   File "/home/monica.fernandez/.sdkman/candidates/spark/current/python/lib/pyspark.zip/pyspark/errors/exceptions/captured.py", line 185, in deco
[2025-07-10T12:14:43.053+0200] {subprocess.py:93} INFO - pyspark.errors.exceptions.captured.AnalysisException: [AMBIGUOUS_REFERENCE] Reference `Distance` is ambiguous, could be: [`Distance`, `Distance`].
[2025-07-10T12:14:44.228+0200] {subprocess.py:93} INFO - 25/07/10 12:14:44 INFO SparkContext: Invoking stop() from shutdown hook
[2025-07-10T12:14:44.229+0200] {subprocess.py:93} INFO - 25/07/10 12:14:44 INFO SparkContext: SparkContext is stopping with exitCode 0.
[2025-07-10T12:14:44.257+0200] {subprocess.py:93} INFO - 25/07/10 12:14:44 INFO CassandraConnector: Disconnected from Cassandra cluster.
[2025-07-10T12:14:44.257+0200] {subprocess.py:93} INFO - 25/07/10 12:14:44 INFO SerialShutdownHooks: Successfully executed shutdown hook: Clearing session cache for C* connector
[2025-07-10T12:14:44.261+0200] {subprocess.py:93} INFO - 25/07/10 12:14:44 INFO SparkUI: Stopped Spark web UI at http://138.4.31.17:4041
[2025-07-10T12:14:44.278+0200] {subprocess.py:93} INFO - 25/07/10 12:14:44 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
[2025-07-10T12:14:44.322+0200] {subprocess.py:93} INFO - 25/07/10 12:14:44 INFO MemoryStore: MemoryStore cleared
[2025-07-10T12:14:44.323+0200] {subprocess.py:93} INFO - 25/07/10 12:14:44 INFO BlockManager: BlockManager stopped
[2025-07-10T12:14:44.338+0200] {subprocess.py:93} INFO - 25/07/10 12:14:44 INFO BlockManagerMaster: BlockManagerMaster stopped
[2025-07-10T12:14:44.345+0200] {subprocess.py:93} INFO - 25/07/10 12:14:44 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
[2025-07-10T12:14:44.357+0200] {subprocess.py:93} INFO - 25/07/10 12:14:44 INFO SparkContext: Successfully stopped SparkContext
[2025-07-10T12:14:44.357+0200] {subprocess.py:93} INFO - 25/07/10 12:14:44 INFO ShutdownHookManager: Shutdown hook called
[2025-07-10T12:14:44.358+0200] {subprocess.py:93} INFO - 25/07/10 12:14:44 INFO ShutdownHookManager: Deleting directory /tmp/spark-64875627-1bdb-48c4-b3bf-1e4a10abda6f
[2025-07-10T12:14:44.362+0200] {subprocess.py:93} INFO - 25/07/10 12:14:44 INFO ShutdownHookManager: Deleting directory /tmp/spark-bad50add-078f-4dc5-a391-3566358df450
[2025-07-10T12:14:44.368+0200] {subprocess.py:93} INFO - 25/07/10 12:14:44 INFO ShutdownHookManager: Deleting directory /tmp/spark-64875627-1bdb-48c4-b3bf-1e4a10abda6f/pyspark-8542906e-5221-4a45-8ec4-370b964e5ebb
[2025-07-10T12:14:44.596+0200] {subprocess.py:97} INFO - Command exited with return code 1
[2025-07-10T12:14:44.615+0200] {taskinstance.py:1768} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/monica.fernandez/practica_creativa/venv-airflow/lib/python3.11/site-packages/airflow/operators/bash.py", line 196, in execute
    raise AirflowException(
airflow.exceptions.AirflowException: Bash command failed. The command returned a non-zero exit code 1.
[2025-07-10T12:14:44.624+0200] {taskinstance.py:1318} INFO - Marking task as UP_FOR_RETRY. dag_id=agile_data_science_batch_prediction_model_training, task_id=pyspark_train_classifier_model, execution_date=20250710T100245, start_date=20250710T101342, end_date=20250710T101444
[2025-07-10T12:14:44.649+0200] {standard_task_runner.py:100} ERROR - Failed to execute job 129 for task pyspark_train_classifier_model (Bash command failed. The command returned a non-zero exit code 1.; 129308)
[2025-07-10T12:14:44.662+0200] {local_task_job.py:208} INFO - Task exited with return code 1
[2025-07-10T12:14:44.692+0200] {taskinstance.py:2578} INFO - 0 downstream tasks scheduled from follow-on schedule check
