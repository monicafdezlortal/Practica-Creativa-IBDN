[2025-07-11T11:45:14.681+0200] {taskinstance.py:1083} INFO - Dependencies all met for <TaskInstance: agile_data_science_batch_prediction_model_training.pyspark_train_classifier_model manual__2025-07-11T09:45:08.846620+00:00 [queued]>
[2025-07-11T11:45:14.700+0200] {taskinstance.py:1083} INFO - Dependencies all met for <TaskInstance: agile_data_science_batch_prediction_model_training.pyspark_train_classifier_model manual__2025-07-11T09:45:08.846620+00:00 [queued]>
[2025-07-11T11:45:14.700+0200] {taskinstance.py:1279} INFO - 
--------------------------------------------------------------------------------
[2025-07-11T11:45:14.700+0200] {taskinstance.py:1280} INFO - Starting attempt 1 of 4
[2025-07-11T11:45:14.700+0200] {taskinstance.py:1281} INFO - 
--------------------------------------------------------------------------------
[2025-07-11T11:45:14.722+0200] {taskinstance.py:1300} INFO - Executing <Task(BashOperator): pyspark_train_classifier_model> on 2025-07-11 09:45:08.846620+00:00
[2025-07-11T11:45:14.725+0200] {standard_task_runner.py:55} INFO - Started process 88248 to run task
[2025-07-11T11:45:14.728+0200] {standard_task_runner.py:82} INFO - Running: ['airflow', 'tasks', 'run', 'agile_data_science_batch_prediction_model_training', 'pyspark_train_classifier_model', 'manual__2025-07-11T09:45:08.846620+00:00', '--job-id', '137', '--raw', '--subdir', 'DAGS_FOLDER/setup.py', '--cfg-path', '/tmp/tmpo5l67sk5']
[2025-07-11T11:45:14.730+0200] {standard_task_runner.py:83} INFO - Job 137: Subtask pyspark_train_classifier_model
[2025-07-11T11:45:14.820+0200] {task_command.py:388} INFO - Running <TaskInstance: agile_data_science_batch_prediction_model_training.pyspark_train_classifier_model manual__2025-07-11T09:45:08.846620+00:00 [running]> on host l014.lab.dit.upm.es
[2025-07-11T11:45:14.915+0200] {taskinstance.py:1507} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=airflow
AIRFLOW_CTX_DAG_ID=agile_data_science_batch_prediction_model_training
AIRFLOW_CTX_TASK_ID=pyspark_train_classifier_model
AIRFLOW_CTX_EXECUTION_DATE=2025-07-11T09:45:08.846620+00:00
AIRFLOW_CTX_TRY_NUMBER=1
AIRFLOW_CTX_DAG_RUN_ID=manual__2025-07-11T09:45:08.846620+00:00
[2025-07-11T11:45:14.917+0200] {subprocess.py:63} INFO - Tmp dir root location: 
 /tmp
[2025-07-11T11:45:14.917+0200] {subprocess.py:75} INFO - Running command: ['/usr/bin/bash', '-c', '\nsource /home/monica.fernandez/practica_creativa/venv-airflow/bin/activate && export PYSPARK_PYTHON=/home/monica.fernandez/practica_creativa/venv-airflow/bin/python && spark-submit --master local[4]   --packages com.datastax.spark:spark-cassandra-connector_2.12:3.5.0   /home/monica.fernandez/practica_creativa/resources/train_spark_mllib_model.py   /home/monica.fernandez/practica_creativa']
[2025-07-11T11:45:14.924+0200] {subprocess.py:86} INFO - Output:
[2025-07-11T11:45:16.865+0200] {subprocess.py:93} INFO - 25/07/11 11:45:16 WARN Utils: Your hostname, l014 resolves to a loopback address: 127.0.1.1; using 138.4.31.14 instead (on interface enp1s0)
[2025-07-11T11:45:16.868+0200] {subprocess.py:93} INFO - 25/07/11 11:45:16 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
[2025-07-11T11:45:17.035+0200] {subprocess.py:93} INFO - :: loading settings :: url = jar:file:/home/monica.fernandez/.sdkman/candidates/spark/3.5.3/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml
[2025-07-11T11:45:17.097+0200] {subprocess.py:93} INFO - Ivy Default Cache set to: /home/monica.fernandez/.ivy2/cache
[2025-07-11T11:45:17.098+0200] {subprocess.py:93} INFO - The jars for the packages stored in: /home/monica.fernandez/.ivy2/jars
[2025-07-11T11:45:17.104+0200] {subprocess.py:93} INFO - com.datastax.spark#spark-cassandra-connector_2.12 added as a dependency
[2025-07-11T11:45:17.108+0200] {subprocess.py:93} INFO - :: resolving dependencies :: org.apache.spark#spark-submit-parent-12375087-49da-4bc4-9457-9beaadbfcabf;1.0
[2025-07-11T11:45:17.109+0200] {subprocess.py:93} INFO - 	confs: [default]
[2025-07-11T11:45:17.266+0200] {subprocess.py:93} INFO - 	found com.datastax.spark#spark-cassandra-connector_2.12;3.5.0 in central
[2025-07-11T11:45:17.312+0200] {subprocess.py:93} INFO - 	found com.datastax.spark#spark-cassandra-connector-driver_2.12;3.5.0 in central
[2025-07-11T11:45:17.352+0200] {subprocess.py:93} INFO - 	found org.scala-lang.modules#scala-collection-compat_2.12;2.11.0 in central
[2025-07-11T11:45:17.433+0200] {subprocess.py:93} INFO - 	found com.datastax.oss#java-driver-core-shaded;4.13.0 in central
[2025-07-11T11:45:17.474+0200] {subprocess.py:93} INFO - 	found com.datastax.oss#native-protocol;1.5.0 in central
[2025-07-11T11:45:17.527+0200] {subprocess.py:93} INFO - 	found com.datastax.oss#java-driver-shaded-guava;25.1-jre-graal-sub-1 in central
[2025-07-11T11:45:17.574+0200] {subprocess.py:93} INFO - 	found com.typesafe#config;1.4.1 in central
[2025-07-11T11:45:17.610+0200] {subprocess.py:93} INFO - 	found org.slf4j#slf4j-api;1.7.26 in central
[2025-07-11T11:45:17.646+0200] {subprocess.py:93} INFO - 	found io.dropwizard.metrics#metrics-core;4.1.18 in central
[2025-07-11T11:45:17.686+0200] {subprocess.py:93} INFO - 	found org.hdrhistogram#HdrHistogram;2.1.12 in central
[2025-07-11T11:45:17.713+0200] {subprocess.py:93} INFO - 	found org.reactivestreams#reactive-streams;1.0.3 in central
[2025-07-11T11:45:17.748+0200] {subprocess.py:93} INFO - 	found com.github.stephenc.jcip#jcip-annotations;1.0-1 in central
[2025-07-11T11:45:17.782+0200] {subprocess.py:93} INFO - 	found com.github.spotbugs#spotbugs-annotations;3.1.12 in central
[2025-07-11T11:45:17.817+0200] {subprocess.py:93} INFO - 	found com.google.code.findbugs#jsr305;3.0.2 in central
[2025-07-11T11:45:17.846+0200] {subprocess.py:93} INFO - 	found com.datastax.oss#java-driver-mapper-runtime;4.13.0 in central
[2025-07-11T11:45:17.868+0200] {subprocess.py:93} INFO - 	found com.datastax.oss#java-driver-query-builder;4.13.0 in central
[2025-07-11T11:45:17.898+0200] {subprocess.py:93} INFO - 	found org.apache.commons#commons-lang3;3.10 in central
[2025-07-11T11:45:17.918+0200] {subprocess.py:93} INFO - 	found com.thoughtworks.paranamer#paranamer;2.8 in central
[2025-07-11T11:45:17.934+0200] {subprocess.py:93} INFO - 	found org.scala-lang#scala-reflect;2.12.11 in central
[2025-07-11T11:45:17.997+0200] {subprocess.py:93} INFO - :: resolution report :: resolve 843ms :: artifacts dl 46ms
[2025-07-11T11:45:17.997+0200] {subprocess.py:93} INFO - 	:: modules in use:
[2025-07-11T11:45:17.998+0200] {subprocess.py:93} INFO - 	com.datastax.oss#java-driver-core-shaded;4.13.0 from central in [default]
[2025-07-11T11:45:17.998+0200] {subprocess.py:93} INFO - 	com.datastax.oss#java-driver-mapper-runtime;4.13.0 from central in [default]
[2025-07-11T11:45:17.998+0200] {subprocess.py:93} INFO - 	com.datastax.oss#java-driver-query-builder;4.13.0 from central in [default]
[2025-07-11T11:45:17.998+0200] {subprocess.py:93} INFO - 	com.datastax.oss#java-driver-shaded-guava;25.1-jre-graal-sub-1 from central in [default]
[2025-07-11T11:45:17.999+0200] {subprocess.py:93} INFO - 	com.datastax.oss#native-protocol;1.5.0 from central in [default]
[2025-07-11T11:45:17.999+0200] {subprocess.py:93} INFO - 	com.datastax.spark#spark-cassandra-connector-driver_2.12;3.5.0 from central in [default]
[2025-07-11T11:45:17.999+0200] {subprocess.py:93} INFO - 	com.datastax.spark#spark-cassandra-connector_2.12;3.5.0 from central in [default]
[2025-07-11T11:45:17.999+0200] {subprocess.py:93} INFO - 	com.github.spotbugs#spotbugs-annotations;3.1.12 from central in [default]
[2025-07-11T11:45:17.999+0200] {subprocess.py:93} INFO - 	com.github.stephenc.jcip#jcip-annotations;1.0-1 from central in [default]
[2025-07-11T11:45:17.999+0200] {subprocess.py:93} INFO - 	com.google.code.findbugs#jsr305;3.0.2 from central in [default]
[2025-07-11T11:45:17.999+0200] {subprocess.py:93} INFO - 	com.thoughtworks.paranamer#paranamer;2.8 from central in [default]
[2025-07-11T11:45:17.999+0200] {subprocess.py:93} INFO - 	com.typesafe#config;1.4.1 from central in [default]
[2025-07-11T11:45:17.999+0200] {subprocess.py:93} INFO - 	io.dropwizard.metrics#metrics-core;4.1.18 from central in [default]
[2025-07-11T11:45:17.999+0200] {subprocess.py:93} INFO - 	org.apache.commons#commons-lang3;3.10 from central in [default]
[2025-07-11T11:45:17.999+0200] {subprocess.py:93} INFO - 	org.hdrhistogram#HdrHistogram;2.1.12 from central in [default]
[2025-07-11T11:45:17.999+0200] {subprocess.py:93} INFO - 	org.reactivestreams#reactive-streams;1.0.3 from central in [default]
[2025-07-11T11:45:17.999+0200] {subprocess.py:93} INFO - 	org.scala-lang#scala-reflect;2.12.11 from central in [default]
[2025-07-11T11:45:17.999+0200] {subprocess.py:93} INFO - 	org.scala-lang.modules#scala-collection-compat_2.12;2.11.0 from central in [default]
[2025-07-11T11:45:17.999+0200] {subprocess.py:93} INFO - 	org.slf4j#slf4j-api;1.7.26 from central in [default]
[2025-07-11T11:45:17.999+0200] {subprocess.py:93} INFO - 	---------------------------------------------------------------------
[2025-07-11T11:45:18.000+0200] {subprocess.py:93} INFO - 	|                  |            modules            ||   artifacts   |
[2025-07-11T11:45:18.000+0200] {subprocess.py:93} INFO - 	|       conf       | number| search|dwnlded|evicted|| number|dwnlded|
[2025-07-11T11:45:18.000+0200] {subprocess.py:93} INFO - 	---------------------------------------------------------------------
[2025-07-11T11:45:18.000+0200] {subprocess.py:93} INFO - 	|      default     |   19  |   0   |   0   |   0   ||   19  |   0   |
[2025-07-11T11:45:18.000+0200] {subprocess.py:93} INFO - 	---------------------------------------------------------------------
[2025-07-11T11:45:18.008+0200] {subprocess.py:93} INFO - :: retrieving :: org.apache.spark#spark-submit-parent-12375087-49da-4bc4-9457-9beaadbfcabf
[2025-07-11T11:45:18.008+0200] {subprocess.py:93} INFO - 	confs: [default]
[2025-07-11T11:45:18.030+0200] {subprocess.py:93} INFO - 	0 artifacts copied, 19 already retrieved (0kB/22ms)
[2025-07-11T11:45:18.203+0200] {subprocess.py:93} INFO - 25/07/11 11:45:18 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2025-07-11T11:45:22.450+0200] {subprocess.py:93} INFO - 25/07/11 11:45:22 INFO SparkContext: Running Spark version 3.5.3
[2025-07-11T11:45:22.451+0200] {subprocess.py:93} INFO - 25/07/11 11:45:22 INFO SparkContext: OS info Linux, 6.1.0-35-amd64, amd64
[2025-07-11T11:45:22.452+0200] {subprocess.py:93} INFO - 25/07/11 11:45:22 INFO SparkContext: Java version 17.0.14
[2025-07-11T11:45:22.505+0200] {subprocess.py:93} INFO - 25/07/11 11:45:22 INFO ResourceUtils: ==============================================================
[2025-07-11T11:45:22.509+0200] {subprocess.py:93} INFO - 25/07/11 11:45:22 INFO ResourceUtils: No custom resources configured for spark.driver.
[2025-07-11T11:45:22.510+0200] {subprocess.py:93} INFO - 25/07/11 11:45:22 INFO ResourceUtils: ==============================================================
[2025-07-11T11:45:22.511+0200] {subprocess.py:93} INFO - 25/07/11 11:45:22 INFO SparkContext: Submitted application: train_spark_mllib_model.py
[2025-07-11T11:45:22.550+0200] {subprocess.py:93} INFO - 25/07/11 11:45:22 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
[2025-07-11T11:45:22.566+0200] {subprocess.py:93} INFO - 25/07/11 11:45:22 INFO ResourceProfile: Limiting resource is cpu
[2025-07-11T11:45:22.568+0200] {subprocess.py:93} INFO - 25/07/11 11:45:22 INFO ResourceProfileManager: Added ResourceProfile id: 0
[2025-07-11T11:45:22.655+0200] {subprocess.py:93} INFO - 25/07/11 11:45:22 INFO SecurityManager: Changing view acls to: monica.fernandez
[2025-07-11T11:45:22.656+0200] {subprocess.py:93} INFO - 25/07/11 11:45:22 INFO SecurityManager: Changing modify acls to: monica.fernandez
[2025-07-11T11:45:22.657+0200] {subprocess.py:93} INFO - 25/07/11 11:45:22 INFO SecurityManager: Changing view acls groups to:
[2025-07-11T11:45:22.658+0200] {subprocess.py:93} INFO - 25/07/11 11:45:22 INFO SecurityManager: Changing modify acls groups to:
[2025-07-11T11:45:22.659+0200] {subprocess.py:93} INFO - 25/07/11 11:45:22 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: monica.fernandez; groups with view permissions: EMPTY; users with modify permissions: monica.fernandez; groups with modify permissions: EMPTY
[2025-07-11T11:45:22.901+0200] {subprocess.py:93} INFO - 25/07/11 11:45:22 INFO Utils: Successfully started service 'sparkDriver' on port 34489.
[2025-07-11T11:45:22.938+0200] {subprocess.py:93} INFO - 25/07/11 11:45:22 INFO SparkEnv: Registering MapOutputTracker
[2025-07-11T11:45:22.974+0200] {subprocess.py:93} INFO - 25/07/11 11:45:22 INFO SparkEnv: Registering BlockManagerMaster
[2025-07-11T11:45:22.992+0200] {subprocess.py:93} INFO - 25/07/11 11:45:22 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[2025-07-11T11:45:22.993+0200] {subprocess.py:93} INFO - 25/07/11 11:45:22 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
[2025-07-11T11:45:22.996+0200] {subprocess.py:93} INFO - 25/07/11 11:45:22 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
[2025-07-11T11:45:23.015+0200] {subprocess.py:93} INFO - 25/07/11 11:45:23 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-984341e7-8ba9-4a2f-9103-0d165246e03b
[2025-07-11T11:45:23.028+0200] {subprocess.py:93} INFO - 25/07/11 11:45:23 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB
[2025-07-11T11:45:23.041+0200] {subprocess.py:93} INFO - 25/07/11 11:45:23 INFO SparkEnv: Registering OutputCommitCoordinator
[2025-07-11T11:45:23.153+0200] {subprocess.py:93} INFO - 25/07/11 11:45:23 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI
[2025-07-11T11:45:23.210+0200] {subprocess.py:93} INFO - 25/07/11 11:45:23 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
[2025-07-11T11:45:23.236+0200] {subprocess.py:93} INFO - 25/07/11 11:45:23 INFO Utils: Successfully started service 'SparkUI' on port 4041.
[2025-07-11T11:45:23.280+0200] {subprocess.py:93} INFO - 25/07/11 11:45:23 INFO SparkContext: Added JAR file:///home/monica.fernandez/.ivy2/jars/com.datastax.spark_spark-cassandra-connector_2.12-3.5.0.jar at spark://138.4.31.14:34489/jars/com.datastax.spark_spark-cassandra-connector_2.12-3.5.0.jar with timestamp 1752227122439
[2025-07-11T11:45:23.280+0200] {subprocess.py:93} INFO - 25/07/11 11:45:23 INFO SparkContext: Added JAR file:///home/monica.fernandez/.ivy2/jars/com.datastax.spark_spark-cassandra-connector-driver_2.12-3.5.0.jar at spark://138.4.31.14:34489/jars/com.datastax.spark_spark-cassandra-connector-driver_2.12-3.5.0.jar with timestamp 1752227122439
[2025-07-11T11:45:23.281+0200] {subprocess.py:93} INFO - 25/07/11 11:45:23 INFO SparkContext: Added JAR file:///home/monica.fernandez/.ivy2/jars/org.scala-lang.modules_scala-collection-compat_2.12-2.11.0.jar at spark://138.4.31.14:34489/jars/org.scala-lang.modules_scala-collection-compat_2.12-2.11.0.jar with timestamp 1752227122439
[2025-07-11T11:45:23.281+0200] {subprocess.py:93} INFO - 25/07/11 11:45:23 INFO SparkContext: Added JAR file:///home/monica.fernandez/.ivy2/jars/com.datastax.oss_java-driver-core-shaded-4.13.0.jar at spark://138.4.31.14:34489/jars/com.datastax.oss_java-driver-core-shaded-4.13.0.jar with timestamp 1752227122439
[2025-07-11T11:45:23.281+0200] {subprocess.py:93} INFO - 25/07/11 11:45:23 INFO SparkContext: Added JAR file:///home/monica.fernandez/.ivy2/jars/com.datastax.oss_java-driver-mapper-runtime-4.13.0.jar at spark://138.4.31.14:34489/jars/com.datastax.oss_java-driver-mapper-runtime-4.13.0.jar with timestamp 1752227122439
[2025-07-11T11:45:23.281+0200] {subprocess.py:93} INFO - 25/07/11 11:45:23 INFO SparkContext: Added JAR file:///home/monica.fernandez/.ivy2/jars/org.apache.commons_commons-lang3-3.10.jar at spark://138.4.31.14:34489/jars/org.apache.commons_commons-lang3-3.10.jar with timestamp 1752227122439
[2025-07-11T11:45:23.281+0200] {subprocess.py:93} INFO - 25/07/11 11:45:23 INFO SparkContext: Added JAR file:///home/monica.fernandez/.ivy2/jars/com.thoughtworks.paranamer_paranamer-2.8.jar at spark://138.4.31.14:34489/jars/com.thoughtworks.paranamer_paranamer-2.8.jar with timestamp 1752227122439
[2025-07-11T11:45:23.282+0200] {subprocess.py:93} INFO - 25/07/11 11:45:23 INFO SparkContext: Added JAR file:///home/monica.fernandez/.ivy2/jars/org.scala-lang_scala-reflect-2.12.11.jar at spark://138.4.31.14:34489/jars/org.scala-lang_scala-reflect-2.12.11.jar with timestamp 1752227122439
[2025-07-11T11:45:23.282+0200] {subprocess.py:93} INFO - 25/07/11 11:45:23 INFO SparkContext: Added JAR file:///home/monica.fernandez/.ivy2/jars/com.datastax.oss_native-protocol-1.5.0.jar at spark://138.4.31.14:34489/jars/com.datastax.oss_native-protocol-1.5.0.jar with timestamp 1752227122439
[2025-07-11T11:45:23.282+0200] {subprocess.py:93} INFO - 25/07/11 11:45:23 INFO SparkContext: Added JAR file:///home/monica.fernandez/.ivy2/jars/com.datastax.oss_java-driver-shaded-guava-25.1-jre-graal-sub-1.jar at spark://138.4.31.14:34489/jars/com.datastax.oss_java-driver-shaded-guava-25.1-jre-graal-sub-1.jar with timestamp 1752227122439
[2025-07-11T11:45:23.283+0200] {subprocess.py:93} INFO - 25/07/11 11:45:23 INFO SparkContext: Added JAR file:///home/monica.fernandez/.ivy2/jars/com.typesafe_config-1.4.1.jar at spark://138.4.31.14:34489/jars/com.typesafe_config-1.4.1.jar with timestamp 1752227122439
[2025-07-11T11:45:23.284+0200] {subprocess.py:93} INFO - 25/07/11 11:45:23 INFO SparkContext: Added JAR file:///home/monica.fernandez/.ivy2/jars/org.slf4j_slf4j-api-1.7.26.jar at spark://138.4.31.14:34489/jars/org.slf4j_slf4j-api-1.7.26.jar with timestamp 1752227122439
[2025-07-11T11:45:23.284+0200] {subprocess.py:93} INFO - 25/07/11 11:45:23 INFO SparkContext: Added JAR file:///home/monica.fernandez/.ivy2/jars/io.dropwizard.metrics_metrics-core-4.1.18.jar at spark://138.4.31.14:34489/jars/io.dropwizard.metrics_metrics-core-4.1.18.jar with timestamp 1752227122439
[2025-07-11T11:45:23.285+0200] {subprocess.py:93} INFO - 25/07/11 11:45:23 INFO SparkContext: Added JAR file:///home/monica.fernandez/.ivy2/jars/org.hdrhistogram_HdrHistogram-2.1.12.jar at spark://138.4.31.14:34489/jars/org.hdrhistogram_HdrHistogram-2.1.12.jar with timestamp 1752227122439
[2025-07-11T11:45:23.285+0200] {subprocess.py:93} INFO - 25/07/11 11:45:23 INFO SparkContext: Added JAR file:///home/monica.fernandez/.ivy2/jars/org.reactivestreams_reactive-streams-1.0.3.jar at spark://138.4.31.14:34489/jars/org.reactivestreams_reactive-streams-1.0.3.jar with timestamp 1752227122439
[2025-07-11T11:45:23.286+0200] {subprocess.py:93} INFO - 25/07/11 11:45:23 INFO SparkContext: Added JAR file:///home/monica.fernandez/.ivy2/jars/com.github.stephenc.jcip_jcip-annotations-1.0-1.jar at spark://138.4.31.14:34489/jars/com.github.stephenc.jcip_jcip-annotations-1.0-1.jar with timestamp 1752227122439
[2025-07-11T11:45:23.286+0200] {subprocess.py:93} INFO - 25/07/11 11:45:23 INFO SparkContext: Added JAR file:///home/monica.fernandez/.ivy2/jars/com.github.spotbugs_spotbugs-annotations-3.1.12.jar at spark://138.4.31.14:34489/jars/com.github.spotbugs_spotbugs-annotations-3.1.12.jar with timestamp 1752227122439
[2025-07-11T11:45:23.286+0200] {subprocess.py:93} INFO - 25/07/11 11:45:23 INFO SparkContext: Added JAR file:///home/monica.fernandez/.ivy2/jars/com.google.code.findbugs_jsr305-3.0.2.jar at spark://138.4.31.14:34489/jars/com.google.code.findbugs_jsr305-3.0.2.jar with timestamp 1752227122439
[2025-07-11T11:45:23.287+0200] {subprocess.py:93} INFO - 25/07/11 11:45:23 INFO SparkContext: Added JAR file:///home/monica.fernandez/.ivy2/jars/com.datastax.oss_java-driver-query-builder-4.13.0.jar at spark://138.4.31.14:34489/jars/com.datastax.oss_java-driver-query-builder-4.13.0.jar with timestamp 1752227122439
[2025-07-11T11:45:23.290+0200] {subprocess.py:93} INFO - 25/07/11 11:45:23 INFO SparkContext: Added file file:///home/monica.fernandez/.ivy2/jars/com.datastax.spark_spark-cassandra-connector_2.12-3.5.0.jar at file:///home/monica.fernandez/.ivy2/jars/com.datastax.spark_spark-cassandra-connector_2.12-3.5.0.jar with timestamp 1752227122439
[2025-07-11T11:45:23.292+0200] {subprocess.py:93} INFO - 25/07/11 11:45:23 INFO Utils: Copying /home/monica.fernandez/.ivy2/jars/com.datastax.spark_spark-cassandra-connector_2.12-3.5.0.jar to /tmp/spark-36b5bbfe-8223-445b-b376-dec57e3c841e/userFiles-61cee7c1-3f19-4f58-b026-1fdb3ca1e002/com.datastax.spark_spark-cassandra-connector_2.12-3.5.0.jar
[2025-07-11T11:45:23.316+0200] {subprocess.py:93} INFO - 25/07/11 11:45:23 INFO SparkContext: Added file file:///home/monica.fernandez/.ivy2/jars/com.datastax.spark_spark-cassandra-connector-driver_2.12-3.5.0.jar at file:///home/monica.fernandez/.ivy2/jars/com.datastax.spark_spark-cassandra-connector-driver_2.12-3.5.0.jar with timestamp 1752227122439
[2025-07-11T11:45:23.319+0200] {subprocess.py:93} INFO - 25/07/11 11:45:23 INFO Utils: Copying /home/monica.fernandez/.ivy2/jars/com.datastax.spark_spark-cassandra-connector-driver_2.12-3.5.0.jar to /tmp/spark-36b5bbfe-8223-445b-b376-dec57e3c841e/userFiles-61cee7c1-3f19-4f58-b026-1fdb3ca1e002/com.datastax.spark_spark-cassandra-connector-driver_2.12-3.5.0.jar
[2025-07-11T11:45:23.324+0200] {subprocess.py:93} INFO - 25/07/11 11:45:23 INFO SparkContext: Added file file:///home/monica.fernandez/.ivy2/jars/org.scala-lang.modules_scala-collection-compat_2.12-2.11.0.jar at file:///home/monica.fernandez/.ivy2/jars/org.scala-lang.modules_scala-collection-compat_2.12-2.11.0.jar with timestamp 1752227122439
[2025-07-11T11:45:23.324+0200] {subprocess.py:93} INFO - 25/07/11 11:45:23 INFO Utils: Copying /home/monica.fernandez/.ivy2/jars/org.scala-lang.modules_scala-collection-compat_2.12-2.11.0.jar to /tmp/spark-36b5bbfe-8223-445b-b376-dec57e3c841e/userFiles-61cee7c1-3f19-4f58-b026-1fdb3ca1e002/org.scala-lang.modules_scala-collection-compat_2.12-2.11.0.jar
[2025-07-11T11:45:23.329+0200] {subprocess.py:93} INFO - 25/07/11 11:45:23 INFO SparkContext: Added file file:///home/monica.fernandez/.ivy2/jars/com.datastax.oss_java-driver-core-shaded-4.13.0.jar at file:///home/monica.fernandez/.ivy2/jars/com.datastax.oss_java-driver-core-shaded-4.13.0.jar with timestamp 1752227122439
[2025-07-11T11:45:23.330+0200] {subprocess.py:93} INFO - 25/07/11 11:45:23 INFO Utils: Copying /home/monica.fernandez/.ivy2/jars/com.datastax.oss_java-driver-core-shaded-4.13.0.jar to /tmp/spark-36b5bbfe-8223-445b-b376-dec57e3c841e/userFiles-61cee7c1-3f19-4f58-b026-1fdb3ca1e002/com.datastax.oss_java-driver-core-shaded-4.13.0.jar
[2025-07-11T11:45:23.344+0200] {subprocess.py:93} INFO - 25/07/11 11:45:23 INFO SparkContext: Added file file:///home/monica.fernandez/.ivy2/jars/com.datastax.oss_java-driver-mapper-runtime-4.13.0.jar at file:///home/monica.fernandez/.ivy2/jars/com.datastax.oss_java-driver-mapper-runtime-4.13.0.jar with timestamp 1752227122439
[2025-07-11T11:45:23.345+0200] {subprocess.py:93} INFO - 25/07/11 11:45:23 INFO Utils: Copying /home/monica.fernandez/.ivy2/jars/com.datastax.oss_java-driver-mapper-runtime-4.13.0.jar to /tmp/spark-36b5bbfe-8223-445b-b376-dec57e3c841e/userFiles-61cee7c1-3f19-4f58-b026-1fdb3ca1e002/com.datastax.oss_java-driver-mapper-runtime-4.13.0.jar
[2025-07-11T11:45:23.349+0200] {subprocess.py:93} INFO - 25/07/11 11:45:23 INFO SparkContext: Added file file:///home/monica.fernandez/.ivy2/jars/org.apache.commons_commons-lang3-3.10.jar at file:///home/monica.fernandez/.ivy2/jars/org.apache.commons_commons-lang3-3.10.jar with timestamp 1752227122439
[2025-07-11T11:45:23.349+0200] {subprocess.py:93} INFO - 25/07/11 11:45:23 INFO Utils: Copying /home/monica.fernandez/.ivy2/jars/org.apache.commons_commons-lang3-3.10.jar to /tmp/spark-36b5bbfe-8223-445b-b376-dec57e3c841e/userFiles-61cee7c1-3f19-4f58-b026-1fdb3ca1e002/org.apache.commons_commons-lang3-3.10.jar
[2025-07-11T11:45:23.361+0200] {subprocess.py:93} INFO - 25/07/11 11:45:23 INFO SparkContext: Added file file:///home/monica.fernandez/.ivy2/jars/com.thoughtworks.paranamer_paranamer-2.8.jar at file:///home/monica.fernandez/.ivy2/jars/com.thoughtworks.paranamer_paranamer-2.8.jar with timestamp 1752227122439
[2025-07-11T11:45:23.361+0200] {subprocess.py:93} INFO - 25/07/11 11:45:23 INFO Utils: Copying /home/monica.fernandez/.ivy2/jars/com.thoughtworks.paranamer_paranamer-2.8.jar to /tmp/spark-36b5bbfe-8223-445b-b376-dec57e3c841e/userFiles-61cee7c1-3f19-4f58-b026-1fdb3ca1e002/com.thoughtworks.paranamer_paranamer-2.8.jar
[2025-07-11T11:45:23.366+0200] {subprocess.py:93} INFO - 25/07/11 11:45:23 INFO SparkContext: Added file file:///home/monica.fernandez/.ivy2/jars/org.scala-lang_scala-reflect-2.12.11.jar at file:///home/monica.fernandez/.ivy2/jars/org.scala-lang_scala-reflect-2.12.11.jar with timestamp 1752227122439
[2025-07-11T11:45:23.366+0200] {subprocess.py:93} INFO - 25/07/11 11:45:23 INFO Utils: Copying /home/monica.fernandez/.ivy2/jars/org.scala-lang_scala-reflect-2.12.11.jar to /tmp/spark-36b5bbfe-8223-445b-b376-dec57e3c841e/userFiles-61cee7c1-3f19-4f58-b026-1fdb3ca1e002/org.scala-lang_scala-reflect-2.12.11.jar
[2025-07-11T11:45:23.373+0200] {subprocess.py:93} INFO - 25/07/11 11:45:23 INFO SparkContext: Added file file:///home/monica.fernandez/.ivy2/jars/com.datastax.oss_native-protocol-1.5.0.jar at file:///home/monica.fernandez/.ivy2/jars/com.datastax.oss_native-protocol-1.5.0.jar with timestamp 1752227122439
[2025-07-11T11:45:23.373+0200] {subprocess.py:93} INFO - 25/07/11 11:45:23 INFO Utils: Copying /home/monica.fernandez/.ivy2/jars/com.datastax.oss_native-protocol-1.5.0.jar to /tmp/spark-36b5bbfe-8223-445b-b376-dec57e3c841e/userFiles-61cee7c1-3f19-4f58-b026-1fdb3ca1e002/com.datastax.oss_native-protocol-1.5.0.jar
[2025-07-11T11:45:23.387+0200] {subprocess.py:93} INFO - 25/07/11 11:45:23 INFO SparkContext: Added file file:///home/monica.fernandez/.ivy2/jars/com.datastax.oss_java-driver-shaded-guava-25.1-jre-graal-sub-1.jar at file:///home/monica.fernandez/.ivy2/jars/com.datastax.oss_java-driver-shaded-guava-25.1-jre-graal-sub-1.jar with timestamp 1752227122439
[2025-07-11T11:45:23.387+0200] {subprocess.py:93} INFO - 25/07/11 11:45:23 INFO Utils: Copying /home/monica.fernandez/.ivy2/jars/com.datastax.oss_java-driver-shaded-guava-25.1-jre-graal-sub-1.jar to /tmp/spark-36b5bbfe-8223-445b-b376-dec57e3c841e/userFiles-61cee7c1-3f19-4f58-b026-1fdb3ca1e002/com.datastax.oss_java-driver-shaded-guava-25.1-jre-graal-sub-1.jar
[2025-07-11T11:45:23.394+0200] {subprocess.py:93} INFO - 25/07/11 11:45:23 INFO SparkContext: Added file file:///home/monica.fernandez/.ivy2/jars/com.typesafe_config-1.4.1.jar at file:///home/monica.fernandez/.ivy2/jars/com.typesafe_config-1.4.1.jar with timestamp 1752227122439
[2025-07-11T11:45:23.395+0200] {subprocess.py:93} INFO - 25/07/11 11:45:23 INFO Utils: Copying /home/monica.fernandez/.ivy2/jars/com.typesafe_config-1.4.1.jar to /tmp/spark-36b5bbfe-8223-445b-b376-dec57e3c841e/userFiles-61cee7c1-3f19-4f58-b026-1fdb3ca1e002/com.typesafe_config-1.4.1.jar
[2025-07-11T11:45:23.399+0200] {subprocess.py:93} INFO - 25/07/11 11:45:23 INFO SparkContext: Added file file:///home/monica.fernandez/.ivy2/jars/org.slf4j_slf4j-api-1.7.26.jar at file:///home/monica.fernandez/.ivy2/jars/org.slf4j_slf4j-api-1.7.26.jar with timestamp 1752227122439
[2025-07-11T11:45:23.400+0200] {subprocess.py:93} INFO - 25/07/11 11:45:23 INFO Utils: Copying /home/monica.fernandez/.ivy2/jars/org.slf4j_slf4j-api-1.7.26.jar to /tmp/spark-36b5bbfe-8223-445b-b376-dec57e3c841e/userFiles-61cee7c1-3f19-4f58-b026-1fdb3ca1e002/org.slf4j_slf4j-api-1.7.26.jar
[2025-07-11T11:45:23.408+0200] {subprocess.py:93} INFO - 25/07/11 11:45:23 INFO SparkContext: Added file file:///home/monica.fernandez/.ivy2/jars/io.dropwizard.metrics_metrics-core-4.1.18.jar at file:///home/monica.fernandez/.ivy2/jars/io.dropwizard.metrics_metrics-core-4.1.18.jar with timestamp 1752227122439
[2025-07-11T11:45:23.408+0200] {subprocess.py:93} INFO - 25/07/11 11:45:23 INFO Utils: Copying /home/monica.fernandez/.ivy2/jars/io.dropwizard.metrics_metrics-core-4.1.18.jar to /tmp/spark-36b5bbfe-8223-445b-b376-dec57e3c841e/userFiles-61cee7c1-3f19-4f58-b026-1fdb3ca1e002/io.dropwizard.metrics_metrics-core-4.1.18.jar
[2025-07-11T11:45:23.412+0200] {subprocess.py:93} INFO - 25/07/11 11:45:23 INFO SparkContext: Added file file:///home/monica.fernandez/.ivy2/jars/org.hdrhistogram_HdrHistogram-2.1.12.jar at file:///home/monica.fernandez/.ivy2/jars/org.hdrhistogram_HdrHistogram-2.1.12.jar with timestamp 1752227122439
[2025-07-11T11:45:23.413+0200] {subprocess.py:93} INFO - 25/07/11 11:45:23 INFO Utils: Copying /home/monica.fernandez/.ivy2/jars/org.hdrhistogram_HdrHistogram-2.1.12.jar to /tmp/spark-36b5bbfe-8223-445b-b376-dec57e3c841e/userFiles-61cee7c1-3f19-4f58-b026-1fdb3ca1e002/org.hdrhistogram_HdrHistogram-2.1.12.jar
[2025-07-11T11:45:23.425+0200] {subprocess.py:93} INFO - 25/07/11 11:45:23 INFO SparkContext: Added file file:///home/monica.fernandez/.ivy2/jars/org.reactivestreams_reactive-streams-1.0.3.jar at file:///home/monica.fernandez/.ivy2/jars/org.reactivestreams_reactive-streams-1.0.3.jar with timestamp 1752227122439
[2025-07-11T11:45:23.425+0200] {subprocess.py:93} INFO - 25/07/11 11:45:23 INFO Utils: Copying /home/monica.fernandez/.ivy2/jars/org.reactivestreams_reactive-streams-1.0.3.jar to /tmp/spark-36b5bbfe-8223-445b-b376-dec57e3c841e/userFiles-61cee7c1-3f19-4f58-b026-1fdb3ca1e002/org.reactivestreams_reactive-streams-1.0.3.jar
[2025-07-11T11:45:23.437+0200] {subprocess.py:93} INFO - 25/07/11 11:45:23 INFO SparkContext: Added file file:///home/monica.fernandez/.ivy2/jars/com.github.stephenc.jcip_jcip-annotations-1.0-1.jar at file:///home/monica.fernandez/.ivy2/jars/com.github.stephenc.jcip_jcip-annotations-1.0-1.jar with timestamp 1752227122439
[2025-07-11T11:45:23.437+0200] {subprocess.py:93} INFO - 25/07/11 11:45:23 INFO Utils: Copying /home/monica.fernandez/.ivy2/jars/com.github.stephenc.jcip_jcip-annotations-1.0-1.jar to /tmp/spark-36b5bbfe-8223-445b-b376-dec57e3c841e/userFiles-61cee7c1-3f19-4f58-b026-1fdb3ca1e002/com.github.stephenc.jcip_jcip-annotations-1.0-1.jar
[2025-07-11T11:45:23.447+0200] {subprocess.py:93} INFO - 25/07/11 11:45:23 INFO SparkContext: Added file file:///home/monica.fernandez/.ivy2/jars/com.github.spotbugs_spotbugs-annotations-3.1.12.jar at file:///home/monica.fernandez/.ivy2/jars/com.github.spotbugs_spotbugs-annotations-3.1.12.jar with timestamp 1752227122439
[2025-07-11T11:45:23.448+0200] {subprocess.py:93} INFO - 25/07/11 11:45:23 INFO Utils: Copying /home/monica.fernandez/.ivy2/jars/com.github.spotbugs_spotbugs-annotations-3.1.12.jar to /tmp/spark-36b5bbfe-8223-445b-b376-dec57e3c841e/userFiles-61cee7c1-3f19-4f58-b026-1fdb3ca1e002/com.github.spotbugs_spotbugs-annotations-3.1.12.jar
[2025-07-11T11:45:23.451+0200] {subprocess.py:93} INFO - 25/07/11 11:45:23 INFO SparkContext: Added file file:///home/monica.fernandez/.ivy2/jars/com.google.code.findbugs_jsr305-3.0.2.jar at file:///home/monica.fernandez/.ivy2/jars/com.google.code.findbugs_jsr305-3.0.2.jar with timestamp 1752227122439
[2025-07-11T11:45:23.451+0200] {subprocess.py:93} INFO - 25/07/11 11:45:23 INFO Utils: Copying /home/monica.fernandez/.ivy2/jars/com.google.code.findbugs_jsr305-3.0.2.jar to /tmp/spark-36b5bbfe-8223-445b-b376-dec57e3c841e/userFiles-61cee7c1-3f19-4f58-b026-1fdb3ca1e002/com.google.code.findbugs_jsr305-3.0.2.jar
[2025-07-11T11:45:23.458+0200] {subprocess.py:93} INFO - 25/07/11 11:45:23 INFO SparkContext: Added file file:///home/monica.fernandez/.ivy2/jars/com.datastax.oss_java-driver-query-builder-4.13.0.jar at file:///home/monica.fernandez/.ivy2/jars/com.datastax.oss_java-driver-query-builder-4.13.0.jar with timestamp 1752227122439
[2025-07-11T11:45:23.459+0200] {subprocess.py:93} INFO - 25/07/11 11:45:23 INFO Utils: Copying /home/monica.fernandez/.ivy2/jars/com.datastax.oss_java-driver-query-builder-4.13.0.jar to /tmp/spark-36b5bbfe-8223-445b-b376-dec57e3c841e/userFiles-61cee7c1-3f19-4f58-b026-1fdb3ca1e002/com.datastax.oss_java-driver-query-builder-4.13.0.jar
[2025-07-11T11:45:23.559+0200] {subprocess.py:93} INFO - 25/07/11 11:45:23 INFO Executor: Starting executor ID driver on host 138.4.31.14
[2025-07-11T11:45:23.559+0200] {subprocess.py:93} INFO - 25/07/11 11:45:23 INFO Executor: OS info Linux, 6.1.0-35-amd64, amd64
[2025-07-11T11:45:23.560+0200] {subprocess.py:93} INFO - 25/07/11 11:45:23 INFO Executor: Java version 17.0.14
[2025-07-11T11:45:23.567+0200] {subprocess.py:93} INFO - 25/07/11 11:45:23 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''
[2025-07-11T11:45:23.568+0200] {subprocess.py:93} INFO - 25/07/11 11:45:23 INFO Executor: Created or updated repl class loader org.apache.spark.util.MutableURLClassLoader@63ee76f2 for default.
[2025-07-11T11:45:23.581+0200] {subprocess.py:93} INFO - 25/07/11 11:45:23 INFO Executor: Fetching file:///home/monica.fernandez/.ivy2/jars/com.datastax.spark_spark-cassandra-connector-driver_2.12-3.5.0.jar with timestamp 1752227122439
[2025-07-11T11:45:23.627+0200] {subprocess.py:93} INFO - 25/07/11 11:45:23 INFO Utils: /home/monica.fernandez/.ivy2/jars/com.datastax.spark_spark-cassandra-connector-driver_2.12-3.5.0.jar has been previously copied to /tmp/spark-36b5bbfe-8223-445b-b376-dec57e3c841e/userFiles-61cee7c1-3f19-4f58-b026-1fdb3ca1e002/com.datastax.spark_spark-cassandra-connector-driver_2.12-3.5.0.jar
[2025-07-11T11:45:23.631+0200] {subprocess.py:93} INFO - 25/07/11 11:45:23 INFO Executor: Fetching file:///home/monica.fernandez/.ivy2/jars/com.google.code.findbugs_jsr305-3.0.2.jar with timestamp 1752227122439
[2025-07-11T11:45:23.632+0200] {subprocess.py:93} INFO - 25/07/11 11:45:23 INFO Utils: /home/monica.fernandez/.ivy2/jars/com.google.code.findbugs_jsr305-3.0.2.jar has been previously copied to /tmp/spark-36b5bbfe-8223-445b-b376-dec57e3c841e/userFiles-61cee7c1-3f19-4f58-b026-1fdb3ca1e002/com.google.code.findbugs_jsr305-3.0.2.jar
[2025-07-11T11:45:23.635+0200] {subprocess.py:93} INFO - 25/07/11 11:45:23 INFO Executor: Fetching file:///home/monica.fernandez/.ivy2/jars/com.typesafe_config-1.4.1.jar with timestamp 1752227122439
[2025-07-11T11:45:23.636+0200] {subprocess.py:93} INFO - 25/07/11 11:45:23 INFO Utils: /home/monica.fernandez/.ivy2/jars/com.typesafe_config-1.4.1.jar has been previously copied to /tmp/spark-36b5bbfe-8223-445b-b376-dec57e3c841e/userFiles-61cee7c1-3f19-4f58-b026-1fdb3ca1e002/com.typesafe_config-1.4.1.jar
[2025-07-11T11:45:23.640+0200] {subprocess.py:93} INFO - 25/07/11 11:45:23 INFO Executor: Fetching file:///home/monica.fernandez/.ivy2/jars/com.thoughtworks.paranamer_paranamer-2.8.jar with timestamp 1752227122439
[2025-07-11T11:45:23.641+0200] {subprocess.py:93} INFO - 25/07/11 11:45:23 INFO Utils: /home/monica.fernandez/.ivy2/jars/com.thoughtworks.paranamer_paranamer-2.8.jar has been previously copied to /tmp/spark-36b5bbfe-8223-445b-b376-dec57e3c841e/userFiles-61cee7c1-3f19-4f58-b026-1fdb3ca1e002/com.thoughtworks.paranamer_paranamer-2.8.jar
[2025-07-11T11:45:23.645+0200] {subprocess.py:93} INFO - 25/07/11 11:45:23 INFO Executor: Fetching file:///home/monica.fernandez/.ivy2/jars/org.hdrhistogram_HdrHistogram-2.1.12.jar with timestamp 1752227122439
[2025-07-11T11:45:23.646+0200] {subprocess.py:93} INFO - 25/07/11 11:45:23 INFO Utils: /home/monica.fernandez/.ivy2/jars/org.hdrhistogram_HdrHistogram-2.1.12.jar has been previously copied to /tmp/spark-36b5bbfe-8223-445b-b376-dec57e3c841e/userFiles-61cee7c1-3f19-4f58-b026-1fdb3ca1e002/org.hdrhistogram_HdrHistogram-2.1.12.jar
[2025-07-11T11:45:23.649+0200] {subprocess.py:93} INFO - 25/07/11 11:45:23 INFO Executor: Fetching file:///home/monica.fernandez/.ivy2/jars/com.github.spotbugs_spotbugs-annotations-3.1.12.jar with timestamp 1752227122439
[2025-07-11T11:45:23.650+0200] {subprocess.py:93} INFO - 25/07/11 11:45:23 INFO Utils: /home/monica.fernandez/.ivy2/jars/com.github.spotbugs_spotbugs-annotations-3.1.12.jar has been previously copied to /tmp/spark-36b5bbfe-8223-445b-b376-dec57e3c841e/userFiles-61cee7c1-3f19-4f58-b026-1fdb3ca1e002/com.github.spotbugs_spotbugs-annotations-3.1.12.jar
[2025-07-11T11:45:23.654+0200] {subprocess.py:93} INFO - 25/07/11 11:45:23 INFO Executor: Fetching file:///home/monica.fernandez/.ivy2/jars/org.apache.commons_commons-lang3-3.10.jar with timestamp 1752227122439
[2025-07-11T11:45:23.656+0200] {subprocess.py:93} INFO - 25/07/11 11:45:23 INFO Utils: /home/monica.fernandez/.ivy2/jars/org.apache.commons_commons-lang3-3.10.jar has been previously copied to /tmp/spark-36b5bbfe-8223-445b-b376-dec57e3c841e/userFiles-61cee7c1-3f19-4f58-b026-1fdb3ca1e002/org.apache.commons_commons-lang3-3.10.jar
[2025-07-11T11:45:23.660+0200] {subprocess.py:93} INFO - 25/07/11 11:45:23 INFO Executor: Fetching file:///home/monica.fernandez/.ivy2/jars/io.dropwizard.metrics_metrics-core-4.1.18.jar with timestamp 1752227122439
[2025-07-11T11:45:23.660+0200] {subprocess.py:93} INFO - 25/07/11 11:45:23 INFO Utils: /home/monica.fernandez/.ivy2/jars/io.dropwizard.metrics_metrics-core-4.1.18.jar has been previously copied to /tmp/spark-36b5bbfe-8223-445b-b376-dec57e3c841e/userFiles-61cee7c1-3f19-4f58-b026-1fdb3ca1e002/io.dropwizard.metrics_metrics-core-4.1.18.jar
[2025-07-11T11:45:23.667+0200] {subprocess.py:93} INFO - 25/07/11 11:45:23 INFO Executor: Fetching file:///home/monica.fernandez/.ivy2/jars/com.datastax.oss_java-driver-shaded-guava-25.1-jre-graal-sub-1.jar with timestamp 1752227122439
[2025-07-11T11:45:23.671+0200] {subprocess.py:93} INFO - 25/07/11 11:45:23 INFO Utils: /home/monica.fernandez/.ivy2/jars/com.datastax.oss_java-driver-shaded-guava-25.1-jre-graal-sub-1.jar has been previously copied to /tmp/spark-36b5bbfe-8223-445b-b376-dec57e3c841e/userFiles-61cee7c1-3f19-4f58-b026-1fdb3ca1e002/com.datastax.oss_java-driver-shaded-guava-25.1-jre-graal-sub-1.jar
[2025-07-11T11:45:23.676+0200] {subprocess.py:93} INFO - 25/07/11 11:45:23 INFO Executor: Fetching file:///home/monica.fernandez/.ivy2/jars/com.datastax.oss_java-driver-core-shaded-4.13.0.jar with timestamp 1752227122439
[2025-07-11T11:45:23.685+0200] {subprocess.py:93} INFO - 25/07/11 11:45:23 INFO Utils: /home/monica.fernandez/.ivy2/jars/com.datastax.oss_java-driver-core-shaded-4.13.0.jar has been previously copied to /tmp/spark-36b5bbfe-8223-445b-b376-dec57e3c841e/userFiles-61cee7c1-3f19-4f58-b026-1fdb3ca1e002/com.datastax.oss_java-driver-core-shaded-4.13.0.jar
[2025-07-11T11:45:23.689+0200] {subprocess.py:93} INFO - 25/07/11 11:45:23 INFO Executor: Fetching file:///home/monica.fernandez/.ivy2/jars/com.datastax.oss_native-protocol-1.5.0.jar with timestamp 1752227122439
[2025-07-11T11:45:23.690+0200] {subprocess.py:93} INFO - 25/07/11 11:45:23 INFO Utils: /home/monica.fernandez/.ivy2/jars/com.datastax.oss_native-protocol-1.5.0.jar has been previously copied to /tmp/spark-36b5bbfe-8223-445b-b376-dec57e3c841e/userFiles-61cee7c1-3f19-4f58-b026-1fdb3ca1e002/com.datastax.oss_native-protocol-1.5.0.jar
[2025-07-11T11:45:23.693+0200] {subprocess.py:93} INFO - 25/07/11 11:45:23 INFO Executor: Fetching file:///home/monica.fernandez/.ivy2/jars/org.slf4j_slf4j-api-1.7.26.jar with timestamp 1752227122439
[2025-07-11T11:45:23.694+0200] {subprocess.py:93} INFO - 25/07/11 11:45:23 INFO Utils: /home/monica.fernandez/.ivy2/jars/org.slf4j_slf4j-api-1.7.26.jar has been previously copied to /tmp/spark-36b5bbfe-8223-445b-b376-dec57e3c841e/userFiles-61cee7c1-3f19-4f58-b026-1fdb3ca1e002/org.slf4j_slf4j-api-1.7.26.jar
[2025-07-11T11:45:23.697+0200] {subprocess.py:93} INFO - 25/07/11 11:45:23 INFO Executor: Fetching file:///home/monica.fernandez/.ivy2/jars/org.scala-lang.modules_scala-collection-compat_2.12-2.11.0.jar with timestamp 1752227122439
[2025-07-11T11:45:23.697+0200] {subprocess.py:93} INFO - 25/07/11 11:45:23 INFO Utils: /home/monica.fernandez/.ivy2/jars/org.scala-lang.modules_scala-collection-compat_2.12-2.11.0.jar has been previously copied to /tmp/spark-36b5bbfe-8223-445b-b376-dec57e3c841e/userFiles-61cee7c1-3f19-4f58-b026-1fdb3ca1e002/org.scala-lang.modules_scala-collection-compat_2.12-2.11.0.jar
[2025-07-11T11:45:23.701+0200] {subprocess.py:93} INFO - 25/07/11 11:45:23 INFO Executor: Fetching file:///home/monica.fernandez/.ivy2/jars/com.datastax.spark_spark-cassandra-connector_2.12-3.5.0.jar with timestamp 1752227122439
[2025-07-11T11:45:23.704+0200] {subprocess.py:93} INFO - 25/07/11 11:45:23 INFO Utils: /home/monica.fernandez/.ivy2/jars/com.datastax.spark_spark-cassandra-connector_2.12-3.5.0.jar has been previously copied to /tmp/spark-36b5bbfe-8223-445b-b376-dec57e3c841e/userFiles-61cee7c1-3f19-4f58-b026-1fdb3ca1e002/com.datastax.spark_spark-cassandra-connector_2.12-3.5.0.jar
[2025-07-11T11:45:23.708+0200] {subprocess.py:93} INFO - 25/07/11 11:45:23 INFO Executor: Fetching file:///home/monica.fernandez/.ivy2/jars/com.datastax.oss_java-driver-query-builder-4.13.0.jar with timestamp 1752227122439
[2025-07-11T11:45:23.709+0200] {subprocess.py:93} INFO - 25/07/11 11:45:23 INFO Utils: /home/monica.fernandez/.ivy2/jars/com.datastax.oss_java-driver-query-builder-4.13.0.jar has been previously copied to /tmp/spark-36b5bbfe-8223-445b-b376-dec57e3c841e/userFiles-61cee7c1-3f19-4f58-b026-1fdb3ca1e002/com.datastax.oss_java-driver-query-builder-4.13.0.jar
[2025-07-11T11:45:23.712+0200] {subprocess.py:93} INFO - 25/07/11 11:45:23 INFO Executor: Fetching file:///home/monica.fernandez/.ivy2/jars/com.github.stephenc.jcip_jcip-annotations-1.0-1.jar with timestamp 1752227122439
[2025-07-11T11:45:23.712+0200] {subprocess.py:93} INFO - 25/07/11 11:45:23 INFO Utils: /home/monica.fernandez/.ivy2/jars/com.github.stephenc.jcip_jcip-annotations-1.0-1.jar has been previously copied to /tmp/spark-36b5bbfe-8223-445b-b376-dec57e3c841e/userFiles-61cee7c1-3f19-4f58-b026-1fdb3ca1e002/com.github.stephenc.jcip_jcip-annotations-1.0-1.jar
[2025-07-11T11:45:23.716+0200] {subprocess.py:93} INFO - 25/07/11 11:45:23 INFO Executor: Fetching file:///home/monica.fernandez/.ivy2/jars/org.reactivestreams_reactive-streams-1.0.3.jar with timestamp 1752227122439
[2025-07-11T11:45:23.716+0200] {subprocess.py:93} INFO - 25/07/11 11:45:23 INFO Utils: /home/monica.fernandez/.ivy2/jars/org.reactivestreams_reactive-streams-1.0.3.jar has been previously copied to /tmp/spark-36b5bbfe-8223-445b-b376-dec57e3c841e/userFiles-61cee7c1-3f19-4f58-b026-1fdb3ca1e002/org.reactivestreams_reactive-streams-1.0.3.jar
[2025-07-11T11:45:23.719+0200] {subprocess.py:93} INFO - 25/07/11 11:45:23 INFO Executor: Fetching file:///home/monica.fernandez/.ivy2/jars/com.datastax.oss_java-driver-mapper-runtime-4.13.0.jar with timestamp 1752227122439
[2025-07-11T11:45:23.720+0200] {subprocess.py:93} INFO - 25/07/11 11:45:23 INFO Utils: /home/monica.fernandez/.ivy2/jars/com.datastax.oss_java-driver-mapper-runtime-4.13.0.jar has been previously copied to /tmp/spark-36b5bbfe-8223-445b-b376-dec57e3c841e/userFiles-61cee7c1-3f19-4f58-b026-1fdb3ca1e002/com.datastax.oss_java-driver-mapper-runtime-4.13.0.jar
[2025-07-11T11:45:23.724+0200] {subprocess.py:93} INFO - 25/07/11 11:45:23 INFO Executor: Fetching file:///home/monica.fernandez/.ivy2/jars/org.scala-lang_scala-reflect-2.12.11.jar with timestamp 1752227122439
[2025-07-11T11:45:23.729+0200] {subprocess.py:93} INFO - 25/07/11 11:45:23 INFO Utils: /home/monica.fernandez/.ivy2/jars/org.scala-lang_scala-reflect-2.12.11.jar has been previously copied to /tmp/spark-36b5bbfe-8223-445b-b376-dec57e3c841e/userFiles-61cee7c1-3f19-4f58-b026-1fdb3ca1e002/org.scala-lang_scala-reflect-2.12.11.jar
[2025-07-11T11:45:23.734+0200] {subprocess.py:93} INFO - 25/07/11 11:45:23 INFO Executor: Fetching spark://138.4.31.14:34489/jars/com.datastax.spark_spark-cassandra-connector_2.12-3.5.0.jar with timestamp 1752227122439
[2025-07-11T11:45:23.777+0200] {subprocess.py:93} INFO - 25/07/11 11:45:23 INFO TransportClientFactory: Successfully created connection to /138.4.31.14:34489 after 30 ms (0 ms spent in bootstraps)
[2025-07-11T11:45:23.786+0200] {subprocess.py:93} INFO - 25/07/11 11:45:23 INFO Utils: Fetching spark://138.4.31.14:34489/jars/com.datastax.spark_spark-cassandra-connector_2.12-3.5.0.jar to /tmp/spark-36b5bbfe-8223-445b-b376-dec57e3c841e/userFiles-61cee7c1-3f19-4f58-b026-1fdb3ca1e002/fetchFileTemp6257300430385484724.tmp
[2025-07-11T11:45:23.826+0200] {subprocess.py:93} INFO - 25/07/11 11:45:23 INFO Utils: /tmp/spark-36b5bbfe-8223-445b-b376-dec57e3c841e/userFiles-61cee7c1-3f19-4f58-b026-1fdb3ca1e002/fetchFileTemp6257300430385484724.tmp has been previously copied to /tmp/spark-36b5bbfe-8223-445b-b376-dec57e3c841e/userFiles-61cee7c1-3f19-4f58-b026-1fdb3ca1e002/com.datastax.spark_spark-cassandra-connector_2.12-3.5.0.jar
[2025-07-11T11:45:23.831+0200] {subprocess.py:93} INFO - 25/07/11 11:45:23 INFO Executor: Adding file:/tmp/spark-36b5bbfe-8223-445b-b376-dec57e3c841e/userFiles-61cee7c1-3f19-4f58-b026-1fdb3ca1e002/com.datastax.spark_spark-cassandra-connector_2.12-3.5.0.jar to class loader default
[2025-07-11T11:45:23.831+0200] {subprocess.py:93} INFO - 25/07/11 11:45:23 INFO Executor: Fetching spark://138.4.31.14:34489/jars/com.datastax.oss_java-driver-core-shaded-4.13.0.jar with timestamp 1752227122439
[2025-07-11T11:45:23.834+0200] {subprocess.py:93} INFO - 25/07/11 11:45:23 INFO Utils: Fetching spark://138.4.31.14:34489/jars/com.datastax.oss_java-driver-core-shaded-4.13.0.jar to /tmp/spark-36b5bbfe-8223-445b-b376-dec57e3c841e/userFiles-61cee7c1-3f19-4f58-b026-1fdb3ca1e002/fetchFileTemp5373473760618508429.tmp
[2025-07-11T11:45:23.871+0200] {subprocess.py:93} INFO - 25/07/11 11:45:23 INFO Utils: /tmp/spark-36b5bbfe-8223-445b-b376-dec57e3c841e/userFiles-61cee7c1-3f19-4f58-b026-1fdb3ca1e002/fetchFileTemp5373473760618508429.tmp has been previously copied to /tmp/spark-36b5bbfe-8223-445b-b376-dec57e3c841e/userFiles-61cee7c1-3f19-4f58-b026-1fdb3ca1e002/com.datastax.oss_java-driver-core-shaded-4.13.0.jar
[2025-07-11T11:45:23.876+0200] {subprocess.py:93} INFO - 25/07/11 11:45:23 INFO Executor: Adding file:/tmp/spark-36b5bbfe-8223-445b-b376-dec57e3c841e/userFiles-61cee7c1-3f19-4f58-b026-1fdb3ca1e002/com.datastax.oss_java-driver-core-shaded-4.13.0.jar to class loader default
[2025-07-11T11:45:23.876+0200] {subprocess.py:93} INFO - 25/07/11 11:45:23 INFO Executor: Fetching spark://138.4.31.14:34489/jars/com.datastax.oss_java-driver-query-builder-4.13.0.jar with timestamp 1752227122439
[2025-07-11T11:45:23.877+0200] {subprocess.py:93} INFO - 25/07/11 11:45:23 INFO Utils: Fetching spark://138.4.31.14:34489/jars/com.datastax.oss_java-driver-query-builder-4.13.0.jar to /tmp/spark-36b5bbfe-8223-445b-b376-dec57e3c841e/userFiles-61cee7c1-3f19-4f58-b026-1fdb3ca1e002/fetchFileTemp11581293789135567285.tmp
[2025-07-11T11:45:23.879+0200] {subprocess.py:93} INFO - 25/07/11 11:45:23 INFO Utils: /tmp/spark-36b5bbfe-8223-445b-b376-dec57e3c841e/userFiles-61cee7c1-3f19-4f58-b026-1fdb3ca1e002/fetchFileTemp11581293789135567285.tmp has been previously copied to /tmp/spark-36b5bbfe-8223-445b-b376-dec57e3c841e/userFiles-61cee7c1-3f19-4f58-b026-1fdb3ca1e002/com.datastax.oss_java-driver-query-builder-4.13.0.jar
[2025-07-11T11:45:23.882+0200] {subprocess.py:93} INFO - 25/07/11 11:45:23 INFO Executor: Adding file:/tmp/spark-36b5bbfe-8223-445b-b376-dec57e3c841e/userFiles-61cee7c1-3f19-4f58-b026-1fdb3ca1e002/com.datastax.oss_java-driver-query-builder-4.13.0.jar to class loader default
[2025-07-11T11:45:23.882+0200] {subprocess.py:93} INFO - 25/07/11 11:45:23 INFO Executor: Fetching spark://138.4.31.14:34489/jars/com.typesafe_config-1.4.1.jar with timestamp 1752227122439
[2025-07-11T11:45:23.883+0200] {subprocess.py:93} INFO - 25/07/11 11:45:23 INFO Utils: Fetching spark://138.4.31.14:34489/jars/com.typesafe_config-1.4.1.jar to /tmp/spark-36b5bbfe-8223-445b-b376-dec57e3c841e/userFiles-61cee7c1-3f19-4f58-b026-1fdb3ca1e002/fetchFileTemp2453882926634038955.tmp
[2025-07-11T11:45:23.886+0200] {subprocess.py:93} INFO - 25/07/11 11:45:23 INFO Utils: /tmp/spark-36b5bbfe-8223-445b-b376-dec57e3c841e/userFiles-61cee7c1-3f19-4f58-b026-1fdb3ca1e002/fetchFileTemp2453882926634038955.tmp has been previously copied to /tmp/spark-36b5bbfe-8223-445b-b376-dec57e3c841e/userFiles-61cee7c1-3f19-4f58-b026-1fdb3ca1e002/com.typesafe_config-1.4.1.jar
[2025-07-11T11:45:23.890+0200] {subprocess.py:93} INFO - 25/07/11 11:45:23 INFO Executor: Adding file:/tmp/spark-36b5bbfe-8223-445b-b376-dec57e3c841e/userFiles-61cee7c1-3f19-4f58-b026-1fdb3ca1e002/com.typesafe_config-1.4.1.jar to class loader default
[2025-07-11T11:45:23.891+0200] {subprocess.py:93} INFO - 25/07/11 11:45:23 INFO Executor: Fetching spark://138.4.31.14:34489/jars/com.datastax.oss_java-driver-mapper-runtime-4.13.0.jar with timestamp 1752227122439
[2025-07-11T11:45:23.891+0200] {subprocess.py:93} INFO - 25/07/11 11:45:23 INFO Utils: Fetching spark://138.4.31.14:34489/jars/com.datastax.oss_java-driver-mapper-runtime-4.13.0.jar to /tmp/spark-36b5bbfe-8223-445b-b376-dec57e3c841e/userFiles-61cee7c1-3f19-4f58-b026-1fdb3ca1e002/fetchFileTemp12162371449123672325.tmp
[2025-07-11T11:45:23.891+0200] {subprocess.py:93} INFO - 25/07/11 11:45:23 INFO Utils: /tmp/spark-36b5bbfe-8223-445b-b376-dec57e3c841e/userFiles-61cee7c1-3f19-4f58-b026-1fdb3ca1e002/fetchFileTemp12162371449123672325.tmp has been previously copied to /tmp/spark-36b5bbfe-8223-445b-b376-dec57e3c841e/userFiles-61cee7c1-3f19-4f58-b026-1fdb3ca1e002/com.datastax.oss_java-driver-mapper-runtime-4.13.0.jar
[2025-07-11T11:45:23.896+0200] {subprocess.py:93} INFO - 25/07/11 11:45:23 INFO Executor: Adding file:/tmp/spark-36b5bbfe-8223-445b-b376-dec57e3c841e/userFiles-61cee7c1-3f19-4f58-b026-1fdb3ca1e002/com.datastax.oss_java-driver-mapper-runtime-4.13.0.jar to class loader default
[2025-07-11T11:45:23.896+0200] {subprocess.py:93} INFO - 25/07/11 11:45:23 INFO Executor: Fetching spark://138.4.31.14:34489/jars/com.datastax.oss_native-protocol-1.5.0.jar with timestamp 1752227122439
[2025-07-11T11:45:23.897+0200] {subprocess.py:93} INFO - 25/07/11 11:45:23 INFO Utils: Fetching spark://138.4.31.14:34489/jars/com.datastax.oss_native-protocol-1.5.0.jar to /tmp/spark-36b5bbfe-8223-445b-b376-dec57e3c841e/userFiles-61cee7c1-3f19-4f58-b026-1fdb3ca1e002/fetchFileTemp5933539144728443618.tmp
[2025-07-11T11:45:23.902+0200] {subprocess.py:93} INFO - 25/07/11 11:45:23 INFO Utils: /tmp/spark-36b5bbfe-8223-445b-b376-dec57e3c841e/userFiles-61cee7c1-3f19-4f58-b026-1fdb3ca1e002/fetchFileTemp5933539144728443618.tmp has been previously copied to /tmp/spark-36b5bbfe-8223-445b-b376-dec57e3c841e/userFiles-61cee7c1-3f19-4f58-b026-1fdb3ca1e002/com.datastax.oss_native-protocol-1.5.0.jar
[2025-07-11T11:45:23.906+0200] {subprocess.py:93} INFO - 25/07/11 11:45:23 INFO Executor: Adding file:/tmp/spark-36b5bbfe-8223-445b-b376-dec57e3c841e/userFiles-61cee7c1-3f19-4f58-b026-1fdb3ca1e002/com.datastax.oss_native-protocol-1.5.0.jar to class loader default
[2025-07-11T11:45:23.906+0200] {subprocess.py:93} INFO - 25/07/11 11:45:23 INFO Executor: Fetching spark://138.4.31.14:34489/jars/com.github.spotbugs_spotbugs-annotations-3.1.12.jar with timestamp 1752227122439
[2025-07-11T11:45:23.908+0200] {subprocess.py:93} INFO - 25/07/11 11:45:23 INFO Utils: Fetching spark://138.4.31.14:34489/jars/com.github.spotbugs_spotbugs-annotations-3.1.12.jar to /tmp/spark-36b5bbfe-8223-445b-b376-dec57e3c841e/userFiles-61cee7c1-3f19-4f58-b026-1fdb3ca1e002/fetchFileTemp13552341472625604416.tmp
[2025-07-11T11:45:23.908+0200] {subprocess.py:93} INFO - 25/07/11 11:45:23 INFO Utils: /tmp/spark-36b5bbfe-8223-445b-b376-dec57e3c841e/userFiles-61cee7c1-3f19-4f58-b026-1fdb3ca1e002/fetchFileTemp13552341472625604416.tmp has been previously copied to /tmp/spark-36b5bbfe-8223-445b-b376-dec57e3c841e/userFiles-61cee7c1-3f19-4f58-b026-1fdb3ca1e002/com.github.spotbugs_spotbugs-annotations-3.1.12.jar
[2025-07-11T11:45:23.912+0200] {subprocess.py:93} INFO - 25/07/11 11:45:23 INFO Executor: Adding file:/tmp/spark-36b5bbfe-8223-445b-b376-dec57e3c841e/userFiles-61cee7c1-3f19-4f58-b026-1fdb3ca1e002/com.github.spotbugs_spotbugs-annotations-3.1.12.jar to class loader default
[2025-07-11T11:45:23.912+0200] {subprocess.py:93} INFO - 25/07/11 11:45:23 INFO Executor: Fetching spark://138.4.31.14:34489/jars/com.github.stephenc.jcip_jcip-annotations-1.0-1.jar with timestamp 1752227122439
[2025-07-11T11:45:23.913+0200] {subprocess.py:93} INFO - 25/07/11 11:45:23 INFO Utils: Fetching spark://138.4.31.14:34489/jars/com.github.stephenc.jcip_jcip-annotations-1.0-1.jar to /tmp/spark-36b5bbfe-8223-445b-b376-dec57e3c841e/userFiles-61cee7c1-3f19-4f58-b026-1fdb3ca1e002/fetchFileTemp16417886076020724383.tmp
[2025-07-11T11:45:23.915+0200] {subprocess.py:93} INFO - 25/07/11 11:45:23 INFO Utils: /tmp/spark-36b5bbfe-8223-445b-b376-dec57e3c841e/userFiles-61cee7c1-3f19-4f58-b026-1fdb3ca1e002/fetchFileTemp16417886076020724383.tmp has been previously copied to /tmp/spark-36b5bbfe-8223-445b-b376-dec57e3c841e/userFiles-61cee7c1-3f19-4f58-b026-1fdb3ca1e002/com.github.stephenc.jcip_jcip-annotations-1.0-1.jar
[2025-07-11T11:45:23.919+0200] {subprocess.py:93} INFO - 25/07/11 11:45:23 INFO Executor: Adding file:/tmp/spark-36b5bbfe-8223-445b-b376-dec57e3c841e/userFiles-61cee7c1-3f19-4f58-b026-1fdb3ca1e002/com.github.stephenc.jcip_jcip-annotations-1.0-1.jar to class loader default
[2025-07-11T11:45:23.919+0200] {subprocess.py:93} INFO - 25/07/11 11:45:23 INFO Executor: Fetching spark://138.4.31.14:34489/jars/org.apache.commons_commons-lang3-3.10.jar with timestamp 1752227122439
[2025-07-11T11:45:23.921+0200] {subprocess.py:93} INFO - 25/07/11 11:45:23 INFO Utils: Fetching spark://138.4.31.14:34489/jars/org.apache.commons_commons-lang3-3.10.jar to /tmp/spark-36b5bbfe-8223-445b-b376-dec57e3c841e/userFiles-61cee7c1-3f19-4f58-b026-1fdb3ca1e002/fetchFileTemp3148339846930303496.tmp
[2025-07-11T11:45:23.923+0200] {subprocess.py:93} INFO - 25/07/11 11:45:23 INFO Utils: /tmp/spark-36b5bbfe-8223-445b-b376-dec57e3c841e/userFiles-61cee7c1-3f19-4f58-b026-1fdb3ca1e002/fetchFileTemp3148339846930303496.tmp has been previously copied to /tmp/spark-36b5bbfe-8223-445b-b376-dec57e3c841e/userFiles-61cee7c1-3f19-4f58-b026-1fdb3ca1e002/org.apache.commons_commons-lang3-3.10.jar
[2025-07-11T11:45:23.928+0200] {subprocess.py:93} INFO - 25/07/11 11:45:23 INFO Executor: Adding file:/tmp/spark-36b5bbfe-8223-445b-b376-dec57e3c841e/userFiles-61cee7c1-3f19-4f58-b026-1fdb3ca1e002/org.apache.commons_commons-lang3-3.10.jar to class loader default
[2025-07-11T11:45:23.928+0200] {subprocess.py:93} INFO - 25/07/11 11:45:23 INFO Executor: Fetching spark://138.4.31.14:34489/jars/com.thoughtworks.paranamer_paranamer-2.8.jar with timestamp 1752227122439
[2025-07-11T11:45:23.928+0200] {subprocess.py:93} INFO - 25/07/11 11:45:23 INFO Utils: Fetching spark://138.4.31.14:34489/jars/com.thoughtworks.paranamer_paranamer-2.8.jar to /tmp/spark-36b5bbfe-8223-445b-b376-dec57e3c841e/userFiles-61cee7c1-3f19-4f58-b026-1fdb3ca1e002/fetchFileTemp2850673946437197018.tmp
[2025-07-11T11:45:23.930+0200] {subprocess.py:93} INFO - 25/07/11 11:45:23 INFO Utils: /tmp/spark-36b5bbfe-8223-445b-b376-dec57e3c841e/userFiles-61cee7c1-3f19-4f58-b026-1fdb3ca1e002/fetchFileTemp2850673946437197018.tmp has been previously copied to /tmp/spark-36b5bbfe-8223-445b-b376-dec57e3c841e/userFiles-61cee7c1-3f19-4f58-b026-1fdb3ca1e002/com.thoughtworks.paranamer_paranamer-2.8.jar
[2025-07-11T11:45:23.934+0200] {subprocess.py:93} INFO - 25/07/11 11:45:23 INFO Executor: Adding file:/tmp/spark-36b5bbfe-8223-445b-b376-dec57e3c841e/userFiles-61cee7c1-3f19-4f58-b026-1fdb3ca1e002/com.thoughtworks.paranamer_paranamer-2.8.jar to class loader default
[2025-07-11T11:45:23.934+0200] {subprocess.py:93} INFO - 25/07/11 11:45:23 INFO Executor: Fetching spark://138.4.31.14:34489/jars/org.scala-lang_scala-reflect-2.12.11.jar with timestamp 1752227122439
[2025-07-11T11:45:23.934+0200] {subprocess.py:93} INFO - 25/07/11 11:45:23 INFO Utils: Fetching spark://138.4.31.14:34489/jars/org.scala-lang_scala-reflect-2.12.11.jar to /tmp/spark-36b5bbfe-8223-445b-b376-dec57e3c841e/userFiles-61cee7c1-3f19-4f58-b026-1fdb3ca1e002/fetchFileTemp2916434954307904147.tmp
[2025-07-11T11:45:23.955+0200] {subprocess.py:93} INFO - 25/07/11 11:45:23 INFO Utils: /tmp/spark-36b5bbfe-8223-445b-b376-dec57e3c841e/userFiles-61cee7c1-3f19-4f58-b026-1fdb3ca1e002/fetchFileTemp2916434954307904147.tmp has been previously copied to /tmp/spark-36b5bbfe-8223-445b-b376-dec57e3c841e/userFiles-61cee7c1-3f19-4f58-b026-1fdb3ca1e002/org.scala-lang_scala-reflect-2.12.11.jar
[2025-07-11T11:45:23.960+0200] {subprocess.py:93} INFO - 25/07/11 11:45:23 INFO Executor: Adding file:/tmp/spark-36b5bbfe-8223-445b-b376-dec57e3c841e/userFiles-61cee7c1-3f19-4f58-b026-1fdb3ca1e002/org.scala-lang_scala-reflect-2.12.11.jar to class loader default
[2025-07-11T11:45:23.960+0200] {subprocess.py:93} INFO - 25/07/11 11:45:23 INFO Executor: Fetching spark://138.4.31.14:34489/jars/org.reactivestreams_reactive-streams-1.0.3.jar with timestamp 1752227122439
[2025-07-11T11:45:23.960+0200] {subprocess.py:93} INFO - 25/07/11 11:45:23 INFO Utils: Fetching spark://138.4.31.14:34489/jars/org.reactivestreams_reactive-streams-1.0.3.jar to /tmp/spark-36b5bbfe-8223-445b-b376-dec57e3c841e/userFiles-61cee7c1-3f19-4f58-b026-1fdb3ca1e002/fetchFileTemp9355146535518918102.tmp
[2025-07-11T11:45:23.962+0200] {subprocess.py:93} INFO - 25/07/11 11:45:23 INFO Utils: /tmp/spark-36b5bbfe-8223-445b-b376-dec57e3c841e/userFiles-61cee7c1-3f19-4f58-b026-1fdb3ca1e002/fetchFileTemp9355146535518918102.tmp has been previously copied to /tmp/spark-36b5bbfe-8223-445b-b376-dec57e3c841e/userFiles-61cee7c1-3f19-4f58-b026-1fdb3ca1e002/org.reactivestreams_reactive-streams-1.0.3.jar
[2025-07-11T11:45:23.965+0200] {subprocess.py:93} INFO - 25/07/11 11:45:23 INFO Executor: Adding file:/tmp/spark-36b5bbfe-8223-445b-b376-dec57e3c841e/userFiles-61cee7c1-3f19-4f58-b026-1fdb3ca1e002/org.reactivestreams_reactive-streams-1.0.3.jar to class loader default
[2025-07-11T11:45:23.966+0200] {subprocess.py:93} INFO - 25/07/11 11:45:23 INFO Executor: Fetching spark://138.4.31.14:34489/jars/com.datastax.oss_java-driver-shaded-guava-25.1-jre-graal-sub-1.jar with timestamp 1752227122439
[2025-07-11T11:45:23.966+0200] {subprocess.py:93} INFO - 25/07/11 11:45:23 INFO Utils: Fetching spark://138.4.31.14:34489/jars/com.datastax.oss_java-driver-shaded-guava-25.1-jre-graal-sub-1.jar to /tmp/spark-36b5bbfe-8223-445b-b376-dec57e3c841e/userFiles-61cee7c1-3f19-4f58-b026-1fdb3ca1e002/fetchFileTemp1166953573661722868.tmp
[2025-07-11T11:45:23.982+0200] {subprocess.py:93} INFO - 25/07/11 11:45:23 INFO Utils: /tmp/spark-36b5bbfe-8223-445b-b376-dec57e3c841e/userFiles-61cee7c1-3f19-4f58-b026-1fdb3ca1e002/fetchFileTemp1166953573661722868.tmp has been previously copied to /tmp/spark-36b5bbfe-8223-445b-b376-dec57e3c841e/userFiles-61cee7c1-3f19-4f58-b026-1fdb3ca1e002/com.datastax.oss_java-driver-shaded-guava-25.1-jre-graal-sub-1.jar
[2025-07-11T11:45:23.986+0200] {subprocess.py:93} INFO - 25/07/11 11:45:23 INFO Executor: Adding file:/tmp/spark-36b5bbfe-8223-445b-b376-dec57e3c841e/userFiles-61cee7c1-3f19-4f58-b026-1fdb3ca1e002/com.datastax.oss_java-driver-shaded-guava-25.1-jre-graal-sub-1.jar to class loader default
[2025-07-11T11:45:23.986+0200] {subprocess.py:93} INFO - 25/07/11 11:45:23 INFO Executor: Fetching spark://138.4.31.14:34489/jars/io.dropwizard.metrics_metrics-core-4.1.18.jar with timestamp 1752227122439
[2025-07-11T11:45:23.987+0200] {subprocess.py:93} INFO - 25/07/11 11:45:23 INFO Utils: Fetching spark://138.4.31.14:34489/jars/io.dropwizard.metrics_metrics-core-4.1.18.jar to /tmp/spark-36b5bbfe-8223-445b-b376-dec57e3c841e/userFiles-61cee7c1-3f19-4f58-b026-1fdb3ca1e002/fetchFileTemp15883045974160841939.tmp
[2025-07-11T11:45:23.988+0200] {subprocess.py:93} INFO - 25/07/11 11:45:23 INFO Utils: /tmp/spark-36b5bbfe-8223-445b-b376-dec57e3c841e/userFiles-61cee7c1-3f19-4f58-b026-1fdb3ca1e002/fetchFileTemp15883045974160841939.tmp has been previously copied to /tmp/spark-36b5bbfe-8223-445b-b376-dec57e3c841e/userFiles-61cee7c1-3f19-4f58-b026-1fdb3ca1e002/io.dropwizard.metrics_metrics-core-4.1.18.jar
[2025-07-11T11:45:23.992+0200] {subprocess.py:93} INFO - 25/07/11 11:45:23 INFO Executor: Adding file:/tmp/spark-36b5bbfe-8223-445b-b376-dec57e3c841e/userFiles-61cee7c1-3f19-4f58-b026-1fdb3ca1e002/io.dropwizard.metrics_metrics-core-4.1.18.jar to class loader default
[2025-07-11T11:45:23.993+0200] {subprocess.py:93} INFO - 25/07/11 11:45:23 INFO Executor: Fetching spark://138.4.31.14:34489/jars/com.datastax.spark_spark-cassandra-connector-driver_2.12-3.5.0.jar with timestamp 1752227122439
[2025-07-11T11:45:23.993+0200] {subprocess.py:93} INFO - 25/07/11 11:45:23 INFO Utils: Fetching spark://138.4.31.14:34489/jars/com.datastax.spark_spark-cassandra-connector-driver_2.12-3.5.0.jar to /tmp/spark-36b5bbfe-8223-445b-b376-dec57e3c841e/userFiles-61cee7c1-3f19-4f58-b026-1fdb3ca1e002/fetchFileTemp7108929577784872875.tmp
[2025-07-11T11:45:23.998+0200] {subprocess.py:93} INFO - 25/07/11 11:45:23 INFO Utils: /tmp/spark-36b5bbfe-8223-445b-b376-dec57e3c841e/userFiles-61cee7c1-3f19-4f58-b026-1fdb3ca1e002/fetchFileTemp7108929577784872875.tmp has been previously copied to /tmp/spark-36b5bbfe-8223-445b-b376-dec57e3c841e/userFiles-61cee7c1-3f19-4f58-b026-1fdb3ca1e002/com.datastax.spark_spark-cassandra-connector-driver_2.12-3.5.0.jar
[2025-07-11T11:45:24.002+0200] {subprocess.py:93} INFO - 25/07/11 11:45:24 INFO Executor: Adding file:/tmp/spark-36b5bbfe-8223-445b-b376-dec57e3c841e/userFiles-61cee7c1-3f19-4f58-b026-1fdb3ca1e002/com.datastax.spark_spark-cassandra-connector-driver_2.12-3.5.0.jar to class loader default
[2025-07-11T11:45:24.003+0200] {subprocess.py:93} INFO - 25/07/11 11:45:24 INFO Executor: Fetching spark://138.4.31.14:34489/jars/org.slf4j_slf4j-api-1.7.26.jar with timestamp 1752227122439
[2025-07-11T11:45:24.005+0200] {subprocess.py:93} INFO - 25/07/11 11:45:24 INFO Utils: Fetching spark://138.4.31.14:34489/jars/org.slf4j_slf4j-api-1.7.26.jar to /tmp/spark-36b5bbfe-8223-445b-b376-dec57e3c841e/userFiles-61cee7c1-3f19-4f58-b026-1fdb3ca1e002/fetchFileTemp1712318069777376975.tmp
[2025-07-11T11:45:24.005+0200] {subprocess.py:93} INFO - 25/07/11 11:45:24 INFO Utils: /tmp/spark-36b5bbfe-8223-445b-b376-dec57e3c841e/userFiles-61cee7c1-3f19-4f58-b026-1fdb3ca1e002/fetchFileTemp1712318069777376975.tmp has been previously copied to /tmp/spark-36b5bbfe-8223-445b-b376-dec57e3c841e/userFiles-61cee7c1-3f19-4f58-b026-1fdb3ca1e002/org.slf4j_slf4j-api-1.7.26.jar
[2025-07-11T11:45:24.009+0200] {subprocess.py:93} INFO - 25/07/11 11:45:24 INFO Executor: Adding file:/tmp/spark-36b5bbfe-8223-445b-b376-dec57e3c841e/userFiles-61cee7c1-3f19-4f58-b026-1fdb3ca1e002/org.slf4j_slf4j-api-1.7.26.jar to class loader default
[2025-07-11T11:45:24.009+0200] {subprocess.py:93} INFO - 25/07/11 11:45:24 INFO Executor: Fetching spark://138.4.31.14:34489/jars/org.hdrhistogram_HdrHistogram-2.1.12.jar with timestamp 1752227122439
[2025-07-11T11:45:24.009+0200] {subprocess.py:93} INFO - 25/07/11 11:45:24 INFO Utils: Fetching spark://138.4.31.14:34489/jars/org.hdrhistogram_HdrHistogram-2.1.12.jar to /tmp/spark-36b5bbfe-8223-445b-b376-dec57e3c841e/userFiles-61cee7c1-3f19-4f58-b026-1fdb3ca1e002/fetchFileTemp8175926266371158.tmp
[2025-07-11T11:45:24.011+0200] {subprocess.py:93} INFO - 25/07/11 11:45:24 INFO Utils: /tmp/spark-36b5bbfe-8223-445b-b376-dec57e3c841e/userFiles-61cee7c1-3f19-4f58-b026-1fdb3ca1e002/fetchFileTemp8175926266371158.tmp has been previously copied to /tmp/spark-36b5bbfe-8223-445b-b376-dec57e3c841e/userFiles-61cee7c1-3f19-4f58-b026-1fdb3ca1e002/org.hdrhistogram_HdrHistogram-2.1.12.jar
[2025-07-11T11:45:24.015+0200] {subprocess.py:93} INFO - 25/07/11 11:45:24 INFO Executor: Adding file:/tmp/spark-36b5bbfe-8223-445b-b376-dec57e3c841e/userFiles-61cee7c1-3f19-4f58-b026-1fdb3ca1e002/org.hdrhistogram_HdrHistogram-2.1.12.jar to class loader default
[2025-07-11T11:45:24.015+0200] {subprocess.py:93} INFO - 25/07/11 11:45:24 INFO Executor: Fetching spark://138.4.31.14:34489/jars/org.scala-lang.modules_scala-collection-compat_2.12-2.11.0.jar with timestamp 1752227122439
[2025-07-11T11:45:24.015+0200] {subprocess.py:93} INFO - 25/07/11 11:45:24 INFO Utils: Fetching spark://138.4.31.14:34489/jars/org.scala-lang.modules_scala-collection-compat_2.12-2.11.0.jar to /tmp/spark-36b5bbfe-8223-445b-b376-dec57e3c841e/userFiles-61cee7c1-3f19-4f58-b026-1fdb3ca1e002/fetchFileTemp12920665154483020569.tmp
[2025-07-11T11:45:24.017+0200] {subprocess.py:93} INFO - 25/07/11 11:45:24 INFO Utils: /tmp/spark-36b5bbfe-8223-445b-b376-dec57e3c841e/userFiles-61cee7c1-3f19-4f58-b026-1fdb3ca1e002/fetchFileTemp12920665154483020569.tmp has been previously copied to /tmp/spark-36b5bbfe-8223-445b-b376-dec57e3c841e/userFiles-61cee7c1-3f19-4f58-b026-1fdb3ca1e002/org.scala-lang.modules_scala-collection-compat_2.12-2.11.0.jar
[2025-07-11T11:45:24.021+0200] {subprocess.py:93} INFO - 25/07/11 11:45:24 INFO Executor: Adding file:/tmp/spark-36b5bbfe-8223-445b-b376-dec57e3c841e/userFiles-61cee7c1-3f19-4f58-b026-1fdb3ca1e002/org.scala-lang.modules_scala-collection-compat_2.12-2.11.0.jar to class loader default
[2025-07-11T11:45:24.021+0200] {subprocess.py:93} INFO - 25/07/11 11:45:24 INFO Executor: Fetching spark://138.4.31.14:34489/jars/com.google.code.findbugs_jsr305-3.0.2.jar with timestamp 1752227122439
[2025-07-11T11:45:24.022+0200] {subprocess.py:93} INFO - 25/07/11 11:45:24 INFO Utils: Fetching spark://138.4.31.14:34489/jars/com.google.code.findbugs_jsr305-3.0.2.jar to /tmp/spark-36b5bbfe-8223-445b-b376-dec57e3c841e/userFiles-61cee7c1-3f19-4f58-b026-1fdb3ca1e002/fetchFileTemp17294807567532998679.tmp
[2025-07-11T11:45:24.025+0200] {subprocess.py:93} INFO - 25/07/11 11:45:24 INFO Utils: /tmp/spark-36b5bbfe-8223-445b-b376-dec57e3c841e/userFiles-61cee7c1-3f19-4f58-b026-1fdb3ca1e002/fetchFileTemp17294807567532998679.tmp has been previously copied to /tmp/spark-36b5bbfe-8223-445b-b376-dec57e3c841e/userFiles-61cee7c1-3f19-4f58-b026-1fdb3ca1e002/com.google.code.findbugs_jsr305-3.0.2.jar
[2025-07-11T11:45:24.030+0200] {subprocess.py:93} INFO - 25/07/11 11:45:24 INFO Executor: Adding file:/tmp/spark-36b5bbfe-8223-445b-b376-dec57e3c841e/userFiles-61cee7c1-3f19-4f58-b026-1fdb3ca1e002/com.google.code.findbugs_jsr305-3.0.2.jar to class loader default
[2025-07-11T11:45:24.041+0200] {subprocess.py:93} INFO - 25/07/11 11:45:24 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 45609.
[2025-07-11T11:45:24.041+0200] {subprocess.py:93} INFO - 25/07/11 11:45:24 INFO NettyBlockTransferService: Server created on 138.4.31.14:45609
[2025-07-11T11:45:24.048+0200] {subprocess.py:93} INFO - 25/07/11 11:45:24 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[2025-07-11T11:45:24.054+0200] {subprocess.py:93} INFO - 25/07/11 11:45:24 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 138.4.31.14, 45609, None)
[2025-07-11T11:45:24.057+0200] {subprocess.py:93} INFO - 25/07/11 11:45:24 INFO BlockManagerMasterEndpoint: Registering block manager 138.4.31.14:45609 with 434.4 MiB RAM, BlockManagerId(driver, 138.4.31.14, 45609, None)
[2025-07-11T11:45:24.059+0200] {subprocess.py:93} INFO - 25/07/11 11:45:24 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 138.4.31.14, 45609, None)
[2025-07-11T11:45:24.060+0200] {subprocess.py:93} INFO - 25/07/11 11:45:24 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 138.4.31.14, 45609, None)
[2025-07-11T11:45:24.949+0200] {subprocess.py:93} INFO - 25/07/11 11:45:24 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
[2025-07-11T11:45:24.951+0200] {subprocess.py:93} INFO - 25/07/11 11:45:24 INFO SharedState: Warehouse path is 'file:/tmp/airflowtmp28u0t302/spark-warehouse'.
[2025-07-11T11:45:25.685+0200] {subprocess.py:93} INFO - 25/07/11 11:45:25 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.
[2025-07-11T11:45:27.010+0200] {subprocess.py:93} INFO - MLflow Run ID: 228200349a7b43bbb88f73a7c3d8e5fe
[2025-07-11T11:45:27.010+0200] {subprocess.py:93} INFO - MLflow Tracking URI: file:///home/monica.fernandez/practica_creativa/mlruns
[2025-07-11T11:45:27.224+0200] {subprocess.py:93} INFO - 25/07/11 11:45:27 INFO InMemoryFileIndex: It took 37 ms to list leaf files for 1 paths.
[2025-07-11T11:45:29.601+0200] {subprocess.py:93} INFO - 25/07/11 11:45:29 INFO DefaultMavenCoordinates: DataStax Java driver for Apache Cassandra(R) (com.datastax.oss:java-driver-core-shaded) version 4.13.0
[2025-07-11T11:45:29.734+0200] {subprocess.py:93} INFO - 25/07/11 11:45:29 INFO Native: Unable to load JNR native implementation. This could be normal if JNR is excluded from the classpath
[2025-07-11T11:45:29.734+0200] {subprocess.py:93} INFO - java.lang.NoClassDefFoundError: jnr/posix/POSIXHandler
[2025-07-11T11:45:29.734+0200] {subprocess.py:93} INFO - 	at com.datastax.oss.driver.internal.core.os.Native$LibcLoader.load(Native.java:42)
[2025-07-11T11:45:29.734+0200] {subprocess.py:93} INFO - 	at com.datastax.oss.driver.internal.core.os.Native.<clinit>(Native.java:59)
[2025-07-11T11:45:29.734+0200] {subprocess.py:93} INFO - 	at com.datastax.oss.driver.internal.core.time.Clock.getInstance(Clock.java:41)
[2025-07-11T11:45:29.734+0200] {subprocess.py:93} INFO - 	at com.datastax.oss.driver.internal.core.time.MonotonicTimestampGenerator.buildClock(MonotonicTimestampGenerator.java:109)
[2025-07-11T11:45:29.734+0200] {subprocess.py:93} INFO - 	at com.datastax.oss.driver.internal.core.time.MonotonicTimestampGenerator.<init>(MonotonicTimestampGenerator.java:43)
[2025-07-11T11:45:29.734+0200] {subprocess.py:93} INFO - 	at com.datastax.oss.driver.internal.core.time.AtomicTimestampGenerator.<init>(AtomicTimestampGenerator.java:52)
[2025-07-11T11:45:29.734+0200] {subprocess.py:93} INFO - 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
[2025-07-11T11:45:29.734+0200] {subprocess.py:93} INFO - 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:77)
[2025-07-11T11:45:29.734+0200] {subprocess.py:93} INFO - 	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
[2025-07-11T11:45:29.734+0200] {subprocess.py:93} INFO - 	at java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:500)
[2025-07-11T11:45:29.735+0200] {subprocess.py:93} INFO - 	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:481)
[2025-07-11T11:45:29.736+0200] {subprocess.py:93} INFO - 	at com.datastax.oss.driver.internal.core.util.Reflection.resolveClass(Reflection.java:329)
[2025-07-11T11:45:29.736+0200] {subprocess.py:93} INFO - 	at com.datastax.oss.driver.internal.core.util.Reflection.buildFromConfig(Reflection.java:235)
[2025-07-11T11:45:29.736+0200] {subprocess.py:93} INFO - 	at com.datastax.oss.driver.internal.core.util.Reflection.buildFromConfig(Reflection.java:110)
[2025-07-11T11:45:29.736+0200] {subprocess.py:93} INFO - 	at com.datastax.oss.driver.internal.core.context.DefaultDriverContext.buildTimestampGenerator(DefaultDriverContext.java:377)
[2025-07-11T11:45:29.736+0200] {subprocess.py:93} INFO - 	at com.datastax.oss.driver.internal.core.util.concurrent.LazyReference.get(LazyReference.java:55)
[2025-07-11T11:45:29.736+0200] {subprocess.py:93} INFO - 	at com.datastax.oss.driver.internal.core.context.DefaultDriverContext.getTimestampGenerator(DefaultDriverContext.java:773)
[2025-07-11T11:45:29.736+0200] {subprocess.py:93} INFO - 	at com.datastax.oss.driver.internal.core.session.DefaultSession$SingleThreaded.init(DefaultSession.java:349)
[2025-07-11T11:45:29.736+0200] {subprocess.py:93} INFO - 	at com.datastax.oss.driver.internal.core.session.DefaultSession$SingleThreaded.access$1100(DefaultSession.java:300)
[2025-07-11T11:45:29.736+0200] {subprocess.py:93} INFO - 	at com.datastax.oss.driver.internal.core.session.DefaultSession.lambda$init$0(DefaultSession.java:146)
[2025-07-11T11:45:29.737+0200] {subprocess.py:93} INFO - 	at com.datastax.oss.driver.shaded.netty.util.concurrent.PromiseTask.runTask(PromiseTask.java:98)
[2025-07-11T11:45:29.737+0200] {subprocess.py:93} INFO - 	at com.datastax.oss.driver.shaded.netty.util.concurrent.PromiseTask.run(PromiseTask.java:106)
[2025-07-11T11:45:29.737+0200] {subprocess.py:93} INFO - 	at com.datastax.oss.driver.shaded.netty.channel.DefaultEventLoop.run(DefaultEventLoop.java:54)
[2025-07-11T11:45:29.737+0200] {subprocess.py:93} INFO - 	at com.datastax.oss.driver.shaded.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989)
[2025-07-11T11:45:29.737+0200] {subprocess.py:93} INFO - 	at com.datastax.oss.driver.shaded.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
[2025-07-11T11:45:29.737+0200] {subprocess.py:93} INFO - 	at com.datastax.oss.driver.shaded.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
[2025-07-11T11:45:29.737+0200] {subprocess.py:93} INFO - 	at java.base/java.lang.Thread.run(Thread.java:840)
[2025-07-11T11:45:29.737+0200] {subprocess.py:93} INFO - Caused by: java.lang.ClassNotFoundException: jnr.posix.POSIXHandler
[2025-07-11T11:45:29.737+0200] {subprocess.py:93} INFO - 	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:445)
[2025-07-11T11:45:29.737+0200] {subprocess.py:93} INFO - 	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:592)
[2025-07-11T11:45:29.737+0200] {subprocess.py:93} INFO - 	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:525)
[2025-07-11T11:45:29.737+0200] {subprocess.py:93} INFO - 	... 27 more
[2025-07-11T11:45:29.738+0200] {subprocess.py:93} INFO - 25/07/11 11:45:29 INFO Clock: Could not access native clock (see debug logs for details), falling back to Java system clock
[2025-07-11T11:45:30.772+0200] {subprocess.py:93} INFO - 25/07/11 11:45:30 INFO CassandraConnector: Connected to Cassandra cluster.
[2025-07-11T11:45:31.373+0200] {subprocess.py:93} INFO - 25/07/11 11:45:31 INFO FileSystemOverwrite: Path /home/monica.fernandez/practica_creativa/models/arrival_bucketizer_2.0.bin already exists. It will be overwritten.
[2025-07-11T11:45:31.686+0200] {subprocess.py:93} INFO - 25/07/11 11:45:31 INFO deprecation: mapred.output.dir is deprecated. Instead, use mapreduce.output.fileoutputformat.outputdir
[2025-07-11T11:45:31.690+0200] {subprocess.py:93} INFO - 25/07/11 11:45:31 INFO HadoopMapRedCommitProtocol: Using output committer class org.apache.hadoop.mapred.FileOutputCommitter
[2025-07-11T11:45:31.693+0200] {subprocess.py:93} INFO - 25/07/11 11:45:31 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2025-07-11T11:45:31.693+0200] {subprocess.py:93} INFO - 25/07/11 11:45:31 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2025-07-11T11:45:31.759+0200] {subprocess.py:93} INFO - 25/07/11 11:45:31 INFO SparkContext: Starting job: runJob at SparkHadoopWriter.scala:83
[2025-07-11T11:45:31.772+0200] {subprocess.py:93} INFO - 25/07/11 11:45:31 INFO DAGScheduler: Got job 0 (runJob at SparkHadoopWriter.scala:83) with 1 output partitions
[2025-07-11T11:45:31.772+0200] {subprocess.py:93} INFO - 25/07/11 11:45:31 INFO DAGScheduler: Final stage: ResultStage 0 (runJob at SparkHadoopWriter.scala:83)
[2025-07-11T11:45:31.772+0200] {subprocess.py:93} INFO - 25/07/11 11:45:31 INFO DAGScheduler: Parents of final stage: List()
[2025-07-11T11:45:31.773+0200] {subprocess.py:93} INFO - 25/07/11 11:45:31 INFO DAGScheduler: Missing parents: List()
[2025-07-11T11:45:31.777+0200] {subprocess.py:93} INFO - 25/07/11 11:45:31 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[1] at saveAsTextFile at ReadWrite.scala:413), which has no missing parents
[2025-07-11T11:45:31.859+0200] {subprocess.py:93} INFO - 25/07/11 11:45:31 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 101.3 KiB, free 434.3 MiB)
[2025-07-11T11:45:31.899+0200] {subprocess.py:93} INFO - 25/07/11 11:45:31 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 36.5 KiB, free 434.3 MiB)
[2025-07-11T11:45:31.904+0200] {subprocess.py:93} INFO - 25/07/11 11:45:31 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 138.4.31.14:45609 (size: 36.5 KiB, free: 434.4 MiB)
[2025-07-11T11:45:31.906+0200] {subprocess.py:93} INFO - 25/07/11 11:45:31 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1585
[2025-07-11T11:45:31.928+0200] {subprocess.py:93} INFO - 25/07/11 11:45:31 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[1] at saveAsTextFile at ReadWrite.scala:413) (first 15 tasks are for partitions Vector(0))
[2025-07-11T11:45:31.929+0200] {subprocess.py:93} INFO - 25/07/11 11:45:31 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
[2025-07-11T11:45:31.988+0200] {subprocess.py:93} INFO - 25/07/11 11:45:31 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (138.4.31.14, executor driver, partition 0, PROCESS_LOCAL, 13275 bytes)
[2025-07-11T11:45:32.011+0200] {subprocess.py:93} INFO - 25/07/11 11:45:32 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
[2025-07-11T11:45:32.151+0200] {subprocess.py:93} INFO - 25/07/11 11:45:32 INFO HadoopMapRedCommitProtocol: Using output committer class org.apache.hadoop.mapred.FileOutputCommitter
[2025-07-11T11:45:32.151+0200] {subprocess.py:93} INFO - 25/07/11 11:45:32 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2025-07-11T11:45:32.152+0200] {subprocess.py:93} INFO - 25/07/11 11:45:32 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2025-07-11T11:45:32.224+0200] {subprocess.py:93} INFO - 25/07/11 11:45:32 INFO FileOutputCommitter: Saved output of task 'attempt_202507111145315829444822944816419_0001_m_000000_0' to file:/home/monica.fernandez/practica_creativa/models/arrival_bucketizer_2.0.bin/metadata/_temporary/0/task_202507111145315829444822944816419_0001_m_000000
[2025-07-11T11:45:32.225+0200] {subprocess.py:93} INFO - 25/07/11 11:45:32 INFO SparkHadoopMapRedUtil: attempt_202507111145315829444822944816419_0001_m_000000_0: Committed. Elapsed time: 1 ms.
[2025-07-11T11:45:32.247+0200] {subprocess.py:93} INFO - 25/07/11 11:45:32 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 1256 bytes result sent to driver
[2025-07-11T11:45:32.258+0200] {subprocess.py:93} INFO - 25/07/11 11:45:32 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 299 ms on 138.4.31.14 (executor driver) (1/1)
[2025-07-11T11:45:32.260+0200] {subprocess.py:93} INFO - 25/07/11 11:45:32 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool
[2025-07-11T11:45:32.265+0200] {subprocess.py:93} INFO - 25/07/11 11:45:32 INFO DAGScheduler: ResultStage 0 (runJob at SparkHadoopWriter.scala:83) finished in 0,476 s
[2025-07-11T11:45:32.268+0200] {subprocess.py:93} INFO - 25/07/11 11:45:32 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
[2025-07-11T11:45:32.268+0200] {subprocess.py:93} INFO - 25/07/11 11:45:32 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
[2025-07-11T11:45:32.270+0200] {subprocess.py:93} INFO - 25/07/11 11:45:32 INFO DAGScheduler: Job 0 finished: runJob at SparkHadoopWriter.scala:83, took 0,511275 s
[2025-07-11T11:45:32.272+0200] {subprocess.py:93} INFO - 25/07/11 11:45:32 INFO SparkHadoopWriter: Start to commit write Job job_202507111145315829444822944816419_0001.
[2025-07-11T11:45:32.300+0200] {subprocess.py:93} INFO - 25/07/11 11:45:32 INFO SparkHadoopWriter: Write Job job_202507111145315829444822944816419_0001 committed. Elapsed time: 27 ms.
[2025-07-11T11:45:33.051+0200] {subprocess.py:93} INFO - 25/07/11 11:45:33 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 138.4.31.14:45609 in memory (size: 36.5 KiB, free: 434.4 MiB)
[2025-07-11T11:45:33.127+0200] {subprocess.py:93} INFO - 25/07/11 11:45:33 INFO V2ScanRelationPushDown:
[2025-07-11T11:45:33.127+0200] {subprocess.py:93} INFO - Pushing operators to origin_dest_distances
[2025-07-11T11:45:33.127+0200] {subprocess.py:93} INFO - Pushed Filters:
[2025-07-11T11:45:33.127+0200] {subprocess.py:93} INFO - Post-Scan Filters:
[2025-07-11T11:45:33.127+0200] {subprocess.py:93} INFO - 
[2025-07-11T11:45:33.204+0200] {subprocess.py:93} INFO - 25/07/11 11:45:33 INFO V2ScanRelationPushDown:
[2025-07-11T11:45:33.205+0200] {subprocess.py:93} INFO - Output: origin#42, dest#43
[2025-07-11T11:45:33.205+0200] {subprocess.py:93} INFO - 
[2025-07-11T11:45:33.599+0200] {subprocess.py:93} INFO - 25/07/11 11:45:33 INFO FileSourceStrategy: Pushed Filters:
[2025-07-11T11:45:33.600+0200] {subprocess.py:93} INFO - 25/07/11 11:45:33 INFO FileSourceStrategy: Post-Scan Filters:
[2025-07-11T11:45:33.857+0200] {subprocess.py:93} INFO - 25/07/11 11:45:33 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 209.9 KiB, free 434.2 MiB)
[2025-07-11T11:45:33.872+0200] {subprocess.py:93} INFO - 25/07/11 11:45:33 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 37.7 KiB, free 434.2 MiB)
[2025-07-11T11:45:33.872+0200] {subprocess.py:93} INFO - 25/07/11 11:45:33 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 138.4.31.14:45609 (size: 37.7 KiB, free: 434.4 MiB)
[2025-07-11T11:45:33.873+0200] {subprocess.py:93} INFO - 25/07/11 11:45:33 INFO SparkContext: Created broadcast 1 from collect at StringIndexer.scala:204
[2025-07-11T11:45:33.899+0200] {subprocess.py:93} INFO - 25/07/11 11:45:33 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
[2025-07-11T11:45:33.967+0200] {subprocess.py:93} INFO - 25/07/11 11:45:33 INFO DAGScheduler: Registering RDD 4 (collect at StringIndexer.scala:204) as input to shuffle 0
[2025-07-11T11:45:33.971+0200] {subprocess.py:93} INFO - 25/07/11 11:45:33 INFO DAGScheduler: Got map stage job 1 (collect at StringIndexer.scala:204) with 2 output partitions
[2025-07-11T11:45:33.971+0200] {subprocess.py:93} INFO - 25/07/11 11:45:33 INFO DAGScheduler: Final stage: ShuffleMapStage 1 (collect at StringIndexer.scala:204)
[2025-07-11T11:45:33.971+0200] {subprocess.py:93} INFO - 25/07/11 11:45:33 INFO DAGScheduler: Parents of final stage: List()
[2025-07-11T11:45:33.973+0200] {subprocess.py:93} INFO - 25/07/11 11:45:33 INFO DAGScheduler: Missing parents: List()
[2025-07-11T11:45:33.979+0200] {subprocess.py:93} INFO - 25/07/11 11:45:33 INFO DAGScheduler: Submitting ShuffleMapStage 1 (MapPartitionsRDD[4] at collect at StringIndexer.scala:204), which has no missing parents
[2025-07-11T11:45:34.065+0200] {subprocess.py:93} INFO - 25/07/11 11:45:34 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 13.8 KiB, free 434.1 MiB)
[2025-07-11T11:45:34.067+0200] {subprocess.py:93} INFO - 25/07/11 11:45:34 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 7.1 KiB, free 434.1 MiB)
[2025-07-11T11:45:34.067+0200] {subprocess.py:93} INFO - 25/07/11 11:45:34 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 138.4.31.14:45609 (size: 7.1 KiB, free: 434.4 MiB)
[2025-07-11T11:45:34.067+0200] {subprocess.py:93} INFO - 25/07/11 11:45:34 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1585
[2025-07-11T11:45:34.069+0200] {subprocess.py:93} INFO - 25/07/11 11:45:34 INFO DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 1 (MapPartitionsRDD[4] at collect at StringIndexer.scala:204) (first 15 tasks are for partitions Vector(0, 1))
[2025-07-11T11:45:34.069+0200] {subprocess.py:93} INFO - 25/07/11 11:45:34 INFO TaskSchedulerImpl: Adding task set 1.0 with 2 tasks resource profile 0
[2025-07-11T11:45:34.074+0200] {subprocess.py:93} INFO - 25/07/11 11:45:34 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (138.4.31.14, executor driver, partition 0, PROCESS_LOCAL, 13595 bytes)
[2025-07-11T11:45:34.076+0200] {subprocess.py:93} INFO - 25/07/11 11:45:34 INFO TaskSetManager: Starting task 1.0 in stage 1.0 (TID 2) (138.4.31.14, executor driver, partition 1, PROCESS_LOCAL, 13595 bytes)
[2025-07-11T11:45:34.076+0200] {subprocess.py:93} INFO - 25/07/11 11:45:34 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)
[2025-07-11T11:45:34.094+0200] {subprocess.py:93} INFO - 25/07/11 11:45:34 INFO Executor: Running task 1.0 in stage 1.0 (TID 2)
[2025-07-11T11:45:34.439+0200] {subprocess.py:93} INFO - 25/07/11 11:45:34 INFO CodeGenerator: Code generated in 404.865361 ms
[2025-07-11T11:45:34.441+0200] {subprocess.py:93} INFO - 25/07/11 11:45:34 INFO CodeGenerator: Code generated in 271.550828 ms
[2025-07-11T11:45:34.459+0200] {subprocess.py:93} INFO - 25/07/11 11:45:34 INFO FileScanRDD: Reading File path: file:///home/monica.fernandez/practica_creativa/data/simple_flight_delay_features.jsonl.bz2, range: 0-4194304, partition values: [empty row]
[2025-07-11T11:45:34.463+0200] {subprocess.py:93} INFO - 25/07/11 11:45:34 INFO FileScanRDD: Reading File path: file:///home/monica.fernandez/practica_creativa/data/simple_flight_delay_features.jsonl.bz2, range: 4194304-4676447, partition values: [empty row]
[2025-07-11T11:45:34.478+0200] {subprocess.py:93} INFO - 25/07/11 11:45:34 INFO DAGScheduler: Registering RDD 8 (collect at StringIndexer.scala:204) as input to shuffle 1
[2025-07-11T11:45:34.478+0200] {subprocess.py:93} INFO - 25/07/11 11:45:34 INFO DAGScheduler: Got map stage job 2 (collect at StringIndexer.scala:204) with 12 output partitions
[2025-07-11T11:45:34.478+0200] {subprocess.py:93} INFO - 25/07/11 11:45:34 INFO DAGScheduler: Final stage: ShuffleMapStage 2 (collect at StringIndexer.scala:204)
[2025-07-11T11:45:34.478+0200] {subprocess.py:93} INFO - 25/07/11 11:45:34 INFO DAGScheduler: Parents of final stage: List()
[2025-07-11T11:45:34.479+0200] {subprocess.py:93} INFO - 25/07/11 11:45:34 INFO DAGScheduler: Missing parents: List()
[2025-07-11T11:45:34.479+0200] {subprocess.py:93} INFO - 25/07/11 11:45:34 INFO DAGScheduler: Submitting ShuffleMapStage 2 (MapPartitionsRDD[8] at collect at StringIndexer.scala:204), which has no missing parents
[2025-07-11T11:45:34.499+0200] {subprocess.py:93} INFO - 25/07/11 11:45:34 INFO CodeGenerator: Code generated in 31.131979 ms
[2025-07-11T11:45:34.553+0200] {subprocess.py:93} INFO - 25/07/11 11:45:34 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 18.2 KiB, free 434.1 MiB)
[2025-07-11T11:45:34.555+0200] {subprocess.py:93} INFO - 25/07/11 11:45:34 INFO CodecPool: Got brand-new decompressor [.bz2]
[2025-07-11T11:45:34.556+0200] {subprocess.py:93} INFO - 25/07/11 11:45:34 INFO CodecPool: Got brand-new decompressor [.bz2]
[2025-07-11T11:45:34.561+0200] {subprocess.py:93} INFO - 25/07/11 11:45:34 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 9.0 KiB, free 434.1 MiB)
[2025-07-11T11:45:34.562+0200] {subprocess.py:93} INFO - 25/07/11 11:45:34 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 138.4.31.14:45609 (size: 9.0 KiB, free: 434.3 MiB)
[2025-07-11T11:45:34.564+0200] {subprocess.py:93} INFO - 25/07/11 11:45:34 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1585
[2025-07-11T11:45:34.565+0200] {subprocess.py:93} INFO - 25/07/11 11:45:34 INFO DAGScheduler: Submitting 12 missing tasks from ShuffleMapStage 2 (MapPartitionsRDD[8] at collect at StringIndexer.scala:204) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11))
[2025-07-11T11:45:34.565+0200] {subprocess.py:93} INFO - 25/07/11 11:45:34 INFO TaskSchedulerImpl: Adding task set 2.0 with 12 tasks resource profile 0
[2025-07-11T11:45:34.580+0200] {subprocess.py:93} INFO - 25/07/11 11:45:34 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 3) (138.4.31.14, executor driver, partition 0, ANY, 14843 bytes)
[2025-07-11T11:45:34.581+0200] {subprocess.py:93} INFO - 25/07/11 11:45:34 INFO TaskSetManager: Starting task 1.0 in stage 2.0 (TID 4) (138.4.31.14, executor driver, partition 1, ANY, 14843 bytes)
[2025-07-11T11:45:34.584+0200] {subprocess.py:93} INFO - 25/07/11 11:45:34 INFO Executor: Running task 0.0 in stage 2.0 (TID 3)
[2025-07-11T11:45:34.585+0200] {subprocess.py:93} INFO - 25/07/11 11:45:34 INFO Executor: Running task 1.0 in stage 2.0 (TID 4)
[2025-07-11T11:45:34.630+0200] {subprocess.py:93} INFO - 25/07/11 11:45:34 INFO CodeGenerator: Code generated in 10.426186 ms
[2025-07-11T11:45:34.662+0200] {subprocess.py:93} INFO - 25/07/11 11:45:34 INFO CodeGenerator: Code generated in 24.201401 ms
[2025-07-11T11:45:35.565+0200] {subprocess.py:93} INFO - 25/07/11 11:45:35 INFO CodeGenerator: Code generated in 16.920014 ms
[2025-07-11T11:45:35.800+0200] {subprocess.py:93} INFO - 25/07/11 11:45:35 INFO Executor: Finished task 0.0 in stage 2.0 (TID 3). 2031 bytes result sent to driver
[2025-07-11T11:45:35.803+0200] {subprocess.py:93} INFO - 25/07/11 11:45:35 INFO Executor: Finished task 1.0 in stage 2.0 (TID 4). 2031 bytes result sent to driver
[2025-07-11T11:45:35.810+0200] {subprocess.py:93} INFO - 25/07/11 11:45:35 INFO TaskSetManager: Starting task 2.0 in stage 2.0 (TID 5) (138.4.31.14, executor driver, partition 2, ANY, 14843 bytes)
[2025-07-11T11:45:35.819+0200] {subprocess.py:93} INFO - 25/07/11 11:45:35 INFO Executor: Running task 2.0 in stage 2.0 (TID 5)
[2025-07-11T11:45:35.820+0200] {subprocess.py:93} INFO - 25/07/11 11:45:35 INFO TaskSetManager: Starting task 3.0 in stage 2.0 (TID 6) (138.4.31.14, executor driver, partition 3, ANY, 14843 bytes)
[2025-07-11T11:45:35.820+0200] {subprocess.py:93} INFO - 25/07/11 11:45:35 INFO Executor: Running task 3.0 in stage 2.0 (TID 6)
[2025-07-11T11:45:35.829+0200] {subprocess.py:93} INFO - 25/07/11 11:45:35 INFO TaskSetManager: Finished task 1.0 in stage 2.0 (TID 4) in 1249 ms on 138.4.31.14 (executor driver) (1/12)
[2025-07-11T11:45:35.849+0200] {subprocess.py:93} INFO - 25/07/11 11:45:35 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 3) in 1281 ms on 138.4.31.14 (executor driver) (2/12)
[2025-07-11T11:45:35.947+0200] {subprocess.py:93} INFO - 25/07/11 11:45:35 INFO Executor: Finished task 2.0 in stage 2.0 (TID 5). 2031 bytes result sent to driver
[2025-07-11T11:45:35.955+0200] {subprocess.py:93} INFO - 25/07/11 11:45:35 INFO TaskSetManager: Starting task 4.0 in stage 2.0 (TID 7) (138.4.31.14, executor driver, partition 4, ANY, 14963 bytes)
[2025-07-11T11:45:35.956+0200] {subprocess.py:93} INFO - 25/07/11 11:45:35 INFO Executor: Running task 4.0 in stage 2.0 (TID 7)
[2025-07-11T11:45:35.958+0200] {subprocess.py:93} INFO - 25/07/11 11:45:35 INFO TaskSetManager: Finished task 2.0 in stage 2.0 (TID 5) in 149 ms on 138.4.31.14 (executor driver) (3/12)
[2025-07-11T11:45:35.967+0200] {subprocess.py:93} INFO - 25/07/11 11:45:35 INFO Executor: Finished task 3.0 in stage 2.0 (TID 6). 2031 bytes result sent to driver
[2025-07-11T11:45:35.968+0200] {subprocess.py:93} INFO - 25/07/11 11:45:35 INFO TaskSetManager: Starting task 5.0 in stage 2.0 (TID 8) (138.4.31.14, executor driver, partition 5, ANY, 14963 bytes)
[2025-07-11T11:45:35.968+0200] {subprocess.py:93} INFO - 25/07/11 11:45:35 INFO TaskSetManager: Finished task 3.0 in stage 2.0 (TID 6) in 154 ms on 138.4.31.14 (executor driver) (4/12)
[2025-07-11T11:45:35.973+0200] {subprocess.py:93} INFO - 25/07/11 11:45:35 INFO Executor: Running task 5.0 in stage 2.0 (TID 8)
[2025-07-11T11:45:36.093+0200] {subprocess.py:93} INFO - 25/07/11 11:45:36 INFO Executor: Finished task 5.0 in stage 2.0 (TID 8). 2031 bytes result sent to driver
[2025-07-11T11:45:36.099+0200] {subprocess.py:93} INFO - 25/07/11 11:45:36 INFO TaskSetManager: Starting task 6.0 in stage 2.0 (TID 9) (138.4.31.14, executor driver, partition 6, ANY, 14843 bytes)
[2025-07-11T11:45:36.099+0200] {subprocess.py:93} INFO - 25/07/11 11:45:36 INFO Executor: Running task 6.0 in stage 2.0 (TID 9)
[2025-07-11T11:45:36.099+0200] {subprocess.py:93} INFO - 25/07/11 11:45:36 INFO TaskSetManager: Finished task 5.0 in stage 2.0 (TID 8) in 132 ms on 138.4.31.14 (executor driver) (5/12)
[2025-07-11T11:45:36.100+0200] {subprocess.py:93} INFO - 25/07/11 11:45:36 INFO Executor: Finished task 4.0 in stage 2.0 (TID 7). 2031 bytes result sent to driver
[2025-07-11T11:45:36.104+0200] {subprocess.py:93} INFO - 25/07/11 11:45:36 INFO TaskSetManager: Starting task 7.0 in stage 2.0 (TID 10) (138.4.31.14, executor driver, partition 7, ANY, 14843 bytes)
[2025-07-11T11:45:36.105+0200] {subprocess.py:93} INFO - 25/07/11 11:45:36 INFO TaskSetManager: Finished task 4.0 in stage 2.0 (TID 7) in 150 ms on 138.4.31.14 (executor driver) (6/12)
[2025-07-11T11:45:36.107+0200] {subprocess.py:93} INFO - 25/07/11 11:45:36 INFO Executor: Running task 7.0 in stage 2.0 (TID 10)
[2025-07-11T11:45:36.216+0200] {subprocess.py:93} INFO - 25/07/11 11:45:36 INFO Executor: Finished task 7.0 in stage 2.0 (TID 10). 2031 bytes result sent to driver
[2025-07-11T11:45:36.217+0200] {subprocess.py:93} INFO - 25/07/11 11:45:36 INFO TaskSetManager: Starting task 8.0 in stage 2.0 (TID 11) (138.4.31.14, executor driver, partition 8, ANY, 14843 bytes)
[2025-07-11T11:45:36.218+0200] {subprocess.py:93} INFO - 25/07/11 11:45:36 INFO TaskSetManager: Finished task 7.0 in stage 2.0 (TID 10) in 115 ms on 138.4.31.14 (executor driver) (7/12)
[2025-07-11T11:45:36.220+0200] {subprocess.py:93} INFO - 25/07/11 11:45:36 INFO Executor: Finished task 6.0 in stage 2.0 (TID 9). 1988 bytes result sent to driver
[2025-07-11T11:45:36.221+0200] {subprocess.py:93} INFO - 25/07/11 11:45:36 INFO Executor: Running task 8.0 in stage 2.0 (TID 11)
[2025-07-11T11:45:36.221+0200] {subprocess.py:93} INFO - 25/07/11 11:45:36 INFO TaskSetManager: Starting task 9.0 in stage 2.0 (TID 12) (138.4.31.14, executor driver, partition 9, ANY, 14843 bytes)
[2025-07-11T11:45:36.222+0200] {subprocess.py:93} INFO - 25/07/11 11:45:36 INFO Executor: Running task 9.0 in stage 2.0 (TID 12)
[2025-07-11T11:45:36.222+0200] {subprocess.py:93} INFO - 25/07/11 11:45:36 INFO TaskSetManager: Finished task 6.0 in stage 2.0 (TID 9) in 124 ms on 138.4.31.14 (executor driver) (8/12)
[2025-07-11T11:45:36.380+0200] {subprocess.py:93} INFO - 25/07/11 11:45:36 INFO Executor: Finished task 9.0 in stage 2.0 (TID 12). 1945 bytes result sent to driver
[2025-07-11T11:45:36.380+0200] {subprocess.py:93} INFO - 25/07/11 11:45:36 INFO TaskSetManager: Starting task 10.0 in stage 2.0 (TID 13) (138.4.31.14, executor driver, partition 10, ANY, 14963 bytes)
[2025-07-11T11:45:36.380+0200] {subprocess.py:93} INFO - 25/07/11 11:45:36 INFO TaskSetManager: Finished task 9.0 in stage 2.0 (TID 12) in 159 ms on 138.4.31.14 (executor driver) (9/12)
[2025-07-11T11:45:36.387+0200] {subprocess.py:93} INFO - 25/07/11 11:45:36 INFO Executor: Running task 10.0 in stage 2.0 (TID 13)
[2025-07-11T11:45:36.407+0200] {subprocess.py:93} INFO - 25/07/11 11:45:36 INFO Executor: Finished task 8.0 in stage 2.0 (TID 11). 1945 bytes result sent to driver
[2025-07-11T11:45:36.408+0200] {subprocess.py:93} INFO - 25/07/11 11:45:36 INFO TaskSetManager: Starting task 11.0 in stage 2.0 (TID 14) (138.4.31.14, executor driver, partition 11, ANY, 14959 bytes)
[2025-07-11T11:45:36.409+0200] {subprocess.py:93} INFO - 25/07/11 11:45:36 INFO TaskSetManager: Finished task 8.0 in stage 2.0 (TID 11) in 193 ms on 138.4.31.14 (executor driver) (10/12)
[2025-07-11T11:45:36.410+0200] {subprocess.py:93} INFO - 25/07/11 11:45:36 INFO Executor: Running task 11.0 in stage 2.0 (TID 14)
[2025-07-11T11:45:36.581+0200] {subprocess.py:93} INFO - 25/07/11 11:45:36 INFO Executor: Finished task 10.0 in stage 2.0 (TID 13). 1945 bytes result sent to driver
[2025-07-11T11:45:36.583+0200] {subprocess.py:93} INFO - 25/07/11 11:45:36 INFO TaskSetManager: Finished task 10.0 in stage 2.0 (TID 13) in 205 ms on 138.4.31.14 (executor driver) (11/12)
[2025-07-11T11:45:36.597+0200] {subprocess.py:93} INFO - 25/07/11 11:45:36 INFO Executor: Finished task 11.0 in stage 2.0 (TID 14). 1988 bytes result sent to driver
[2025-07-11T11:45:36.597+0200] {subprocess.py:93} INFO - 25/07/11 11:45:36 INFO TaskSetManager: Finished task 11.0 in stage 2.0 (TID 14) in 190 ms on 138.4.31.14 (executor driver) (12/12)
[2025-07-11T11:45:36.598+0200] {subprocess.py:93} INFO - 25/07/11 11:45:36 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool
[2025-07-11T11:45:36.598+0200] {subprocess.py:93} INFO - 25/07/11 11:45:36 INFO DAGScheduler: ShuffleMapStage 2 (collect at StringIndexer.scala:204) finished in 2,116 s
[2025-07-11T11:45:36.599+0200] {subprocess.py:93} INFO - 25/07/11 11:45:36 INFO DAGScheduler: looking for newly runnable stages
[2025-07-11T11:45:36.599+0200] {subprocess.py:93} INFO - 25/07/11 11:45:36 INFO DAGScheduler: running: Set(ShuffleMapStage 1)
[2025-07-11T11:45:36.600+0200] {subprocess.py:93} INFO - 25/07/11 11:45:36 INFO DAGScheduler: waiting: Set()
[2025-07-11T11:45:36.600+0200] {subprocess.py:93} INFO - 25/07/11 11:45:36 INFO DAGScheduler: failed: Set()
[2025-07-11T11:45:36.677+0200] {subprocess.py:93} INFO - 25/07/11 11:45:36 INFO ShufflePartitionsUtil: For shuffle(1), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576
[2025-07-11T11:45:36.808+0200] {subprocess.py:93} INFO - 25/07/11 11:45:36 INFO SparkContext: Starting job: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264
[2025-07-11T11:45:36.810+0200] {subprocess.py:93} INFO - 25/07/11 11:45:36 INFO DAGScheduler: Got job 3 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) with 1 output partitions
[2025-07-11T11:45:36.810+0200] {subprocess.py:93} INFO - 25/07/11 11:45:36 INFO DAGScheduler: Final stage: ResultStage 4 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264)
[2025-07-11T11:45:36.810+0200] {subprocess.py:93} INFO - 25/07/11 11:45:36 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 3)
[2025-07-11T11:45:36.810+0200] {subprocess.py:93} INFO - 25/07/11 11:45:36 INFO DAGScheduler: Missing parents: List()
[2025-07-11T11:45:36.811+0200] {subprocess.py:93} INFO - 25/07/11 11:45:36 INFO DAGScheduler: Submitting ResultStage 4 (MapPartitionsRDD[10] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264), which has no missing parents
[2025-07-11T11:45:36.821+0200] {subprocess.py:93} INFO - 25/07/11 11:45:36 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 8.2 KiB, free 434.1 MiB)
[2025-07-11T11:45:36.825+0200] {subprocess.py:93} INFO - 25/07/11 11:45:36 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 4.2 KiB, free 434.1 MiB)
[2025-07-11T11:45:36.826+0200] {subprocess.py:93} INFO - 25/07/11 11:45:36 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 138.4.31.14:45609 (size: 4.2 KiB, free: 434.3 MiB)
[2025-07-11T11:45:36.828+0200] {subprocess.py:93} INFO - 25/07/11 11:45:36 INFO SparkContext: Created broadcast 4 from broadcast at DAGScheduler.scala:1585
[2025-07-11T11:45:36.831+0200] {subprocess.py:93} INFO - 25/07/11 11:45:36 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 4 (MapPartitionsRDD[10] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) (first 15 tasks are for partitions Vector(0))
[2025-07-11T11:45:36.831+0200] {subprocess.py:93} INFO - 25/07/11 11:45:36 INFO TaskSchedulerImpl: Adding task set 4.0 with 1 tasks resource profile 0
[2025-07-11T11:45:36.836+0200] {subprocess.py:93} INFO - 25/07/11 11:45:36 INFO TaskSetManager: Starting task 0.0 in stage 4.0 (TID 15) (138.4.31.14, executor driver, partition 0, NODE_LOCAL, 12968 bytes)
[2025-07-11T11:45:36.837+0200] {subprocess.py:93} INFO - 25/07/11 11:45:36 INFO Executor: Running task 0.0 in stage 4.0 (TID 15)
[2025-07-11T11:45:36.947+0200] {subprocess.py:93} INFO - 25/07/11 11:45:36 INFO ShuffleBlockFetcherIterator: Getting 12 (193.5 KiB) non-empty blocks including 12 (193.5 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2025-07-11T11:45:36.949+0200] {subprocess.py:93} INFO - 25/07/11 11:45:36 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 37 ms
[2025-07-11T11:45:37.071+0200] {subprocess.py:93} INFO - 25/07/11 11:45:37 INFO Executor: Finished task 0.0 in stage 4.0 (TID 15). 40800 bytes result sent to driver
[2025-07-11T11:45:37.071+0200] {subprocess.py:93} INFO - 25/07/11 11:45:37 INFO TaskSetManager: Finished task 0.0 in stage 4.0 (TID 15) in 232 ms on 138.4.31.14 (executor driver) (1/1)
[2025-07-11T11:45:37.071+0200] {subprocess.py:93} INFO - 25/07/11 11:45:37 INFO TaskSchedulerImpl: Removed TaskSet 4.0, whose tasks have all completed, from pool
[2025-07-11T11:45:37.071+0200] {subprocess.py:93} INFO - 25/07/11 11:45:37 INFO DAGScheduler: ResultStage 4 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) finished in 0,251 s
[2025-07-11T11:45:37.071+0200] {subprocess.py:93} INFO - 25/07/11 11:45:37 INFO DAGScheduler: Job 3 is finished. Cancelling potential speculative or zombie tasks for this job
[2025-07-11T11:45:37.072+0200] {subprocess.py:93} INFO - 25/07/11 11:45:37 INFO TaskSchedulerImpl: Killing all running tasks in stage 4: Stage finished
[2025-07-11T11:45:37.072+0200] {subprocess.py:93} INFO - 25/07/11 11:45:37 INFO DAGScheduler: Job 3 finished: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264, took 0,260663 s
[2025-07-11T11:45:37.110+0200] {subprocess.py:93} INFO - 25/07/11 11:45:37 INFO Executor: Finished task 1.0 in stage 1.0 (TID 2). 1959 bytes result sent to driver
[2025-07-11T11:45:37.111+0200] {subprocess.py:93} INFO - 25/07/11 11:45:37 INFO TaskSetManager: Finished task 1.0 in stage 1.0 (TID 2) in 3036 ms on 138.4.31.14 (executor driver) (1/2)
[2025-07-11T11:45:37.124+0200] {subprocess.py:93} INFO - 25/07/11 11:45:37 INFO CodeGenerator: Code generated in 16.653535 ms
[2025-07-11T11:45:37.174+0200] {subprocess.py:93} INFO - 25/07/11 11:45:37 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 4.3 MiB, free 429.8 MiB)
[2025-07-11T11:45:37.194+0200] {subprocess.py:93} INFO - 25/07/11 11:45:37 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 90.0 KiB, free 429.8 MiB)
[2025-07-11T11:45:37.194+0200] {subprocess.py:93} INFO - 25/07/11 11:45:37 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 138.4.31.14:45609 (size: 90.0 KiB, free: 434.3 MiB)
[2025-07-11T11:45:37.195+0200] {subprocess.py:93} INFO - 25/07/11 11:45:37 INFO SparkContext: Created broadcast 5 from $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264
[2025-07-11T11:45:38.134+0200] {subprocess.py:93} INFO - 25/07/11 11:45:38 INFO BlockManagerInfo: Removed broadcast_4_piece0 on 138.4.31.14:45609 in memory (size: 4.2 KiB, free: 434.3 MiB)
[2025-07-11T11:45:43.007+0200] {subprocess.py:93} INFO - 25/07/11 11:45:43 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 1959 bytes result sent to driver
[2025-07-11T11:45:43.008+0200] {subprocess.py:93} INFO - 25/07/11 11:45:43 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 8937 ms on 138.4.31.14 (executor driver) (2/2)
[2025-07-11T11:45:43.008+0200] {subprocess.py:93} INFO - 25/07/11 11:45:43 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool
[2025-07-11T11:45:43.008+0200] {subprocess.py:93} INFO - 25/07/11 11:45:43 INFO DAGScheduler: ShuffleMapStage 1 (collect at StringIndexer.scala:204) finished in 9,023 s
[2025-07-11T11:45:43.009+0200] {subprocess.py:93} INFO - 25/07/11 11:45:43 INFO DAGScheduler: looking for newly runnable stages
[2025-07-11T11:45:43.009+0200] {subprocess.py:93} INFO - 25/07/11 11:45:43 INFO DAGScheduler: running: Set()
[2025-07-11T11:45:43.009+0200] {subprocess.py:93} INFO - 25/07/11 11:45:43 INFO DAGScheduler: waiting: Set()
[2025-07-11T11:45:43.009+0200] {subprocess.py:93} INFO - 25/07/11 11:45:43 INFO DAGScheduler: failed: Set()
[2025-07-11T11:45:43.016+0200] {subprocess.py:93} INFO - 25/07/11 11:45:43 INFO ShufflePartitionsUtil: For shuffle(0), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576
[2025-07-11T11:45:43.045+0200] {subprocess.py:93} INFO - 25/07/11 11:45:43 INFO CodeGenerator: Code generated in 11.277145 ms
[2025-07-11T11:45:43.054+0200] {subprocess.py:93} INFO - 25/07/11 11:45:43 INFO DAGScheduler: Registering RDD 14 (collect at StringIndexer.scala:204) as input to shuffle 2
[2025-07-11T11:45:43.055+0200] {subprocess.py:93} INFO - 25/07/11 11:45:43 INFO DAGScheduler: Got map stage job 4 (collect at StringIndexer.scala:204) with 1 output partitions
[2025-07-11T11:45:43.055+0200] {subprocess.py:93} INFO - 25/07/11 11:45:43 INFO DAGScheduler: Final stage: ShuffleMapStage 6 (collect at StringIndexer.scala:204)
[2025-07-11T11:45:43.055+0200] {subprocess.py:93} INFO - 25/07/11 11:45:43 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 5)
[2025-07-11T11:45:43.055+0200] {subprocess.py:93} INFO - 25/07/11 11:45:43 INFO DAGScheduler: Missing parents: List()
[2025-07-11T11:45:43.055+0200] {subprocess.py:93} INFO - 25/07/11 11:45:43 INFO DAGScheduler: Submitting ShuffleMapStage 6 (MapPartitionsRDD[14] at collect at StringIndexer.scala:204), which has no missing parents
[2025-07-11T11:45:43.087+0200] {subprocess.py:93} INFO - 25/07/11 11:45:43 INFO MemoryStore: Block broadcast_6 stored as values in memory (estimated size 49.5 KiB, free 429.7 MiB)
[2025-07-11T11:45:43.088+0200] {subprocess.py:93} INFO - 25/07/11 11:45:43 INFO MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 23.5 KiB, free 429.7 MiB)
[2025-07-11T11:45:43.088+0200] {subprocess.py:93} INFO - 25/07/11 11:45:43 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on 138.4.31.14:45609 (size: 23.5 KiB, free: 434.2 MiB)
[2025-07-11T11:45:43.088+0200] {subprocess.py:93} INFO - 25/07/11 11:45:43 INFO SparkContext: Created broadcast 6 from broadcast at DAGScheduler.scala:1585
[2025-07-11T11:45:43.089+0200] {subprocess.py:93} INFO - 25/07/11 11:45:43 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 6 (MapPartitionsRDD[14] at collect at StringIndexer.scala:204) (first 15 tasks are for partitions Vector(0))
[2025-07-11T11:45:43.089+0200] {subprocess.py:93} INFO - 25/07/11 11:45:43 INFO TaskSchedulerImpl: Adding task set 6.0 with 1 tasks resource profile 0
[2025-07-11T11:45:43.091+0200] {subprocess.py:93} INFO - 25/07/11 11:45:43 INFO TaskSetManager: Starting task 0.0 in stage 6.0 (TID 16) (138.4.31.14, executor driver, partition 0, NODE_LOCAL, 12957 bytes)
[2025-07-11T11:45:43.092+0200] {subprocess.py:93} INFO - 25/07/11 11:45:43 INFO Executor: Running task 0.0 in stage 6.0 (TID 16)
[2025-07-11T11:45:43.106+0200] {subprocess.py:93} INFO - 25/07/11 11:45:43 INFO ShuffleBlockFetcherIterator: Getting 2 (1126.9 KiB) non-empty blocks including 2 (1126.9 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2025-07-11T11:45:43.107+0200] {subprocess.py:93} INFO - 25/07/11 11:45:43 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2025-07-11T11:45:43.118+0200] {subprocess.py:93} INFO - 25/07/11 11:45:43 INFO CodeGenerator: Code generated in 10.965865 ms
[2025-07-11T11:45:43.148+0200] {subprocess.py:93} INFO - 25/07/11 11:45:43 INFO CodeGenerator: Code generated in 5.534246 ms
[2025-07-11T11:45:43.161+0200] {subprocess.py:93} INFO - 25/07/11 11:45:43 INFO CodeGenerator: Code generated in 5.648563 ms
[2025-07-11T11:45:43.174+0200] {subprocess.py:93} INFO - 25/07/11 11:45:43 INFO CodeGenerator: Code generated in 10.097329 ms
[2025-07-11T11:45:43.224+0200] {subprocess.py:93} INFO - 25/07/11 11:45:43 INFO CodeGenerator: Code generated in 11.45984 ms
[2025-07-11T11:45:43.751+0200] {subprocess.py:93} INFO - 25/07/11 11:45:43 INFO CodeGenerator: Code generated in 16.572043 ms
[2025-07-11T11:45:43.986+0200] {subprocess.py:93} INFO - 25/07/11 11:45:43 INFO Executor: Finished task 0.0 in stage 6.0 (TID 16). 6487 bytes result sent to driver
[2025-07-11T11:45:43.990+0200] {subprocess.py:93} INFO - 25/07/11 11:45:43 INFO TaskSetManager: Finished task 0.0 in stage 6.0 (TID 16) in 899 ms on 138.4.31.14 (executor driver) (1/1)
[2025-07-11T11:45:43.990+0200] {subprocess.py:93} INFO - 25/07/11 11:45:43 INFO TaskSchedulerImpl: Removed TaskSet 6.0, whose tasks have all completed, from pool
[2025-07-11T11:45:43.991+0200] {subprocess.py:93} INFO - 25/07/11 11:45:43 INFO DAGScheduler: ShuffleMapStage 6 (collect at StringIndexer.scala:204) finished in 0,934 s
[2025-07-11T11:45:43.991+0200] {subprocess.py:93} INFO - 25/07/11 11:45:43 INFO DAGScheduler: looking for newly runnable stages
[2025-07-11T11:45:43.991+0200] {subprocess.py:93} INFO - 25/07/11 11:45:43 INFO DAGScheduler: running: Set()
[2025-07-11T11:45:43.991+0200] {subprocess.py:93} INFO - 25/07/11 11:45:43 INFO DAGScheduler: waiting: Set()
[2025-07-11T11:45:43.991+0200] {subprocess.py:93} INFO - 25/07/11 11:45:43 INFO DAGScheduler: failed: Set()
[2025-07-11T11:45:44.027+0200] {subprocess.py:93} INFO - 25/07/11 11:45:44 INFO SparkContext: Starting job: collect at StringIndexer.scala:204
[2025-07-11T11:45:44.028+0200] {subprocess.py:93} INFO - 25/07/11 11:45:44 INFO DAGScheduler: Got job 5 (collect at StringIndexer.scala:204) with 1 output partitions
[2025-07-11T11:45:44.028+0200] {subprocess.py:93} INFO - 25/07/11 11:45:44 INFO DAGScheduler: Final stage: ResultStage 9 (collect at StringIndexer.scala:204)
[2025-07-11T11:45:44.028+0200] {subprocess.py:93} INFO - 25/07/11 11:45:44 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 8)
[2025-07-11T11:45:44.028+0200] {subprocess.py:93} INFO - 25/07/11 11:45:44 INFO DAGScheduler: Missing parents: List()
[2025-07-11T11:45:44.028+0200] {subprocess.py:93} INFO - 25/07/11 11:45:44 INFO DAGScheduler: Submitting ResultStage 9 (MapPartitionsRDD[17] at collect at StringIndexer.scala:204), which has no missing parents
[2025-07-11T11:45:44.036+0200] {subprocess.py:93} INFO - 25/07/11 11:45:44 INFO MemoryStore: Block broadcast_7 stored as values in memory (estimated size 48.9 KiB, free 429.7 MiB)
[2025-07-11T11:45:44.037+0200] {subprocess.py:93} INFO - 25/07/11 11:45:44 INFO MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 23.4 KiB, free 429.6 MiB)
[2025-07-11T11:45:44.037+0200] {subprocess.py:93} INFO - 25/07/11 11:45:44 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on 138.4.31.14:45609 (size: 23.4 KiB, free: 434.2 MiB)
[2025-07-11T11:45:44.038+0200] {subprocess.py:93} INFO - 25/07/11 11:45:44 INFO SparkContext: Created broadcast 7 from broadcast at DAGScheduler.scala:1585
[2025-07-11T11:45:44.038+0200] {subprocess.py:93} INFO - 25/07/11 11:45:44 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 9 (MapPartitionsRDD[17] at collect at StringIndexer.scala:204) (first 15 tasks are for partitions Vector(0))
[2025-07-11T11:45:44.038+0200] {subprocess.py:93} INFO - 25/07/11 11:45:44 INFO TaskSchedulerImpl: Adding task set 9.0 with 1 tasks resource profile 0
[2025-07-11T11:45:44.040+0200] {subprocess.py:93} INFO - 25/07/11 11:45:44 INFO TaskSetManager: Starting task 0.0 in stage 9.0 (TID 17) (138.4.31.14, executor driver, partition 0, NODE_LOCAL, 12952 bytes)
[2025-07-11T11:45:44.040+0200] {subprocess.py:93} INFO - 25/07/11 11:45:44 INFO Executor: Running task 0.0 in stage 9.0 (TID 17)
[2025-07-11T11:45:44.051+0200] {subprocess.py:93} INFO - 25/07/11 11:45:44 INFO ShuffleBlockFetcherIterator: Getting 1 (445.0 B) non-empty blocks including 1 (445.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2025-07-11T11:45:44.051+0200] {subprocess.py:93} INFO - 25/07/11 11:45:44 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2025-07-11T11:45:44.070+0200] {subprocess.py:93} INFO - 25/07/11 11:45:44 INFO CodeGenerator: Code generated in 9.574927 ms
[2025-07-11T11:45:44.115+0200] {subprocess.py:93} INFO - 25/07/11 11:45:44 INFO Executor: Finished task 0.0 in stage 9.0 (TID 17). 8089 bytes result sent to driver
[2025-07-11T11:45:44.119+0200] {subprocess.py:93} INFO - 25/07/11 11:45:44 INFO TaskSetManager: Finished task 0.0 in stage 9.0 (TID 17) in 80 ms on 138.4.31.14 (executor driver) (1/1)
[2025-07-11T11:45:44.119+0200] {subprocess.py:93} INFO - 25/07/11 11:45:44 INFO TaskSchedulerImpl: Removed TaskSet 9.0, whose tasks have all completed, from pool
[2025-07-11T11:45:44.121+0200] {subprocess.py:93} INFO - 25/07/11 11:45:44 INFO DAGScheduler: ResultStage 9 (collect at StringIndexer.scala:204) finished in 0,086 s
[2025-07-11T11:45:44.121+0200] {subprocess.py:93} INFO - 25/07/11 11:45:44 INFO DAGScheduler: Job 5 is finished. Cancelling potential speculative or zombie tasks for this job
[2025-07-11T11:45:44.121+0200] {subprocess.py:93} INFO - 25/07/11 11:45:44 INFO TaskSchedulerImpl: Killing all running tasks in stage 9: Stage finished
[2025-07-11T11:45:44.121+0200] {subprocess.py:93} INFO - 25/07/11 11:45:44 INFO DAGScheduler: Job 5 finished: collect at StringIndexer.scala:204, took 0,093185 s
[2025-07-11T11:45:44.134+0200] {subprocess.py:93} INFO - 25/07/11 11:45:44 INFO CodeGenerator: Code generated in 9.18419 ms
[2025-07-11T11:45:44.276+0200] {subprocess.py:93} INFO - 25/07/11 11:45:44 INFO FileSystemOverwrite: Path /home/monica.fernandez/practica_creativa/models/string_indexer_model_Carrier.bin already exists. It will be overwritten.
[2025-07-11T11:45:44.311+0200] {subprocess.py:93} INFO - 25/07/11 11:45:44 INFO HadoopMapRedCommitProtocol: Using output committer class org.apache.hadoop.mapred.FileOutputCommitter
[2025-07-11T11:45:44.311+0200] {subprocess.py:93} INFO - 25/07/11 11:45:44 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2025-07-11T11:45:44.311+0200] {subprocess.py:93} INFO - 25/07/11 11:45:44 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2025-07-11T11:45:44.406+0200] {subprocess.py:93} INFO - 25/07/11 11:45:44 INFO SparkContext: Starting job: runJob at SparkHadoopWriter.scala:83
[2025-07-11T11:45:44.406+0200] {subprocess.py:93} INFO - 25/07/11 11:45:44 INFO DAGScheduler: Got job 6 (runJob at SparkHadoopWriter.scala:83) with 1 output partitions
[2025-07-11T11:45:44.406+0200] {subprocess.py:93} INFO - 25/07/11 11:45:44 INFO DAGScheduler: Final stage: ResultStage 10 (runJob at SparkHadoopWriter.scala:83)
[2025-07-11T11:45:44.406+0200] {subprocess.py:93} INFO - 25/07/11 11:45:44 INFO DAGScheduler: Parents of final stage: List()
[2025-07-11T11:45:44.406+0200] {subprocess.py:93} INFO - 25/07/11 11:45:44 INFO DAGScheduler: Missing parents: List()
[2025-07-11T11:45:44.406+0200] {subprocess.py:93} INFO - 25/07/11 11:45:44 INFO DAGScheduler: Submitting ResultStage 10 (MapPartitionsRDD[19] at saveAsTextFile at ReadWrite.scala:413), which has no missing parents
[2025-07-11T11:45:44.429+0200] {subprocess.py:93} INFO - 25/07/11 11:45:44 INFO MemoryStore: Block broadcast_8 stored as values in memory (estimated size 101.3 KiB, free 429.5 MiB)
[2025-07-11T11:45:44.434+0200] {subprocess.py:93} INFO - 25/07/11 11:45:44 INFO MemoryStore: Block broadcast_8_piece0 stored as bytes in memory (estimated size 36.5 KiB, free 429.5 MiB)
[2025-07-11T11:45:44.436+0200] {subprocess.py:93} INFO - 25/07/11 11:45:44 INFO BlockManagerInfo: Added broadcast_8_piece0 in memory on 138.4.31.14:45609 (size: 36.5 KiB, free: 434.2 MiB)
[2025-07-11T11:45:44.437+0200] {subprocess.py:93} INFO - 25/07/11 11:45:44 INFO SparkContext: Created broadcast 8 from broadcast at DAGScheduler.scala:1585
[2025-07-11T11:45:44.438+0200] {subprocess.py:93} INFO - 25/07/11 11:45:44 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 10 (MapPartitionsRDD[19] at saveAsTextFile at ReadWrite.scala:413) (first 15 tasks are for partitions Vector(0))
[2025-07-11T11:45:44.438+0200] {subprocess.py:93} INFO - 25/07/11 11:45:44 INFO TaskSchedulerImpl: Adding task set 10.0 with 1 tasks resource profile 0
[2025-07-11T11:45:44.439+0200] {subprocess.py:93} INFO - 25/07/11 11:45:44 INFO TaskSetManager: Starting task 0.0 in stage 10.0 (TID 18) (138.4.31.14, executor driver, partition 0, PROCESS_LOCAL, 13282 bytes)
[2025-07-11T11:45:44.440+0200] {subprocess.py:93} INFO - 25/07/11 11:45:44 INFO Executor: Running task 0.0 in stage 10.0 (TID 18)
[2025-07-11T11:45:44.462+0200] {subprocess.py:93} INFO - 25/07/11 11:45:44 INFO HadoopMapRedCommitProtocol: Using output committer class org.apache.hadoop.mapred.FileOutputCommitter
[2025-07-11T11:45:44.463+0200] {subprocess.py:93} INFO - 25/07/11 11:45:44 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2025-07-11T11:45:44.463+0200] {subprocess.py:93} INFO - 25/07/11 11:45:44 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2025-07-11T11:45:44.502+0200] {subprocess.py:93} INFO - 25/07/11 11:45:44 INFO FileOutputCommitter: Saved output of task 'attempt_202507111145444699128117127318970_0019_m_000000_0' to file:/home/monica.fernandez/practica_creativa/models/string_indexer_model_Carrier.bin/metadata/_temporary/0/task_202507111145444699128117127318970_0019_m_000000
[2025-07-11T11:45:44.502+0200] {subprocess.py:93} INFO - 25/07/11 11:45:44 INFO SparkHadoopMapRedUtil: attempt_202507111145444699128117127318970_0019_m_000000_0: Committed. Elapsed time: 1 ms.
[2025-07-11T11:45:44.504+0200] {subprocess.py:93} INFO - 25/07/11 11:45:44 INFO Executor: Finished task 0.0 in stage 10.0 (TID 18). 1170 bytes result sent to driver
[2025-07-11T11:45:44.505+0200] {subprocess.py:93} INFO - 25/07/11 11:45:44 INFO TaskSetManager: Finished task 0.0 in stage 10.0 (TID 18) in 67 ms on 138.4.31.14 (executor driver) (1/1)
[2025-07-11T11:45:44.505+0200] {subprocess.py:93} INFO - 25/07/11 11:45:44 INFO TaskSchedulerImpl: Removed TaskSet 10.0, whose tasks have all completed, from pool
[2025-07-11T11:45:44.506+0200] {subprocess.py:93} INFO - 25/07/11 11:45:44 INFO DAGScheduler: ResultStage 10 (runJob at SparkHadoopWriter.scala:83) finished in 0,097 s
[2025-07-11T11:45:44.506+0200] {subprocess.py:93} INFO - 25/07/11 11:45:44 INFO DAGScheduler: Job 6 is finished. Cancelling potential speculative or zombie tasks for this job
[2025-07-11T11:45:44.506+0200] {subprocess.py:93} INFO - 25/07/11 11:45:44 INFO TaskSchedulerImpl: Killing all running tasks in stage 10: Stage finished
[2025-07-11T11:45:44.508+0200] {subprocess.py:93} INFO - 25/07/11 11:45:44 INFO DAGScheduler: Job 6 finished: runJob at SparkHadoopWriter.scala:83, took 0,102504 s
[2025-07-11T11:45:44.508+0200] {subprocess.py:93} INFO - 25/07/11 11:45:44 INFO SparkHadoopWriter: Start to commit write Job job_202507111145444699128117127318970_0019.
[2025-07-11T11:45:44.550+0200] {subprocess.py:93} INFO - 25/07/11 11:45:44 INFO SparkHadoopWriter: Write Job job_202507111145444699128117127318970_0019 committed. Elapsed time: 41 ms.
[2025-07-11T11:45:44.911+0200] {subprocess.py:93} INFO - 25/07/11 11:45:44 INFO CodeGenerator: Code generated in 18.76065 ms
[2025-07-11T11:45:44.928+0200] {subprocess.py:93} INFO - 25/07/11 11:45:44 INFO DAGScheduler: Registering RDD 22 (parquet at StringIndexer.scala:499) as input to shuffle 3
[2025-07-11T11:45:44.929+0200] {subprocess.py:93} INFO - 25/07/11 11:45:44 INFO DAGScheduler: Got map stage job 7 (parquet at StringIndexer.scala:499) with 1 output partitions
[2025-07-11T11:45:44.930+0200] {subprocess.py:93} INFO - 25/07/11 11:45:44 INFO DAGScheduler: Final stage: ShuffleMapStage 11 (parquet at StringIndexer.scala:499)
[2025-07-11T11:45:44.930+0200] {subprocess.py:93} INFO - 25/07/11 11:45:44 INFO DAGScheduler: Parents of final stage: List()
[2025-07-11T11:45:44.930+0200] {subprocess.py:93} INFO - 25/07/11 11:45:44 INFO DAGScheduler: Missing parents: List()
[2025-07-11T11:45:44.930+0200] {subprocess.py:93} INFO - 25/07/11 11:45:44 INFO DAGScheduler: Submitting ShuffleMapStage 11 (MapPartitionsRDD[22] at parquet at StringIndexer.scala:499), which has no missing parents
[2025-07-11T11:45:44.932+0200] {subprocess.py:93} INFO - 25/07/11 11:45:44 INFO MemoryStore: Block broadcast_9 stored as values in memory (estimated size 8.1 KiB, free 429.5 MiB)
[2025-07-11T11:45:44.935+0200] {subprocess.py:93} INFO - 25/07/11 11:45:44 INFO MemoryStore: Block broadcast_9_piece0 stored as bytes in memory (estimated size 4.4 KiB, free 429.5 MiB)
[2025-07-11T11:45:44.935+0200] {subprocess.py:93} INFO - 25/07/11 11:45:44 INFO BlockManagerInfo: Added broadcast_9_piece0 in memory on 138.4.31.14:45609 (size: 4.4 KiB, free: 434.2 MiB)
[2025-07-11T11:45:44.935+0200] {subprocess.py:93} INFO - 25/07/11 11:45:44 INFO SparkContext: Created broadcast 9 from broadcast at DAGScheduler.scala:1585
[2025-07-11T11:45:44.936+0200] {subprocess.py:93} INFO - 25/07/11 11:45:44 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 11 (MapPartitionsRDD[22] at parquet at StringIndexer.scala:499) (first 15 tasks are for partitions Vector(0))
[2025-07-11T11:45:44.936+0200] {subprocess.py:93} INFO - 25/07/11 11:45:44 INFO TaskSchedulerImpl: Adding task set 11.0 with 1 tasks resource profile 0
[2025-07-11T11:45:44.938+0200] {subprocess.py:93} INFO - 25/07/11 11:45:44 INFO TaskSetManager: Starting task 0.0 in stage 11.0 (TID 19) (138.4.31.14, executor driver, partition 0, PROCESS_LOCAL, 13380 bytes)
[2025-07-11T11:45:44.938+0200] {subprocess.py:93} INFO - 25/07/11 11:45:44 INFO Executor: Running task 0.0 in stage 11.0 (TID 19)
[2025-07-11T11:45:44.945+0200] {subprocess.py:93} INFO - 25/07/11 11:45:44 INFO Executor: Finished task 0.0 in stage 11.0 (TID 19). 1628 bytes result sent to driver
[2025-07-11T11:45:44.946+0200] {subprocess.py:93} INFO - 25/07/11 11:45:44 INFO TaskSetManager: Finished task 0.0 in stage 11.0 (TID 19) in 9 ms on 138.4.31.14 (executor driver) (1/1)
[2025-07-11T11:45:44.947+0200] {subprocess.py:93} INFO - 25/07/11 11:45:44 INFO TaskSchedulerImpl: Removed TaskSet 11.0, whose tasks have all completed, from pool
[2025-07-11T11:45:44.947+0200] {subprocess.py:93} INFO - 25/07/11 11:45:44 INFO DAGScheduler: ShuffleMapStage 11 (parquet at StringIndexer.scala:499) finished in 0,017 s
[2025-07-11T11:45:44.947+0200] {subprocess.py:93} INFO - 25/07/11 11:45:44 INFO DAGScheduler: looking for newly runnable stages
[2025-07-11T11:45:44.947+0200] {subprocess.py:93} INFO - 25/07/11 11:45:44 INFO DAGScheduler: running: Set()
[2025-07-11T11:45:44.947+0200] {subprocess.py:93} INFO - 25/07/11 11:45:44 INFO DAGScheduler: waiting: Set()
[2025-07-11T11:45:44.947+0200] {subprocess.py:93} INFO - 25/07/11 11:45:44 INFO DAGScheduler: failed: Set()
[2025-07-11T11:45:45.000+0200] {subprocess.py:93} INFO - 25/07/11 11:45:45 INFO ParquetUtils: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
[2025-07-11T11:45:45.019+0200] {subprocess.py:93} INFO - 25/07/11 11:45:45 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2025-07-11T11:45:45.019+0200] {subprocess.py:93} INFO - 25/07/11 11:45:45 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2025-07-11T11:45:45.021+0200] {subprocess.py:93} INFO - 25/07/11 11:45:45 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[2025-07-11T11:45:45.021+0200] {subprocess.py:93} INFO - 25/07/11 11:45:45 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2025-07-11T11:45:45.021+0200] {subprocess.py:93} INFO - 25/07/11 11:45:45 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2025-07-11T11:45:45.022+0200] {subprocess.py:93} INFO - 25/07/11 11:45:45 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[2025-07-11T11:45:45.043+0200] {subprocess.py:93} INFO - 25/07/11 11:45:45 INFO SparkContext: Starting job: parquet at StringIndexer.scala:499
[2025-07-11T11:45:45.044+0200] {subprocess.py:93} INFO - 25/07/11 11:45:45 INFO DAGScheduler: Got job 8 (parquet at StringIndexer.scala:499) with 1 output partitions
[2025-07-11T11:45:45.044+0200] {subprocess.py:93} INFO - 25/07/11 11:45:45 INFO DAGScheduler: Final stage: ResultStage 13 (parquet at StringIndexer.scala:499)
[2025-07-11T11:45:45.044+0200] {subprocess.py:93} INFO - 25/07/11 11:45:45 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 12)
[2025-07-11T11:45:45.044+0200] {subprocess.py:93} INFO - 25/07/11 11:45:45 INFO DAGScheduler: Missing parents: List()
[2025-07-11T11:45:45.045+0200] {subprocess.py:93} INFO - 25/07/11 11:45:45 INFO DAGScheduler: Submitting ResultStage 13 (MapPartitionsRDD[24] at parquet at StringIndexer.scala:499), which has no missing parents
[2025-07-11T11:45:45.068+0200] {subprocess.py:93} INFO - 25/07/11 11:45:45 INFO MemoryStore: Block broadcast_10 stored as values in memory (estimated size 230.2 KiB, free 429.3 MiB)
[2025-07-11T11:45:45.071+0200] {subprocess.py:93} INFO - 25/07/11 11:45:45 INFO MemoryStore: Block broadcast_10_piece0 stored as bytes in memory (estimated size 80.9 KiB, free 429.2 MiB)
[2025-07-11T11:45:45.071+0200] {subprocess.py:93} INFO - 25/07/11 11:45:45 INFO BlockManagerInfo: Added broadcast_10_piece0 in memory on 138.4.31.14:45609 (size: 80.9 KiB, free: 434.1 MiB)
[2025-07-11T11:45:45.072+0200] {subprocess.py:93} INFO - 25/07/11 11:45:45 INFO SparkContext: Created broadcast 10 from broadcast at DAGScheduler.scala:1585
[2025-07-11T11:45:45.072+0200] {subprocess.py:93} INFO - 25/07/11 11:45:45 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 13 (MapPartitionsRDD[24] at parquet at StringIndexer.scala:499) (first 15 tasks are for partitions Vector(0))
[2025-07-11T11:45:45.072+0200] {subprocess.py:93} INFO - 25/07/11 11:45:45 INFO TaskSchedulerImpl: Adding task set 13.0 with 1 tasks resource profile 0
[2025-07-11T11:45:45.073+0200] {subprocess.py:93} INFO - 25/07/11 11:45:45 INFO TaskSetManager: Starting task 0.0 in stage 13.0 (TID 20) (138.4.31.14, executor driver, partition 0, NODE_LOCAL, 12952 bytes)
[2025-07-11T11:45:45.074+0200] {subprocess.py:93} INFO - 25/07/11 11:45:45 INFO Executor: Running task 0.0 in stage 13.0 (TID 20)
[2025-07-11T11:45:45.094+0200] {subprocess.py:93} INFO - 25/07/11 11:45:45 INFO ShuffleBlockFetcherIterator: Getting 1 (228.0 B) non-empty blocks including 1 (228.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2025-07-11T11:45:45.095+0200] {subprocess.py:93} INFO - 25/07/11 11:45:45 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2025-07-11T11:45:45.096+0200] {subprocess.py:93} INFO - 25/07/11 11:45:45 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2025-07-11T11:45:45.096+0200] {subprocess.py:93} INFO - 25/07/11 11:45:45 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2025-07-11T11:45:45.097+0200] {subprocess.py:93} INFO - 25/07/11 11:45:45 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[2025-07-11T11:45:45.097+0200] {subprocess.py:93} INFO - 25/07/11 11:45:45 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2025-07-11T11:45:45.097+0200] {subprocess.py:93} INFO - 25/07/11 11:45:45 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2025-07-11T11:45:45.097+0200] {subprocess.py:93} INFO - 25/07/11 11:45:45 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[2025-07-11T11:45:45.103+0200] {subprocess.py:93} INFO - 25/07/11 11:45:45 INFO CodecConfig: Compression: SNAPPY
[2025-07-11T11:45:45.106+0200] {subprocess.py:93} INFO - 25/07/11 11:45:45 INFO CodecConfig: Compression: SNAPPY
[2025-07-11T11:45:45.131+0200] {subprocess.py:93} INFO - 25/07/11 11:45:45 INFO ParquetOutputFormat: ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]
[2025-07-11T11:45:45.159+0200] {subprocess.py:93} INFO - 25/07/11 11:45:45 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
[2025-07-11T11:45:45.160+0200] {subprocess.py:93} INFO - {
[2025-07-11T11:45:45.160+0200] {subprocess.py:93} INFO -   "type" : "struct",
[2025-07-11T11:45:45.160+0200] {subprocess.py:93} INFO -   "fields" : [ {
[2025-07-11T11:45:45.160+0200] {subprocess.py:93} INFO -     "name" : "labelsArray",
[2025-07-11T11:45:45.160+0200] {subprocess.py:93} INFO -     "type" : {
[2025-07-11T11:45:45.160+0200] {subprocess.py:93} INFO -       "type" : "array",
[2025-07-11T11:45:45.160+0200] {subprocess.py:93} INFO -       "elementType" : {
[2025-07-11T11:45:45.160+0200] {subprocess.py:93} INFO -         "type" : "array",
[2025-07-11T11:45:45.160+0200] {subprocess.py:93} INFO -         "elementType" : "string",
[2025-07-11T11:45:45.160+0200] {subprocess.py:93} INFO -         "containsNull" : true
[2025-07-11T11:45:45.160+0200] {subprocess.py:93} INFO -       },
[2025-07-11T11:45:45.160+0200] {subprocess.py:93} INFO -       "containsNull" : true
[2025-07-11T11:45:45.160+0200] {subprocess.py:93} INFO -     },
[2025-07-11T11:45:45.160+0200] {subprocess.py:93} INFO -     "nullable" : true,
[2025-07-11T11:45:45.161+0200] {subprocess.py:93} INFO -     "metadata" : { }
[2025-07-11T11:45:45.161+0200] {subprocess.py:93} INFO -   } ]
[2025-07-11T11:45:45.161+0200] {subprocess.py:93} INFO - }
[2025-07-11T11:45:45.161+0200] {subprocess.py:93} INFO - and corresponding Parquet message type:
[2025-07-11T11:45:45.161+0200] {subprocess.py:93} INFO - message spark_schema {
[2025-07-11T11:45:45.161+0200] {subprocess.py:93} INFO -   optional group labelsArray (LIST) {
[2025-07-11T11:45:45.161+0200] {subprocess.py:93} INFO -     repeated group list {
[2025-07-11T11:45:45.161+0200] {subprocess.py:93} INFO -       optional group element (LIST) {
[2025-07-11T11:45:45.161+0200] {subprocess.py:93} INFO -         repeated group list {
[2025-07-11T11:45:45.161+0200] {subprocess.py:93} INFO -           optional binary element (STRING);
[2025-07-11T11:45:45.161+0200] {subprocess.py:93} INFO -         }
[2025-07-11T11:45:45.161+0200] {subprocess.py:93} INFO -       }
[2025-07-11T11:45:45.161+0200] {subprocess.py:93} INFO -     }
[2025-07-11T11:45:45.162+0200] {subprocess.py:93} INFO -   }
[2025-07-11T11:45:45.162+0200] {subprocess.py:93} INFO - }
[2025-07-11T11:45:45.162+0200] {subprocess.py:93} INFO - 
[2025-07-11T11:45:45.162+0200] {subprocess.py:93} INFO - 
[2025-07-11T11:45:45.225+0200] {subprocess.py:93} INFO - 25/07/11 11:45:45 INFO CodecPool: Got brand-new compressor [.snappy]
[2025-07-11T11:45:45.705+0200] {subprocess.py:93} INFO - 25/07/11 11:45:45 INFO FileOutputCommitter: Saved output of task 'attempt_202507111145455976619656327544575_0013_m_000000_20' to file:/home/monica.fernandez/practica_creativa/models/string_indexer_model_Carrier.bin/data/_temporary/0/task_202507111145455976619656327544575_0013_m_000000
[2025-07-11T11:45:45.705+0200] {subprocess.py:93} INFO - 25/07/11 11:45:45 INFO SparkHadoopMapRedUtil: attempt_202507111145455976619656327544575_0013_m_000000_20: Committed. Elapsed time: 2 ms.
[2025-07-11T11:45:45.709+0200] {subprocess.py:93} INFO - 25/07/11 11:45:45 INFO Executor: Finished task 0.0 in stage 13.0 (TID 20). 4783 bytes result sent to driver
[2025-07-11T11:45:45.710+0200] {subprocess.py:93} INFO - 25/07/11 11:45:45 INFO TaskSetManager: Finished task 0.0 in stage 13.0 (TID 20) in 637 ms on 138.4.31.14 (executor driver) (1/1)
[2025-07-11T11:45:45.710+0200] {subprocess.py:93} INFO - 25/07/11 11:45:45 INFO TaskSchedulerImpl: Removed TaskSet 13.0, whose tasks have all completed, from pool
[2025-07-11T11:45:45.712+0200] {subprocess.py:93} INFO - 25/07/11 11:45:45 INFO DAGScheduler: ResultStage 13 (parquet at StringIndexer.scala:499) finished in 0,667 s
[2025-07-11T11:45:45.712+0200] {subprocess.py:93} INFO - 25/07/11 11:45:45 INFO DAGScheduler: Job 8 is finished. Cancelling potential speculative or zombie tasks for this job
[2025-07-11T11:45:45.712+0200] {subprocess.py:93} INFO - 25/07/11 11:45:45 INFO TaskSchedulerImpl: Killing all running tasks in stage 13: Stage finished
[2025-07-11T11:45:45.713+0200] {subprocess.py:93} INFO - 25/07/11 11:45:45 INFO DAGScheduler: Job 8 finished: parquet at StringIndexer.scala:499, took 0,669550 s
[2025-07-11T11:45:45.716+0200] {subprocess.py:93} INFO - 25/07/11 11:45:45 INFO FileFormatWriter: Start to commit write Job e7f23947-dc3b-4de4-a242-0c753b4da433.
[2025-07-11T11:45:45.738+0200] {subprocess.py:93} INFO - 25/07/11 11:45:45 INFO FileFormatWriter: Write Job e7f23947-dc3b-4de4-a242-0c753b4da433 committed. Elapsed time: 21 ms.
[2025-07-11T11:45:45.740+0200] {subprocess.py:93} INFO - 25/07/11 11:45:45 INFO FileFormatWriter: Finished processing stats for write job e7f23947-dc3b-4de4-a242-0c753b4da433.
[2025-07-11T11:45:45.793+0200] {subprocess.py:93} INFO - 25/07/11 11:45:45 INFO V2ScanRelationPushDown:
[2025-07-11T11:45:45.793+0200] {subprocess.py:93} INFO - Pushing operators to origin_dest_distances
[2025-07-11T11:45:45.793+0200] {subprocess.py:93} INFO - Pushed Filters:
[2025-07-11T11:45:45.793+0200] {subprocess.py:93} INFO - Post-Scan Filters:
[2025-07-11T11:45:45.793+0200] {subprocess.py:93} INFO - 
[2025-07-11T11:45:45.801+0200] {subprocess.py:93} INFO - 25/07/11 11:45:45 INFO V2ScanRelationPushDown:
[2025-07-11T11:45:45.801+0200] {subprocess.py:93} INFO - Output: origin#42, dest#43
[2025-07-11T11:45:45.801+0200] {subprocess.py:93} INFO - 
[2025-07-11T11:45:45.818+0200] {subprocess.py:93} INFO - 25/07/11 11:45:45 INFO FileSourceStrategy: Pushed Filters:
[2025-07-11T11:45:45.818+0200] {subprocess.py:93} INFO - 25/07/11 11:45:45 INFO FileSourceStrategy: Post-Scan Filters:
[2025-07-11T11:45:45.841+0200] {subprocess.py:93} INFO - 25/07/11 11:45:45 INFO MemoryStore: Block broadcast_11 stored as values in memory (estimated size 209.9 KiB, free 429.0 MiB)
[2025-07-11T11:45:45.851+0200] {subprocess.py:93} INFO - 25/07/11 11:45:45 INFO MemoryStore: Block broadcast_11_piece0 stored as bytes in memory (estimated size 37.7 KiB, free 428.9 MiB)
[2025-07-11T11:45:45.852+0200] {subprocess.py:93} INFO - 25/07/11 11:45:45 INFO BlockManagerInfo: Added broadcast_11_piece0 in memory on 138.4.31.14:45609 (size: 37.7 KiB, free: 434.1 MiB)
[2025-07-11T11:45:45.852+0200] {subprocess.py:93} INFO - 25/07/11 11:45:45 INFO SparkContext: Created broadcast 11 from collect at StringIndexer.scala:204
[2025-07-11T11:45:45.853+0200] {subprocess.py:93} INFO - 25/07/11 11:45:45 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
[2025-07-11T11:45:45.859+0200] {subprocess.py:93} INFO - 25/07/11 11:45:45 INFO DAGScheduler: Registering RDD 27 (collect at StringIndexer.scala:204) as input to shuffle 4
[2025-07-11T11:45:45.859+0200] {subprocess.py:93} INFO - 25/07/11 11:45:45 INFO DAGScheduler: Got map stage job 9 (collect at StringIndexer.scala:204) with 2 output partitions
[2025-07-11T11:45:45.860+0200] {subprocess.py:93} INFO - 25/07/11 11:45:45 INFO DAGScheduler: Final stage: ShuffleMapStage 14 (collect at StringIndexer.scala:204)
[2025-07-11T11:45:45.860+0200] {subprocess.py:93} INFO - 25/07/11 11:45:45 INFO DAGScheduler: Parents of final stage: List()
[2025-07-11T11:45:45.860+0200] {subprocess.py:93} INFO - 25/07/11 11:45:45 INFO DAGScheduler: Missing parents: List()
[2025-07-11T11:45:45.860+0200] {subprocess.py:93} INFO - 25/07/11 11:45:45 INFO DAGScheduler: Submitting ShuffleMapStage 14 (MapPartitionsRDD[27] at collect at StringIndexer.scala:204), which has no missing parents
[2025-07-11T11:45:45.862+0200] {subprocess.py:93} INFO - 25/07/11 11:45:45 INFO MemoryStore: Block broadcast_12 stored as values in memory (estimated size 13.7 KiB, free 428.9 MiB)
[2025-07-11T11:45:45.865+0200] {subprocess.py:93} INFO - 25/07/11 11:45:45 INFO MemoryStore: Block broadcast_12_piece0 stored as bytes in memory (estimated size 7.0 KiB, free 428.9 MiB)
[2025-07-11T11:45:45.865+0200] {subprocess.py:93} INFO - 25/07/11 11:45:45 INFO BlockManagerInfo: Added broadcast_12_piece0 in memory on 138.4.31.14:45609 (size: 7.0 KiB, free: 434.1 MiB)
[2025-07-11T11:45:45.865+0200] {subprocess.py:93} INFO - 25/07/11 11:45:45 INFO SparkContext: Created broadcast 12 from broadcast at DAGScheduler.scala:1585
[2025-07-11T11:45:45.866+0200] {subprocess.py:93} INFO - 25/07/11 11:45:45 INFO DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 14 (MapPartitionsRDD[27] at collect at StringIndexer.scala:204) (first 15 tasks are for partitions Vector(0, 1))
[2025-07-11T11:45:45.866+0200] {subprocess.py:93} INFO - 25/07/11 11:45:45 INFO TaskSchedulerImpl: Adding task set 14.0 with 2 tasks resource profile 0
[2025-07-11T11:45:45.867+0200] {subprocess.py:93} INFO - 25/07/11 11:45:45 INFO DAGScheduler: Registering RDD 31 (collect at StringIndexer.scala:204) as input to shuffle 5
[2025-07-11T11:45:45.867+0200] {subprocess.py:93} INFO - 25/07/11 11:45:45 INFO DAGScheduler: Got map stage job 10 (collect at StringIndexer.scala:204) with 12 output partitions
[2025-07-11T11:45:45.867+0200] {subprocess.py:93} INFO - 25/07/11 11:45:45 INFO TaskSetManager: Starting task 0.0 in stage 14.0 (TID 21) (138.4.31.14, executor driver, partition 0, PROCESS_LOCAL, 13595 bytes)
[2025-07-11T11:45:45.867+0200] {subprocess.py:93} INFO - 25/07/11 11:45:45 INFO DAGScheduler: Final stage: ShuffleMapStage 15 (collect at StringIndexer.scala:204)
[2025-07-11T11:45:45.867+0200] {subprocess.py:93} INFO - 25/07/11 11:45:45 INFO DAGScheduler: Parents of final stage: List()
[2025-07-11T11:45:45.867+0200] {subprocess.py:93} INFO - 25/07/11 11:45:45 INFO DAGScheduler: Missing parents: List()
[2025-07-11T11:45:45.867+0200] {subprocess.py:93} INFO - 25/07/11 11:45:45 INFO TaskSetManager: Starting task 1.0 in stage 14.0 (TID 22) (138.4.31.14, executor driver, partition 1, PROCESS_LOCAL, 13595 bytes)
[2025-07-11T11:45:45.867+0200] {subprocess.py:93} INFO - 25/07/11 11:45:45 INFO DAGScheduler: Submitting ShuffleMapStage 15 (MapPartitionsRDD[31] at collect at StringIndexer.scala:204), which has no missing parents
[2025-07-11T11:45:45.867+0200] {subprocess.py:93} INFO - 25/07/11 11:45:45 INFO Executor: Running task 0.0 in stage 14.0 (TID 21)
[2025-07-11T11:45:45.870+0200] {subprocess.py:93} INFO - 25/07/11 11:45:45 INFO MemoryStore: Block broadcast_13 stored as values in memory (estimated size 18.2 KiB, free 428.9 MiB)
[2025-07-11T11:45:45.872+0200] {subprocess.py:93} INFO - 25/07/11 11:45:45 INFO Executor: Running task 1.0 in stage 14.0 (TID 22)
[2025-07-11T11:45:45.881+0200] {subprocess.py:93} INFO - 25/07/11 11:45:45 INFO MemoryStore: Block broadcast_13_piece0 stored as bytes in memory (estimated size 9.0 KiB, free 428.9 MiB)
[2025-07-11T11:45:45.881+0200] {subprocess.py:93} INFO - 25/07/11 11:45:45 INFO BlockManagerInfo: Added broadcast_13_piece0 in memory on 138.4.31.14:45609 (size: 9.0 KiB, free: 434.0 MiB)
[2025-07-11T11:45:45.882+0200] {subprocess.py:93} INFO - 25/07/11 11:45:45 INFO SparkContext: Created broadcast 13 from broadcast at DAGScheduler.scala:1585
[2025-07-11T11:45:45.882+0200] {subprocess.py:93} INFO - 25/07/11 11:45:45 INFO DAGScheduler: Submitting 12 missing tasks from ShuffleMapStage 15 (MapPartitionsRDD[31] at collect at StringIndexer.scala:204) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11))
[2025-07-11T11:45:45.883+0200] {subprocess.py:93} INFO - 25/07/11 11:45:45 INFO TaskSchedulerImpl: Adding task set 15.0 with 12 tasks resource profile 0
[2025-07-11T11:45:45.883+0200] {subprocess.py:93} INFO - 25/07/11 11:45:45 INFO CodeGenerator: Code generated in 11.147878 ms
[2025-07-11T11:45:45.885+0200] {subprocess.py:93} INFO - 25/07/11 11:45:45 INFO FileScanRDD: Reading File path: file:///home/monica.fernandez/practica_creativa/data/simple_flight_delay_features.jsonl.bz2, range: 4194304-4676447, partition values: [empty row]
[2025-07-11T11:45:45.885+0200] {subprocess.py:93} INFO - 25/07/11 11:45:45 INFO TaskSetManager: Starting task 0.0 in stage 15.0 (TID 23) (138.4.31.14, executor driver, partition 0, ANY, 14843 bytes)
[2025-07-11T11:45:45.885+0200] {subprocess.py:93} INFO - 25/07/11 11:45:45 INFO TaskSetManager: Starting task 1.0 in stage 15.0 (TID 24) (138.4.31.14, executor driver, partition 1, ANY, 14963 bytes)
[2025-07-11T11:45:45.885+0200] {subprocess.py:93} INFO - 25/07/11 11:45:45 INFO Executor: Running task 1.0 in stage 15.0 (TID 24)
[2025-07-11T11:45:45.887+0200] {subprocess.py:93} INFO - 25/07/11 11:45:45 INFO Executor: Running task 0.0 in stage 15.0 (TID 23)
[2025-07-11T11:45:45.890+0200] {subprocess.py:93} INFO - 25/07/11 11:45:45 INFO FileScanRDD: Reading File path: file:///home/monica.fernandez/practica_creativa/data/simple_flight_delay_features.jsonl.bz2, range: 0-4194304, partition values: [empty row]
[2025-07-11T11:45:45.987+0200] {subprocess.py:93} INFO - 25/07/11 11:45:45 INFO Executor: Finished task 0.0 in stage 15.0 (TID 23). 1988 bytes result sent to driver
[2025-07-11T11:45:45.989+0200] {subprocess.py:93} INFO - 25/07/11 11:45:45 INFO TaskSetManager: Starting task 2.0 in stage 15.0 (TID 25) (138.4.31.14, executor driver, partition 2, ANY, 14959 bytes)
[2025-07-11T11:45:45.989+0200] {subprocess.py:93} INFO - 25/07/11 11:45:45 INFO TaskSetManager: Finished task 0.0 in stage 15.0 (TID 23) in 105 ms on 138.4.31.14 (executor driver) (1/12)
[2025-07-11T11:45:45.989+0200] {subprocess.py:93} INFO - 25/07/11 11:45:45 INFO Executor: Running task 2.0 in stage 15.0 (TID 25)
[2025-07-11T11:45:45.997+0200] {subprocess.py:93} INFO - 25/07/11 11:45:45 INFO Executor: Finished task 1.0 in stage 15.0 (TID 24). 1988 bytes result sent to driver
[2025-07-11T11:45:45.998+0200] {subprocess.py:93} INFO - 25/07/11 11:45:45 INFO TaskSetManager: Starting task 3.0 in stage 15.0 (TID 26) (138.4.31.14, executor driver, partition 3, ANY, 14843 bytes)
[2025-07-11T11:45:46.000+0200] {subprocess.py:93} INFO - 25/07/11 11:45:46 INFO TaskSetManager: Finished task 1.0 in stage 15.0 (TID 24) in 116 ms on 138.4.31.14 (executor driver) (2/12)
[2025-07-11T11:45:46.002+0200] {subprocess.py:93} INFO - 25/07/11 11:45:46 INFO Executor: Running task 3.0 in stage 15.0 (TID 26)
[2025-07-11T11:45:46.068+0200] {subprocess.py:93} INFO - 25/07/11 11:45:46 INFO Executor: Finished task 3.0 in stage 15.0 (TID 26). 1945 bytes result sent to driver
[2025-07-11T11:45:46.070+0200] {subprocess.py:93} INFO - 25/07/11 11:45:46 INFO TaskSetManager: Starting task 4.0 in stage 15.0 (TID 27) (138.4.31.14, executor driver, partition 4, ANY, 14843 bytes)
[2025-07-11T11:45:46.070+0200] {subprocess.py:93} INFO - 25/07/11 11:45:46 INFO Executor: Running task 4.0 in stage 15.0 (TID 27)
[2025-07-11T11:45:46.070+0200] {subprocess.py:93} INFO - 25/07/11 11:45:46 INFO TaskSetManager: Finished task 3.0 in stage 15.0 (TID 26) in 72 ms on 138.4.31.14 (executor driver) (3/12)
[2025-07-11T11:45:46.093+0200] {subprocess.py:93} INFO - 25/07/11 11:45:46 INFO Executor: Finished task 2.0 in stage 15.0 (TID 25). 1988 bytes result sent to driver
[2025-07-11T11:45:46.094+0200] {subprocess.py:93} INFO - 25/07/11 11:45:46 INFO TaskSetManager: Starting task 5.0 in stage 15.0 (TID 28) (138.4.31.14, executor driver, partition 5, ANY, 14843 bytes)
[2025-07-11T11:45:46.098+0200] {subprocess.py:93} INFO - 25/07/11 11:45:46 INFO TaskSetManager: Finished task 2.0 in stage 15.0 (TID 25) in 107 ms on 138.4.31.14 (executor driver) (4/12)
[2025-07-11T11:45:46.100+0200] {subprocess.py:93} INFO - 25/07/11 11:45:46 INFO Executor: Running task 5.0 in stage 15.0 (TID 28)
[2025-07-11T11:45:46.161+0200] {subprocess.py:93} INFO - 25/07/11 11:45:46 INFO BlockManagerInfo: Removed broadcast_8_piece0 on 138.4.31.14:45609 in memory (size: 36.5 KiB, free: 434.1 MiB)
[2025-07-11T11:45:46.216+0200] {subprocess.py:93} INFO - 25/07/11 11:45:46 INFO Executor: Finished task 4.0 in stage 15.0 (TID 27). 1988 bytes result sent to driver
[2025-07-11T11:45:46.219+0200] {subprocess.py:93} INFO - 25/07/11 11:45:46 INFO TaskSetManager: Starting task 6.0 in stage 15.0 (TID 29) (138.4.31.14, executor driver, partition 6, ANY, 14843 bytes)
[2025-07-11T11:45:46.219+0200] {subprocess.py:93} INFO - 25/07/11 11:45:46 INFO TaskSetManager: Finished task 4.0 in stage 15.0 (TID 27) in 150 ms on 138.4.31.14 (executor driver) (5/12)
[2025-07-11T11:45:46.223+0200] {subprocess.py:93} INFO - 25/07/11 11:45:46 INFO Executor: Running task 6.0 in stage 15.0 (TID 29)
[2025-07-11T11:45:46.244+0200] {subprocess.py:93} INFO - 25/07/11 11:45:46 INFO Executor: Finished task 5.0 in stage 15.0 (TID 28). 1988 bytes result sent to driver
[2025-07-11T11:45:46.245+0200] {subprocess.py:93} INFO - 25/07/11 11:45:46 INFO TaskSetManager: Starting task 7.0 in stage 15.0 (TID 30) (138.4.31.14, executor driver, partition 7, ANY, 14963 bytes)
[2025-07-11T11:45:46.246+0200] {subprocess.py:93} INFO - 25/07/11 11:45:46 INFO TaskSetManager: Finished task 5.0 in stage 15.0 (TID 28) in 152 ms on 138.4.31.14 (executor driver) (6/12)
[2025-07-11T11:45:46.249+0200] {subprocess.py:93} INFO - 25/07/11 11:45:46 INFO Executor: Running task 7.0 in stage 15.0 (TID 30)
[2025-07-11T11:45:46.258+0200] {subprocess.py:93} INFO - 25/07/11 11:45:46 INFO BlockManagerInfo: Removed broadcast_9_piece0 on 138.4.31.14:45609 in memory (size: 4.4 KiB, free: 434.1 MiB)
[2025-07-11T11:45:46.362+0200] {subprocess.py:93} INFO - 25/07/11 11:45:46 INFO Executor: Finished task 6.0 in stage 15.0 (TID 29). 1945 bytes result sent to driver
[2025-07-11T11:45:46.362+0200] {subprocess.py:93} INFO - 25/07/11 11:45:46 INFO TaskSetManager: Starting task 8.0 in stage 15.0 (TID 31) (138.4.31.14, executor driver, partition 8, ANY, 14843 bytes)
[2025-07-11T11:45:46.363+0200] {subprocess.py:93} INFO - 25/07/11 11:45:46 INFO Executor: Running task 8.0 in stage 15.0 (TID 31)
[2025-07-11T11:45:46.363+0200] {subprocess.py:93} INFO - 25/07/11 11:45:46 INFO TaskSetManager: Finished task 6.0 in stage 15.0 (TID 29) in 145 ms on 138.4.31.14 (executor driver) (7/12)
[2025-07-11T11:45:46.369+0200] {subprocess.py:93} INFO - 25/07/11 11:45:46 INFO BlockManagerInfo: Removed broadcast_10_piece0 on 138.4.31.14:45609 in memory (size: 80.9 KiB, free: 434.2 MiB)
[2025-07-11T11:45:46.378+0200] {subprocess.py:93} INFO - 25/07/11 11:45:46 INFO Executor: Finished task 7.0 in stage 15.0 (TID 30). 1945 bytes result sent to driver
[2025-07-11T11:45:46.380+0200] {subprocess.py:93} INFO - 25/07/11 11:45:46 INFO TaskSetManager: Starting task 9.0 in stage 15.0 (TID 32) (138.4.31.14, executor driver, partition 9, ANY, 14963 bytes)
[2025-07-11T11:45:46.380+0200] {subprocess.py:93} INFO - 25/07/11 11:45:46 INFO Executor: Running task 9.0 in stage 15.0 (TID 32)
[2025-07-11T11:45:46.381+0200] {subprocess.py:93} INFO - 25/07/11 11:45:46 INFO TaskSetManager: Finished task 7.0 in stage 15.0 (TID 30) in 136 ms on 138.4.31.14 (executor driver) (8/12)
[2025-07-11T11:45:46.454+0200] {subprocess.py:93} INFO - 25/07/11 11:45:46 INFO BlockManagerInfo: Removed broadcast_7_piece0 on 138.4.31.14:45609 in memory (size: 23.4 KiB, free: 434.2 MiB)
[2025-07-11T11:45:46.473+0200] {subprocess.py:93} INFO - 25/07/11 11:45:46 INFO Executor: Finished task 8.0 in stage 15.0 (TID 31). 1945 bytes result sent to driver
[2025-07-11T11:45:46.475+0200] {subprocess.py:93} INFO - 25/07/11 11:45:46 INFO TaskSetManager: Starting task 10.0 in stage 15.0 (TID 33) (138.4.31.14, executor driver, partition 10, ANY, 14843 bytes)
[2025-07-11T11:45:46.478+0200] {subprocess.py:93} INFO - 25/07/11 11:45:46 INFO Executor: Running task 10.0 in stage 15.0 (TID 33)
[2025-07-11T11:45:46.480+0200] {subprocess.py:93} INFO - 25/07/11 11:45:46 INFO TaskSetManager: Finished task 8.0 in stage 15.0 (TID 31) in 113 ms on 138.4.31.14 (executor driver) (9/12)
[2025-07-11T11:45:46.497+0200] {subprocess.py:93} INFO - 25/07/11 11:45:46 INFO BlockManagerInfo: Removed broadcast_6_piece0 on 138.4.31.14:45609 in memory (size: 23.5 KiB, free: 434.2 MiB)
[2025-07-11T11:45:46.586+0200] {subprocess.py:93} INFO - 25/07/11 11:45:46 INFO Executor: Finished task 9.0 in stage 15.0 (TID 32). 1945 bytes result sent to driver
[2025-07-11T11:45:46.588+0200] {subprocess.py:93} INFO - 25/07/11 11:45:46 INFO TaskSetManager: Starting task 11.0 in stage 15.0 (TID 34) (138.4.31.14, executor driver, partition 11, ANY, 14843 bytes)
[2025-07-11T11:45:46.588+0200] {subprocess.py:93} INFO - 25/07/11 11:45:46 INFO TaskSetManager: Finished task 9.0 in stage 15.0 (TID 32) in 208 ms on 138.4.31.14 (executor driver) (10/12)
[2025-07-11T11:45:46.588+0200] {subprocess.py:93} INFO - 25/07/11 11:45:46 INFO Executor: Running task 11.0 in stage 15.0 (TID 34)
[2025-07-11T11:45:46.610+0200] {subprocess.py:93} INFO - 25/07/11 11:45:46 INFO Executor: Finished task 10.0 in stage 15.0 (TID 33). 1945 bytes result sent to driver
[2025-07-11T11:45:46.610+0200] {subprocess.py:93} INFO - 25/07/11 11:45:46 INFO TaskSetManager: Finished task 10.0 in stage 15.0 (TID 33) in 136 ms on 138.4.31.14 (executor driver) (11/12)
[2025-07-11T11:45:46.687+0200] {subprocess.py:93} INFO - 25/07/11 11:45:46 INFO Executor: Finished task 11.0 in stage 15.0 (TID 34). 1945 bytes result sent to driver
[2025-07-11T11:45:46.694+0200] {subprocess.py:93} INFO - 25/07/11 11:45:46 INFO TaskSetManager: Finished task 11.0 in stage 15.0 (TID 34) in 108 ms on 138.4.31.14 (executor driver) (12/12)
[2025-07-11T11:45:46.695+0200] {subprocess.py:93} INFO - 25/07/11 11:45:46 INFO TaskSchedulerImpl: Removed TaskSet 15.0, whose tasks have all completed, from pool
[2025-07-11T11:45:46.695+0200] {subprocess.py:93} INFO - 25/07/11 11:45:46 INFO DAGScheduler: ShuffleMapStage 15 (collect at StringIndexer.scala:204) finished in 0,825 s
[2025-07-11T11:45:46.695+0200] {subprocess.py:93} INFO - 25/07/11 11:45:46 INFO DAGScheduler: looking for newly runnable stages
[2025-07-11T11:45:46.695+0200] {subprocess.py:93} INFO - 25/07/11 11:45:46 INFO DAGScheduler: running: Set(ShuffleMapStage 14)
[2025-07-11T11:45:46.695+0200] {subprocess.py:93} INFO - 25/07/11 11:45:46 INFO DAGScheduler: waiting: Set()
[2025-07-11T11:45:46.695+0200] {subprocess.py:93} INFO - 25/07/11 11:45:46 INFO DAGScheduler: failed: Set()
[2025-07-11T11:45:46.735+0200] {subprocess.py:93} INFO - 25/07/11 11:45:46 INFO ShufflePartitionsUtil: For shuffle(5), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576
[2025-07-11T11:45:46.855+0200] {subprocess.py:93} INFO - 25/07/11 11:45:46 INFO SparkContext: Starting job: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264
[2025-07-11T11:45:46.859+0200] {subprocess.py:93} INFO - 25/07/11 11:45:46 INFO DAGScheduler: Got job 11 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) with 1 output partitions
[2025-07-11T11:45:46.860+0200] {subprocess.py:93} INFO - 25/07/11 11:45:46 INFO DAGScheduler: Final stage: ResultStage 17 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264)
[2025-07-11T11:45:46.860+0200] {subprocess.py:93} INFO - 25/07/11 11:45:46 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 16)
[2025-07-11T11:45:46.860+0200] {subprocess.py:93} INFO - 25/07/11 11:45:46 INFO DAGScheduler: Missing parents: List()
[2025-07-11T11:45:46.860+0200] {subprocess.py:93} INFO - 25/07/11 11:45:46 INFO DAGScheduler: Submitting ResultStage 17 (MapPartitionsRDD[33] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264), which has no missing parents
[2025-07-11T11:45:46.868+0200] {subprocess.py:93} INFO - 25/07/11 11:45:46 INFO MemoryStore: Block broadcast_14 stored as values in memory (estimated size 8.2 KiB, free 429.5 MiB)
[2025-07-11T11:45:46.868+0200] {subprocess.py:93} INFO - 25/07/11 11:45:46 INFO MemoryStore: Block broadcast_14_piece0 stored as bytes in memory (estimated size 4.2 KiB, free 429.5 MiB)
[2025-07-11T11:45:46.868+0200] {subprocess.py:93} INFO - 25/07/11 11:45:46 INFO BlockManagerInfo: Added broadcast_14_piece0 in memory on 138.4.31.14:45609 (size: 4.2 KiB, free: 434.2 MiB)
[2025-07-11T11:45:46.871+0200] {subprocess.py:93} INFO - 25/07/11 11:45:46 INFO SparkContext: Created broadcast 14 from broadcast at DAGScheduler.scala:1585
[2025-07-11T11:45:46.873+0200] {subprocess.py:93} INFO - 25/07/11 11:45:46 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 17 (MapPartitionsRDD[33] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) (first 15 tasks are for partitions Vector(0))
[2025-07-11T11:45:46.873+0200] {subprocess.py:93} INFO - 25/07/11 11:45:46 INFO TaskSchedulerImpl: Adding task set 17.0 with 1 tasks resource profile 0
[2025-07-11T11:45:46.874+0200] {subprocess.py:93} INFO - 25/07/11 11:45:46 INFO TaskSetManager: Starting task 0.0 in stage 17.0 (TID 35) (138.4.31.14, executor driver, partition 0, NODE_LOCAL, 12968 bytes)
[2025-07-11T11:45:46.876+0200] {subprocess.py:93} INFO - 25/07/11 11:45:46 INFO Executor: Running task 0.0 in stage 17.0 (TID 35)
[2025-07-11T11:45:46.904+0200] {subprocess.py:93} INFO - 25/07/11 11:45:46 INFO ShuffleBlockFetcherIterator: Getting 12 (193.5 KiB) non-empty blocks including 12 (193.5 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2025-07-11T11:45:46.905+0200] {subprocess.py:93} INFO - 25/07/11 11:45:46 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 3 ms
[2025-07-11T11:45:47.034+0200] {subprocess.py:93} INFO - 25/07/11 11:45:47 INFO Executor: Finished task 0.0 in stage 17.0 (TID 35). 40817 bytes result sent to driver
[2025-07-11T11:45:47.035+0200] {subprocess.py:93} INFO - 25/07/11 11:45:47 INFO TaskSetManager: Finished task 0.0 in stage 17.0 (TID 35) in 161 ms on 138.4.31.14 (executor driver) (1/1)
[2025-07-11T11:45:47.035+0200] {subprocess.py:93} INFO - 25/07/11 11:45:47 INFO TaskSchedulerImpl: Removed TaskSet 17.0, whose tasks have all completed, from pool
[2025-07-11T11:45:47.038+0200] {subprocess.py:93} INFO - 25/07/11 11:45:47 INFO DAGScheduler: ResultStage 17 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) finished in 0,172 s
[2025-07-11T11:45:47.040+0200] {subprocess.py:93} INFO - 25/07/11 11:45:47 INFO DAGScheduler: Job 11 is finished. Cancelling potential speculative or zombie tasks for this job
[2025-07-11T11:45:47.040+0200] {subprocess.py:93} INFO - 25/07/11 11:45:47 INFO TaskSchedulerImpl: Killing all running tasks in stage 17: Stage finished
[2025-07-11T11:45:47.040+0200] {subprocess.py:93} INFO - 25/07/11 11:45:47 INFO DAGScheduler: Job 11 finished: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264, took 0,181638 s
[2025-07-11T11:45:47.062+0200] {subprocess.py:93} INFO - 25/07/11 11:45:47 INFO MemoryStore: Block broadcast_15 stored as values in memory (estimated size 4.3 MiB, free 425.2 MiB)
[2025-07-11T11:45:47.072+0200] {subprocess.py:93} INFO - 25/07/11 11:45:47 INFO MemoryStore: Block broadcast_15_piece0 stored as bytes in memory (estimated size 89.9 KiB, free 425.1 MiB)
[2025-07-11T11:45:47.073+0200] {subprocess.py:93} INFO - 25/07/11 11:45:47 INFO BlockManagerInfo: Added broadcast_15_piece0 in memory on 138.4.31.14:45609 (size: 89.9 KiB, free: 434.1 MiB)
[2025-07-11T11:45:47.074+0200] {subprocess.py:93} INFO - 25/07/11 11:45:47 INFO SparkContext: Created broadcast 15 from $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264
[2025-07-11T11:45:47.549+0200] {subprocess.py:93} INFO - 25/07/11 11:45:47 INFO BlockManagerInfo: Removed broadcast_14_piece0 on 138.4.31.14:45609 in memory (size: 4.2 KiB, free: 434.1 MiB)
[2025-07-11T11:45:47.831+0200] {subprocess.py:93} INFO - 25/07/11 11:45:47 INFO Executor: Finished task 1.0 in stage 14.0 (TID 22). 1959 bytes result sent to driver
[2025-07-11T11:45:47.845+0200] {subprocess.py:93} INFO - 25/07/11 11:45:47 INFO TaskSetManager: Finished task 1.0 in stage 14.0 (TID 22) in 1977 ms on 138.4.31.14 (executor driver) (1/2)
[2025-07-11T11:45:54.265+0200] {subprocess.py:93} INFO - 25/07/11 11:45:54 INFO Executor: Finished task 0.0 in stage 14.0 (TID 21). 1959 bytes result sent to driver
[2025-07-11T11:45:54.267+0200] {subprocess.py:93} INFO - 25/07/11 11:45:54 INFO TaskSetManager: Finished task 0.0 in stage 14.0 (TID 21) in 8400 ms on 138.4.31.14 (executor driver) (2/2)
[2025-07-11T11:45:54.267+0200] {subprocess.py:93} INFO - 25/07/11 11:45:54 INFO TaskSchedulerImpl: Removed TaskSet 14.0, whose tasks have all completed, from pool
[2025-07-11T11:45:54.268+0200] {subprocess.py:93} INFO - 25/07/11 11:45:54 INFO DAGScheduler: ShuffleMapStage 14 (collect at StringIndexer.scala:204) finished in 8,407 s
[2025-07-11T11:45:54.268+0200] {subprocess.py:93} INFO - 25/07/11 11:45:54 INFO DAGScheduler: looking for newly runnable stages
[2025-07-11T11:45:54.268+0200] {subprocess.py:93} INFO - 25/07/11 11:45:54 INFO DAGScheduler: running: Set()
[2025-07-11T11:45:54.268+0200] {subprocess.py:93} INFO - 25/07/11 11:45:54 INFO DAGScheduler: waiting: Set()
[2025-07-11T11:45:54.270+0200] {subprocess.py:93} INFO - 25/07/11 11:45:54 INFO DAGScheduler: failed: Set()
[2025-07-11T11:45:54.286+0200] {subprocess.py:93} INFO - 25/07/11 11:45:54 INFO ShufflePartitionsUtil: For shuffle(4), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576
[2025-07-11T11:45:54.339+0200] {subprocess.py:93} INFO - 25/07/11 11:45:54 INFO CodeGenerator: Code generated in 27.396938 ms
[2025-07-11T11:45:54.346+0200] {subprocess.py:93} INFO - 25/07/11 11:45:54 INFO DAGScheduler: Registering RDD 37 (collect at StringIndexer.scala:204) as input to shuffle 6
[2025-07-11T11:45:54.346+0200] {subprocess.py:93} INFO - 25/07/11 11:45:54 INFO DAGScheduler: Got map stage job 12 (collect at StringIndexer.scala:204) with 1 output partitions
[2025-07-11T11:45:54.346+0200] {subprocess.py:93} INFO - 25/07/11 11:45:54 INFO DAGScheduler: Final stage: ShuffleMapStage 19 (collect at StringIndexer.scala:204)
[2025-07-11T11:45:54.346+0200] {subprocess.py:93} INFO - 25/07/11 11:45:54 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 18)
[2025-07-11T11:45:54.346+0200] {subprocess.py:93} INFO - 25/07/11 11:45:54 INFO DAGScheduler: Missing parents: List()
[2025-07-11T11:45:54.347+0200] {subprocess.py:93} INFO - 25/07/11 11:45:54 INFO DAGScheduler: Submitting ShuffleMapStage 19 (MapPartitionsRDD[37] at collect at StringIndexer.scala:204), which has no missing parents
[2025-07-11T11:45:54.350+0200] {subprocess.py:93} INFO - 25/07/11 11:45:54 INFO MemoryStore: Block broadcast_16 stored as values in memory (estimated size 49.5 KiB, free 425.1 MiB)
[2025-07-11T11:45:54.351+0200] {subprocess.py:93} INFO - 25/07/11 11:45:54 INFO MemoryStore: Block broadcast_16_piece0 stored as bytes in memory (estimated size 23.6 KiB, free 425.1 MiB)
[2025-07-11T11:45:54.352+0200] {subprocess.py:93} INFO - 25/07/11 11:45:54 INFO BlockManagerInfo: Added broadcast_16_piece0 in memory on 138.4.31.14:45609 (size: 23.6 KiB, free: 434.1 MiB)
[2025-07-11T11:45:54.352+0200] {subprocess.py:93} INFO - 25/07/11 11:45:54 INFO SparkContext: Created broadcast 16 from broadcast at DAGScheduler.scala:1585
[2025-07-11T11:45:54.353+0200] {subprocess.py:93} INFO - 25/07/11 11:45:54 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 19 (MapPartitionsRDD[37] at collect at StringIndexer.scala:204) (first 15 tasks are for partitions Vector(0))
[2025-07-11T11:45:54.353+0200] {subprocess.py:93} INFO - 25/07/11 11:45:54 INFO TaskSchedulerImpl: Adding task set 19.0 with 1 tasks resource profile 0
[2025-07-11T11:45:54.356+0200] {subprocess.py:93} INFO - 25/07/11 11:45:54 INFO TaskSetManager: Starting task 0.0 in stage 19.0 (TID 36) (138.4.31.14, executor driver, partition 0, NODE_LOCAL, 12957 bytes)
[2025-07-11T11:45:54.356+0200] {subprocess.py:93} INFO - 25/07/11 11:45:54 INFO Executor: Running task 0.0 in stage 19.0 (TID 36)
[2025-07-11T11:45:54.363+0200] {subprocess.py:93} INFO - 25/07/11 11:45:54 INFO ShuffleBlockFetcherIterator: Getting 2 (887.5 KiB) non-empty blocks including 2 (887.5 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2025-07-11T11:45:54.363+0200] {subprocess.py:93} INFO - 25/07/11 11:45:54 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2025-07-11T11:45:54.373+0200] {subprocess.py:93} INFO - 25/07/11 11:45:54 INFO CodeGenerator: Code generated in 9.265909 ms
[2025-07-11T11:45:54.823+0200] {subprocess.py:93} INFO - 25/07/11 11:45:54 INFO Executor: Finished task 0.0 in stage 19.0 (TID 36). 6444 bytes result sent to driver
[2025-07-11T11:45:54.824+0200] {subprocess.py:93} INFO - 25/07/11 11:45:54 INFO TaskSetManager: Finished task 0.0 in stage 19.0 (TID 36) in 470 ms on 138.4.31.14 (executor driver) (1/1)
[2025-07-11T11:45:54.824+0200] {subprocess.py:93} INFO - 25/07/11 11:45:54 INFO TaskSchedulerImpl: Removed TaskSet 19.0, whose tasks have all completed, from pool
[2025-07-11T11:45:54.825+0200] {subprocess.py:93} INFO - 25/07/11 11:45:54 INFO DAGScheduler: ShuffleMapStage 19 (collect at StringIndexer.scala:204) finished in 0,477 s
[2025-07-11T11:45:54.825+0200] {subprocess.py:93} INFO - 25/07/11 11:45:54 INFO DAGScheduler: looking for newly runnable stages
[2025-07-11T11:45:54.825+0200] {subprocess.py:93} INFO - 25/07/11 11:45:54 INFO DAGScheduler: running: Set()
[2025-07-11T11:45:54.825+0200] {subprocess.py:93} INFO - 25/07/11 11:45:54 INFO DAGScheduler: waiting: Set()
[2025-07-11T11:45:54.825+0200] {subprocess.py:93} INFO - 25/07/11 11:45:54 INFO DAGScheduler: failed: Set()
[2025-07-11T11:45:54.848+0200] {subprocess.py:93} INFO - 25/07/11 11:45:54 INFO SparkContext: Starting job: collect at StringIndexer.scala:204
[2025-07-11T11:45:54.848+0200] {subprocess.py:93} INFO - 25/07/11 11:45:54 INFO DAGScheduler: Got job 13 (collect at StringIndexer.scala:204) with 1 output partitions
[2025-07-11T11:45:54.848+0200] {subprocess.py:93} INFO - 25/07/11 11:45:54 INFO DAGScheduler: Final stage: ResultStage 22 (collect at StringIndexer.scala:204)
[2025-07-11T11:45:54.848+0200] {subprocess.py:93} INFO - 25/07/11 11:45:54 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 21)
[2025-07-11T11:45:54.848+0200] {subprocess.py:93} INFO - 25/07/11 11:45:54 INFO DAGScheduler: Missing parents: List()
[2025-07-11T11:45:54.849+0200] {subprocess.py:93} INFO - 25/07/11 11:45:54 INFO DAGScheduler: Submitting ResultStage 22 (MapPartitionsRDD[40] at collect at StringIndexer.scala:204), which has no missing parents
[2025-07-11T11:45:54.851+0200] {subprocess.py:93} INFO - 25/07/11 11:45:54 INFO MemoryStore: Block broadcast_17 stored as values in memory (estimated size 48.7 KiB, free 425.0 MiB)
[2025-07-11T11:45:54.852+0200] {subprocess.py:93} INFO - 25/07/11 11:45:54 INFO MemoryStore: Block broadcast_17_piece0 stored as bytes in memory (estimated size 23.3 KiB, free 425.0 MiB)
[2025-07-11T11:45:54.852+0200] {subprocess.py:93} INFO - 25/07/11 11:45:54 INFO BlockManagerInfo: Added broadcast_17_piece0 in memory on 138.4.31.14:45609 (size: 23.3 KiB, free: 434.1 MiB)
[2025-07-11T11:45:54.853+0200] {subprocess.py:93} INFO - 25/07/11 11:45:54 INFO SparkContext: Created broadcast 17 from broadcast at DAGScheduler.scala:1585
[2025-07-11T11:45:54.853+0200] {subprocess.py:93} INFO - 25/07/11 11:45:54 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 22 (MapPartitionsRDD[40] at collect at StringIndexer.scala:204) (first 15 tasks are for partitions Vector(0))
[2025-07-11T11:45:54.854+0200] {subprocess.py:93} INFO - 25/07/11 11:45:54 INFO TaskSchedulerImpl: Adding task set 22.0 with 1 tasks resource profile 0
[2025-07-11T11:45:54.855+0200] {subprocess.py:93} INFO - 25/07/11 11:45:54 INFO TaskSetManager: Starting task 0.0 in stage 22.0 (TID 37) (138.4.31.14, executor driver, partition 0, NODE_LOCAL, 12952 bytes)
[2025-07-11T11:45:54.856+0200] {subprocess.py:93} INFO - 25/07/11 11:45:54 INFO Executor: Running task 0.0 in stage 22.0 (TID 37)
[2025-07-11T11:45:54.861+0200] {subprocess.py:93} INFO - 25/07/11 11:45:54 INFO ShuffleBlockFetcherIterator: Getting 1 (3.2 KiB) non-empty blocks including 1 (3.2 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2025-07-11T11:45:54.861+0200] {subprocess.py:93} INFO - 25/07/11 11:45:54 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2025-07-11T11:45:54.900+0200] {subprocess.py:93} INFO - 25/07/11 11:45:54 INFO Executor: Finished task 0.0 in stage 22.0 (TID 37). 10880 bytes result sent to driver
[2025-07-11T11:45:54.902+0200] {subprocess.py:93} INFO - 25/07/11 11:45:54 INFO TaskSetManager: Finished task 0.0 in stage 22.0 (TID 37) in 47 ms on 138.4.31.14 (executor driver) (1/1)
[2025-07-11T11:45:54.902+0200] {subprocess.py:93} INFO - 25/07/11 11:45:54 INFO TaskSchedulerImpl: Removed TaskSet 22.0, whose tasks have all completed, from pool
[2025-07-11T11:45:54.905+0200] {subprocess.py:93} INFO - 25/07/11 11:45:54 INFO DAGScheduler: ResultStage 22 (collect at StringIndexer.scala:204) finished in 0,054 s
[2025-07-11T11:45:54.905+0200] {subprocess.py:93} INFO - 25/07/11 11:45:54 INFO DAGScheduler: Job 13 is finished. Cancelling potential speculative or zombie tasks for this job
[2025-07-11T11:45:54.905+0200] {subprocess.py:93} INFO - 25/07/11 11:45:54 INFO TaskSchedulerImpl: Killing all running tasks in stage 22: Stage finished
[2025-07-11T11:45:54.905+0200] {subprocess.py:93} INFO - 25/07/11 11:45:54 INFO DAGScheduler: Job 13 finished: collect at StringIndexer.scala:204, took 0,058076 s
[2025-07-11T11:45:55.037+0200] {subprocess.py:93} INFO - 25/07/11 11:45:55 INFO FileSystemOverwrite: Path /home/monica.fernandez/practica_creativa/models/string_indexer_model_Origin.bin already exists. It will be overwritten.
[2025-07-11T11:45:55.071+0200] {subprocess.py:93} INFO - 25/07/11 11:45:55 INFO HadoopMapRedCommitProtocol: Using output committer class org.apache.hadoop.mapred.FileOutputCommitter
[2025-07-11T11:45:55.071+0200] {subprocess.py:93} INFO - 25/07/11 11:45:55 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2025-07-11T11:45:55.071+0200] {subprocess.py:93} INFO - 25/07/11 11:45:55 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2025-07-11T11:45:55.132+0200] {subprocess.py:93} INFO - 25/07/11 11:45:55 INFO SparkContext: Starting job: runJob at SparkHadoopWriter.scala:83
[2025-07-11T11:45:55.139+0200] {subprocess.py:93} INFO - 25/07/11 11:45:55 INFO DAGScheduler: Got job 14 (runJob at SparkHadoopWriter.scala:83) with 1 output partitions
[2025-07-11T11:45:55.140+0200] {subprocess.py:93} INFO - 25/07/11 11:45:55 INFO DAGScheduler: Final stage: ResultStage 23 (runJob at SparkHadoopWriter.scala:83)
[2025-07-11T11:45:55.140+0200] {subprocess.py:93} INFO - 25/07/11 11:45:55 INFO DAGScheduler: Parents of final stage: List()
[2025-07-11T11:45:55.140+0200] {subprocess.py:93} INFO - 25/07/11 11:45:55 INFO DAGScheduler: Missing parents: List()
[2025-07-11T11:45:55.140+0200] {subprocess.py:93} INFO - 25/07/11 11:45:55 INFO DAGScheduler: Submitting ResultStage 23 (MapPartitionsRDD[42] at saveAsTextFile at ReadWrite.scala:413), which has no missing parents
[2025-07-11T11:45:55.149+0200] {subprocess.py:93} INFO - 25/07/11 11:45:55 INFO MemoryStore: Block broadcast_18 stored as values in memory (estimated size 101.3 KiB, free 424.9 MiB)
[2025-07-11T11:45:55.151+0200] {subprocess.py:93} INFO - 25/07/11 11:45:55 INFO MemoryStore: Block broadcast_18_piece0 stored as bytes in memory (estimated size 36.6 KiB, free 424.9 MiB)
[2025-07-11T11:45:55.151+0200] {subprocess.py:93} INFO - 25/07/11 11:45:55 INFO BlockManagerInfo: Added broadcast_18_piece0 in memory on 138.4.31.14:45609 (size: 36.6 KiB, free: 434.0 MiB)
[2025-07-11T11:45:55.154+0200] {subprocess.py:93} INFO - 25/07/11 11:45:55 INFO SparkContext: Created broadcast 18 from broadcast at DAGScheduler.scala:1585
[2025-07-11T11:45:55.157+0200] {subprocess.py:93} INFO - 25/07/11 11:45:55 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 23 (MapPartitionsRDD[42] at saveAsTextFile at ReadWrite.scala:413) (first 15 tasks are for partitions Vector(0))
[2025-07-11T11:45:55.163+0200] {subprocess.py:93} INFO - 25/07/11 11:45:55 INFO TaskSchedulerImpl: Adding task set 23.0 with 1 tasks resource profile 0
[2025-07-11T11:45:55.165+0200] {subprocess.py:93} INFO - 25/07/11 11:45:55 INFO TaskSetManager: Starting task 0.0 in stage 23.0 (TID 38) (138.4.31.14, executor driver, partition 0, PROCESS_LOCAL, 13280 bytes)
[2025-07-11T11:45:55.165+0200] {subprocess.py:93} INFO - 25/07/11 11:45:55 INFO Executor: Running task 0.0 in stage 23.0 (TID 38)
[2025-07-11T11:45:55.176+0200] {subprocess.py:93} INFO - 25/07/11 11:45:55 INFO HadoopMapRedCommitProtocol: Using output committer class org.apache.hadoop.mapred.FileOutputCommitter
[2025-07-11T11:45:55.176+0200] {subprocess.py:93} INFO - 25/07/11 11:45:55 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2025-07-11T11:45:55.177+0200] {subprocess.py:93} INFO - 25/07/11 11:45:55 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2025-07-11T11:45:55.236+0200] {subprocess.py:93} INFO - 25/07/11 11:45:55 INFO FileOutputCommitter: Saved output of task 'attempt_202507111145552188246567483062117_0042_m_000000_0' to file:/home/monica.fernandez/practica_creativa/models/string_indexer_model_Origin.bin/metadata/_temporary/0/task_202507111145552188246567483062117_0042_m_000000
[2025-07-11T11:45:55.236+0200] {subprocess.py:93} INFO - 25/07/11 11:45:55 INFO SparkHadoopMapRedUtil: attempt_202507111145552188246567483062117_0042_m_000000_0: Committed. Elapsed time: 1 ms.
[2025-07-11T11:45:55.238+0200] {subprocess.py:93} INFO - 25/07/11 11:45:55 INFO Executor: Finished task 0.0 in stage 23.0 (TID 38). 1170 bytes result sent to driver
[2025-07-11T11:45:55.239+0200] {subprocess.py:93} INFO - 25/07/11 11:45:55 INFO TaskSetManager: Finished task 0.0 in stage 23.0 (TID 38) in 74 ms on 138.4.31.14 (executor driver) (1/1)
[2025-07-11T11:45:55.239+0200] {subprocess.py:93} INFO - 25/07/11 11:45:55 INFO TaskSchedulerImpl: Removed TaskSet 23.0, whose tasks have all completed, from pool
[2025-07-11T11:45:55.242+0200] {subprocess.py:93} INFO - 25/07/11 11:45:55 INFO DAGScheduler: ResultStage 23 (runJob at SparkHadoopWriter.scala:83) finished in 0,104 s
[2025-07-11T11:45:55.242+0200] {subprocess.py:93} INFO - 25/07/11 11:45:55 INFO DAGScheduler: Job 14 is finished. Cancelling potential speculative or zombie tasks for this job
[2025-07-11T11:45:55.242+0200] {subprocess.py:93} INFO - 25/07/11 11:45:55 INFO TaskSchedulerImpl: Killing all running tasks in stage 23: Stage finished
[2025-07-11T11:45:55.242+0200] {subprocess.py:93} INFO - 25/07/11 11:45:55 INFO DAGScheduler: Job 14 finished: runJob at SparkHadoopWriter.scala:83, took 0,110022 s
[2025-07-11T11:45:55.242+0200] {subprocess.py:93} INFO - 25/07/11 11:45:55 INFO SparkHadoopWriter: Start to commit write Job job_202507111145552188246567483062117_0042.
[2025-07-11T11:45:55.266+0200] {subprocess.py:93} INFO - 25/07/11 11:45:55 INFO SparkHadoopWriter: Write Job job_202507111145552188246567483062117_0042 committed. Elapsed time: 23 ms.
[2025-07-11T11:45:55.310+0200] {subprocess.py:93} INFO - 25/07/11 11:45:55 INFO DAGScheduler: Registering RDD 45 (parquet at StringIndexer.scala:499) as input to shuffle 7
[2025-07-11T11:45:55.310+0200] {subprocess.py:93} INFO - 25/07/11 11:45:55 INFO DAGScheduler: Got map stage job 15 (parquet at StringIndexer.scala:499) with 1 output partitions
[2025-07-11T11:45:55.311+0200] {subprocess.py:93} INFO - 25/07/11 11:45:55 INFO DAGScheduler: Final stage: ShuffleMapStage 24 (parquet at StringIndexer.scala:499)
[2025-07-11T11:45:55.311+0200] {subprocess.py:93} INFO - 25/07/11 11:45:55 INFO DAGScheduler: Parents of final stage: List()
[2025-07-11T11:45:55.312+0200] {subprocess.py:93} INFO - 25/07/11 11:45:55 INFO DAGScheduler: Missing parents: List()
[2025-07-11T11:45:55.312+0200] {subprocess.py:93} INFO - 25/07/11 11:45:55 INFO DAGScheduler: Submitting ShuffleMapStage 24 (MapPartitionsRDD[45] at parquet at StringIndexer.scala:499), which has no missing parents
[2025-07-11T11:45:55.314+0200] {subprocess.py:93} INFO - 25/07/11 11:45:55 INFO MemoryStore: Block broadcast_19 stored as values in memory (estimated size 8.1 KiB, free 424.9 MiB)
[2025-07-11T11:45:55.315+0200] {subprocess.py:93} INFO - 25/07/11 11:45:55 INFO MemoryStore: Block broadcast_19_piece0 stored as bytes in memory (estimated size 4.4 KiB, free 424.9 MiB)
[2025-07-11T11:45:55.315+0200] {subprocess.py:93} INFO - 25/07/11 11:45:55 INFO BlockManagerInfo: Added broadcast_19_piece0 in memory on 138.4.31.14:45609 (size: 4.4 KiB, free: 434.0 MiB)
[2025-07-11T11:45:55.316+0200] {subprocess.py:93} INFO - 25/07/11 11:45:55 INFO SparkContext: Created broadcast 19 from broadcast at DAGScheduler.scala:1585
[2025-07-11T11:45:55.316+0200] {subprocess.py:93} INFO - 25/07/11 11:45:55 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 24 (MapPartitionsRDD[45] at parquet at StringIndexer.scala:499) (first 15 tasks are for partitions Vector(0))
[2025-07-11T11:45:55.316+0200] {subprocess.py:93} INFO - 25/07/11 11:45:55 INFO TaskSchedulerImpl: Adding task set 24.0 with 1 tasks resource profile 0
[2025-07-11T11:45:55.317+0200] {subprocess.py:93} INFO - 25/07/11 11:45:55 INFO TaskSetManager: Starting task 0.0 in stage 24.0 (TID 39) (138.4.31.14, executor driver, partition 0, PROCESS_LOCAL, 18200 bytes)
[2025-07-11T11:45:55.317+0200] {subprocess.py:93} INFO - 25/07/11 11:45:55 INFO Executor: Running task 0.0 in stage 24.0 (TID 39)
[2025-07-11T11:45:55.322+0200] {subprocess.py:93} INFO - 25/07/11 11:45:55 INFO Executor: Finished task 0.0 in stage 24.0 (TID 39). 1628 bytes result sent to driver
[2025-07-11T11:45:55.323+0200] {subprocess.py:93} INFO - 25/07/11 11:45:55 INFO TaskSetManager: Finished task 0.0 in stage 24.0 (TID 39) in 6 ms on 138.4.31.14 (executor driver) (1/1)
[2025-07-11T11:45:55.323+0200] {subprocess.py:93} INFO - 25/07/11 11:45:55 INFO TaskSchedulerImpl: Removed TaskSet 24.0, whose tasks have all completed, from pool
[2025-07-11T11:45:55.324+0200] {subprocess.py:93} INFO - 25/07/11 11:45:55 INFO DAGScheduler: ShuffleMapStage 24 (parquet at StringIndexer.scala:499) finished in 0,011 s
[2025-07-11T11:45:55.324+0200] {subprocess.py:93} INFO - 25/07/11 11:45:55 INFO DAGScheduler: looking for newly runnable stages
[2025-07-11T11:45:55.324+0200] {subprocess.py:93} INFO - 25/07/11 11:45:55 INFO DAGScheduler: running: Set()
[2025-07-11T11:45:55.324+0200] {subprocess.py:93} INFO - 25/07/11 11:45:55 INFO DAGScheduler: waiting: Set()
[2025-07-11T11:45:55.324+0200] {subprocess.py:93} INFO - 25/07/11 11:45:55 INFO DAGScheduler: failed: Set()
[2025-07-11T11:45:55.331+0200] {subprocess.py:93} INFO - 25/07/11 11:45:55 INFO ParquetUtils: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
[2025-07-11T11:45:55.332+0200] {subprocess.py:93} INFO - 25/07/11 11:45:55 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2025-07-11T11:45:55.332+0200] {subprocess.py:93} INFO - 25/07/11 11:45:55 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2025-07-11T11:45:55.332+0200] {subprocess.py:93} INFO - 25/07/11 11:45:55 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[2025-07-11T11:45:55.332+0200] {subprocess.py:93} INFO - 25/07/11 11:45:55 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2025-07-11T11:45:55.332+0200] {subprocess.py:93} INFO - 25/07/11 11:45:55 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2025-07-11T11:45:55.333+0200] {subprocess.py:93} INFO - 25/07/11 11:45:55 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[2025-07-11T11:45:55.362+0200] {subprocess.py:93} INFO - 25/07/11 11:45:55 INFO SparkContext: Starting job: parquet at StringIndexer.scala:499
[2025-07-11T11:45:55.364+0200] {subprocess.py:93} INFO - 25/07/11 11:45:55 INFO DAGScheduler: Got job 16 (parquet at StringIndexer.scala:499) with 1 output partitions
[2025-07-11T11:45:55.364+0200] {subprocess.py:93} INFO - 25/07/11 11:45:55 INFO DAGScheduler: Final stage: ResultStage 26 (parquet at StringIndexer.scala:499)
[2025-07-11T11:45:55.364+0200] {subprocess.py:93} INFO - 25/07/11 11:45:55 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 25)
[2025-07-11T11:45:55.364+0200] {subprocess.py:93} INFO - 25/07/11 11:45:55 INFO DAGScheduler: Missing parents: List()
[2025-07-11T11:45:55.364+0200] {subprocess.py:93} INFO - 25/07/11 11:45:55 INFO DAGScheduler: Submitting ResultStage 26 (MapPartitionsRDD[47] at parquet at StringIndexer.scala:499), which has no missing parents
[2025-07-11T11:45:55.385+0200] {subprocess.py:93} INFO - 25/07/11 11:45:55 INFO MemoryStore: Block broadcast_20 stored as values in memory (estimated size 230.2 KiB, free 424.6 MiB)
[2025-07-11T11:45:55.387+0200] {subprocess.py:93} INFO - 25/07/11 11:45:55 INFO MemoryStore: Block broadcast_20_piece0 stored as bytes in memory (estimated size 80.9 KiB, free 424.6 MiB)
[2025-07-11T11:45:55.388+0200] {subprocess.py:93} INFO - 25/07/11 11:45:55 INFO BlockManagerInfo: Added broadcast_20_piece0 in memory on 138.4.31.14:45609 (size: 80.9 KiB, free: 434.0 MiB)
[2025-07-11T11:45:55.388+0200] {subprocess.py:93} INFO - 25/07/11 11:45:55 INFO SparkContext: Created broadcast 20 from broadcast at DAGScheduler.scala:1585
[2025-07-11T11:45:55.389+0200] {subprocess.py:93} INFO - 25/07/11 11:45:55 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 26 (MapPartitionsRDD[47] at parquet at StringIndexer.scala:499) (first 15 tasks are for partitions Vector(0))
[2025-07-11T11:45:55.390+0200] {subprocess.py:93} INFO - 25/07/11 11:45:55 INFO TaskSchedulerImpl: Adding task set 26.0 with 1 tasks resource profile 0
[2025-07-11T11:45:55.392+0200] {subprocess.py:93} INFO - 25/07/11 11:45:55 INFO TaskSetManager: Starting task 0.0 in stage 26.0 (TID 40) (138.4.31.14, executor driver, partition 0, NODE_LOCAL, 12952 bytes)
[2025-07-11T11:45:55.392+0200] {subprocess.py:93} INFO - 25/07/11 11:45:55 INFO Executor: Running task 0.0 in stage 26.0 (TID 40)
[2025-07-11T11:45:55.402+0200] {subprocess.py:93} INFO - 25/07/11 11:45:55 INFO ShuffleBlockFetcherIterator: Getting 1 (2.9 KiB) non-empty blocks including 1 (2.9 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2025-07-11T11:45:55.402+0200] {subprocess.py:93} INFO - 25/07/11 11:45:55 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2025-07-11T11:45:55.403+0200] {subprocess.py:93} INFO - 25/07/11 11:45:55 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2025-07-11T11:45:55.403+0200] {subprocess.py:93} INFO - 25/07/11 11:45:55 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2025-07-11T11:45:55.403+0200] {subprocess.py:93} INFO - 25/07/11 11:45:55 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[2025-07-11T11:45:55.403+0200] {subprocess.py:93} INFO - 25/07/11 11:45:55 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2025-07-11T11:45:55.403+0200] {subprocess.py:93} INFO - 25/07/11 11:45:55 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2025-07-11T11:45:55.404+0200] {subprocess.py:93} INFO - 25/07/11 11:45:55 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[2025-07-11T11:45:55.404+0200] {subprocess.py:93} INFO - 25/07/11 11:45:55 INFO CodecConfig: Compression: SNAPPY
[2025-07-11T11:45:55.404+0200] {subprocess.py:93} INFO - 25/07/11 11:45:55 INFO CodecConfig: Compression: SNAPPY
[2025-07-11T11:45:55.405+0200] {subprocess.py:93} INFO - 25/07/11 11:45:55 INFO ParquetOutputFormat: ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]
[2025-07-11T11:45:55.405+0200] {subprocess.py:93} INFO - 25/07/11 11:45:55 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
[2025-07-11T11:45:55.406+0200] {subprocess.py:93} INFO - {
[2025-07-11T11:45:55.406+0200] {subprocess.py:93} INFO -   "type" : "struct",
[2025-07-11T11:45:55.406+0200] {subprocess.py:93} INFO -   "fields" : [ {
[2025-07-11T11:45:55.406+0200] {subprocess.py:93} INFO -     "name" : "labelsArray",
[2025-07-11T11:45:55.406+0200] {subprocess.py:93} INFO -     "type" : {
[2025-07-11T11:45:55.406+0200] {subprocess.py:93} INFO -       "type" : "array",
[2025-07-11T11:45:55.406+0200] {subprocess.py:93} INFO -       "elementType" : {
[2025-07-11T11:45:55.406+0200] {subprocess.py:93} INFO -         "type" : "array",
[2025-07-11T11:45:55.406+0200] {subprocess.py:93} INFO -         "elementType" : "string",
[2025-07-11T11:45:55.406+0200] {subprocess.py:93} INFO -         "containsNull" : true
[2025-07-11T11:45:55.406+0200] {subprocess.py:93} INFO -       },
[2025-07-11T11:45:55.406+0200] {subprocess.py:93} INFO -       "containsNull" : true
[2025-07-11T11:45:55.406+0200] {subprocess.py:93} INFO -     },
[2025-07-11T11:45:55.406+0200] {subprocess.py:93} INFO -     "nullable" : true,
[2025-07-11T11:45:55.406+0200] {subprocess.py:93} INFO -     "metadata" : { }
[2025-07-11T11:45:55.407+0200] {subprocess.py:93} INFO -   } ]
[2025-07-11T11:45:55.407+0200] {subprocess.py:93} INFO - }
[2025-07-11T11:45:55.407+0200] {subprocess.py:93} INFO - and corresponding Parquet message type:
[2025-07-11T11:45:55.407+0200] {subprocess.py:93} INFO - message spark_schema {
[2025-07-11T11:45:55.407+0200] {subprocess.py:93} INFO -   optional group labelsArray (LIST) {
[2025-07-11T11:45:55.407+0200] {subprocess.py:93} INFO -     repeated group list {
[2025-07-11T11:45:55.407+0200] {subprocess.py:93} INFO -       optional group element (LIST) {
[2025-07-11T11:45:55.407+0200] {subprocess.py:93} INFO -         repeated group list {
[2025-07-11T11:45:55.407+0200] {subprocess.py:93} INFO -           optional binary element (STRING);
[2025-07-11T11:45:55.407+0200] {subprocess.py:93} INFO -         }
[2025-07-11T11:45:55.407+0200] {subprocess.py:93} INFO -       }
[2025-07-11T11:45:55.407+0200] {subprocess.py:93} INFO -     }
[2025-07-11T11:45:55.407+0200] {subprocess.py:93} INFO -   }
[2025-07-11T11:45:55.407+0200] {subprocess.py:93} INFO - }
[2025-07-11T11:45:55.407+0200] {subprocess.py:93} INFO - 
[2025-07-11T11:45:55.407+0200] {subprocess.py:93} INFO - 
[2025-07-11T11:45:55.448+0200] {subprocess.py:93} INFO - 25/07/11 11:45:55 INFO FileOutputCommitter: Saved output of task 'attempt_202507111145557356070694410785984_0026_m_000000_40' to file:/home/monica.fernandez/practica_creativa/models/string_indexer_model_Origin.bin/data/_temporary/0/task_202507111145557356070694410785984_0026_m_000000
[2025-07-11T11:45:55.448+0200] {subprocess.py:93} INFO - 25/07/11 11:45:55 INFO SparkHadoopMapRedUtil: attempt_202507111145557356070694410785984_0026_m_000000_40: Committed. Elapsed time: 1 ms.
[2025-07-11T11:45:55.449+0200] {subprocess.py:93} INFO - 25/07/11 11:45:55 INFO Executor: Finished task 0.0 in stage 26.0 (TID 40). 4740 bytes result sent to driver
[2025-07-11T11:45:55.451+0200] {subprocess.py:93} INFO - 25/07/11 11:45:55 INFO TaskSetManager: Finished task 0.0 in stage 26.0 (TID 40) in 59 ms on 138.4.31.14 (executor driver) (1/1)
[2025-07-11T11:45:55.451+0200] {subprocess.py:93} INFO - 25/07/11 11:45:55 INFO TaskSchedulerImpl: Removed TaskSet 26.0, whose tasks have all completed, from pool
[2025-07-11T11:45:55.452+0200] {subprocess.py:93} INFO - 25/07/11 11:45:55 INFO DAGScheduler: ResultStage 26 (parquet at StringIndexer.scala:499) finished in 0,084 s
[2025-07-11T11:45:55.452+0200] {subprocess.py:93} INFO - 25/07/11 11:45:55 INFO DAGScheduler: Job 16 is finished. Cancelling potential speculative or zombie tasks for this job
[2025-07-11T11:45:55.452+0200] {subprocess.py:93} INFO - 25/07/11 11:45:55 INFO TaskSchedulerImpl: Killing all running tasks in stage 26: Stage finished
[2025-07-11T11:45:55.452+0200] {subprocess.py:93} INFO - 25/07/11 11:45:55 INFO DAGScheduler: Job 16 finished: parquet at StringIndexer.scala:499, took 0,088285 s
[2025-07-11T11:45:55.452+0200] {subprocess.py:93} INFO - 25/07/11 11:45:55 INFO FileFormatWriter: Start to commit write Job 8bcf52a4-2307-46ef-9830-543f837e3846.
[2025-07-11T11:45:55.474+0200] {subprocess.py:93} INFO - 25/07/11 11:45:55 INFO FileFormatWriter: Write Job 8bcf52a4-2307-46ef-9830-543f837e3846 committed. Elapsed time: 22 ms.
[2025-07-11T11:45:55.476+0200] {subprocess.py:93} INFO - 25/07/11 11:45:55 INFO FileFormatWriter: Finished processing stats for write job 8bcf52a4-2307-46ef-9830-543f837e3846.
[2025-07-11T11:45:55.538+0200] {subprocess.py:93} INFO - 25/07/11 11:45:55 INFO V2ScanRelationPushDown:
[2025-07-11T11:45:55.538+0200] {subprocess.py:93} INFO - Pushing operators to origin_dest_distances
[2025-07-11T11:45:55.539+0200] {subprocess.py:93} INFO - Pushed Filters:
[2025-07-11T11:45:55.539+0200] {subprocess.py:93} INFO - Post-Scan Filters:
[2025-07-11T11:45:55.539+0200] {subprocess.py:93} INFO - 
[2025-07-11T11:45:55.549+0200] {subprocess.py:93} INFO - 25/07/11 11:45:55 INFO V2ScanRelationPushDown:
[2025-07-11T11:45:55.550+0200] {subprocess.py:93} INFO - Output: origin#42, dest#43
[2025-07-11T11:45:55.550+0200] {subprocess.py:93} INFO - 
[2025-07-11T11:45:55.567+0200] {subprocess.py:93} INFO - 25/07/11 11:45:55 INFO FileSourceStrategy: Pushed Filters:
[2025-07-11T11:45:55.567+0200] {subprocess.py:93} INFO - 25/07/11 11:45:55 INFO FileSourceStrategy: Post-Scan Filters:
[2025-07-11T11:45:55.584+0200] {subprocess.py:93} INFO - 25/07/11 11:45:55 INFO MemoryStore: Block broadcast_21 stored as values in memory (estimated size 209.9 KiB, free 424.3 MiB)
[2025-07-11T11:45:55.592+0200] {subprocess.py:93} INFO - 25/07/11 11:45:55 INFO MemoryStore: Block broadcast_21_piece0 stored as bytes in memory (estimated size 37.7 KiB, free 424.3 MiB)
[2025-07-11T11:45:55.592+0200] {subprocess.py:93} INFO - 25/07/11 11:45:55 INFO BlockManagerInfo: Added broadcast_21_piece0 in memory on 138.4.31.14:45609 (size: 37.7 KiB, free: 433.9 MiB)
[2025-07-11T11:45:55.592+0200] {subprocess.py:93} INFO - 25/07/11 11:45:55 INFO SparkContext: Created broadcast 21 from collect at StringIndexer.scala:204
[2025-07-11T11:45:55.593+0200] {subprocess.py:93} INFO - 25/07/11 11:45:55 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
[2025-07-11T11:45:55.596+0200] {subprocess.py:93} INFO - 25/07/11 11:45:55 INFO DAGScheduler: Registering RDD 50 (collect at StringIndexer.scala:204) as input to shuffle 8
[2025-07-11T11:45:55.597+0200] {subprocess.py:93} INFO - 25/07/11 11:45:55 INFO DAGScheduler: Got map stage job 17 (collect at StringIndexer.scala:204) with 2 output partitions
[2025-07-11T11:45:55.597+0200] {subprocess.py:93} INFO - 25/07/11 11:45:55 INFO DAGScheduler: Final stage: ShuffleMapStage 27 (collect at StringIndexer.scala:204)
[2025-07-11T11:45:55.597+0200] {subprocess.py:93} INFO - 25/07/11 11:45:55 INFO DAGScheduler: Parents of final stage: List()
[2025-07-11T11:45:55.597+0200] {subprocess.py:93} INFO - 25/07/11 11:45:55 INFO DAGScheduler: Missing parents: List()
[2025-07-11T11:45:55.597+0200] {subprocess.py:93} INFO - 25/07/11 11:45:55 INFO DAGScheduler: Submitting ShuffleMapStage 27 (MapPartitionsRDD[50] at collect at StringIndexer.scala:204), which has no missing parents
[2025-07-11T11:45:55.599+0200] {subprocess.py:93} INFO - 25/07/11 11:45:55 INFO MemoryStore: Block broadcast_22 stored as values in memory (estimated size 13.7 KiB, free 424.3 MiB)
[2025-07-11T11:45:55.599+0200] {subprocess.py:93} INFO - 25/07/11 11:45:55 INFO MemoryStore: Block broadcast_22_piece0 stored as bytes in memory (estimated size 7.0 KiB, free 424.3 MiB)
[2025-07-11T11:45:55.599+0200] {subprocess.py:93} INFO - 25/07/11 11:45:55 INFO BlockManagerInfo: Added broadcast_22_piece0 in memory on 138.4.31.14:45609 (size: 7.0 KiB, free: 433.9 MiB)
[2025-07-11T11:45:55.600+0200] {subprocess.py:93} INFO - 25/07/11 11:45:55 INFO SparkContext: Created broadcast 22 from broadcast at DAGScheduler.scala:1585
[2025-07-11T11:45:55.601+0200] {subprocess.py:93} INFO - 25/07/11 11:45:55 INFO DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 27 (MapPartitionsRDD[50] at collect at StringIndexer.scala:204) (first 15 tasks are for partitions Vector(0, 1))
[2025-07-11T11:45:55.601+0200] {subprocess.py:93} INFO - 25/07/11 11:45:55 INFO TaskSchedulerImpl: Adding task set 27.0 with 2 tasks resource profile 0
[2025-07-11T11:45:55.602+0200] {subprocess.py:93} INFO - 25/07/11 11:45:55 INFO TaskSetManager: Starting task 0.0 in stage 27.0 (TID 41) (138.4.31.14, executor driver, partition 0, PROCESS_LOCAL, 13595 bytes)
[2025-07-11T11:45:55.603+0200] {subprocess.py:93} INFO - 25/07/11 11:45:55 INFO TaskSetManager: Starting task 1.0 in stage 27.0 (TID 42) (138.4.31.14, executor driver, partition 1, PROCESS_LOCAL, 13595 bytes)
[2025-07-11T11:45:55.603+0200] {subprocess.py:93} INFO - 25/07/11 11:45:55 INFO Executor: Running task 0.0 in stage 27.0 (TID 41)
[2025-07-11T11:45:55.607+0200] {subprocess.py:93} INFO - 25/07/11 11:45:55 INFO Executor: Running task 1.0 in stage 27.0 (TID 42)
[2025-07-11T11:45:55.609+0200] {subprocess.py:93} INFO - 25/07/11 11:45:55 INFO FileScanRDD: Reading File path: file:///home/monica.fernandez/practica_creativa/data/simple_flight_delay_features.jsonl.bz2, range: 0-4194304, partition values: [empty row]
[2025-07-11T11:45:55.610+0200] {subprocess.py:93} INFO - 25/07/11 11:45:55 INFO DAGScheduler: Registering RDD 54 (collect at StringIndexer.scala:204) as input to shuffle 9
[2025-07-11T11:45:55.610+0200] {subprocess.py:93} INFO - 25/07/11 11:45:55 INFO DAGScheduler: Got map stage job 18 (collect at StringIndexer.scala:204) with 12 output partitions
[2025-07-11T11:45:55.610+0200] {subprocess.py:93} INFO - 25/07/11 11:45:55 INFO DAGScheduler: Final stage: ShuffleMapStage 28 (collect at StringIndexer.scala:204)
[2025-07-11T11:45:55.610+0200] {subprocess.py:93} INFO - 25/07/11 11:45:55 INFO DAGScheduler: Parents of final stage: List()
[2025-07-11T11:45:55.610+0200] {subprocess.py:93} INFO - 25/07/11 11:45:55 INFO DAGScheduler: Missing parents: List()
[2025-07-11T11:45:55.610+0200] {subprocess.py:93} INFO - 25/07/11 11:45:55 INFO DAGScheduler: Submitting ShuffleMapStage 28 (MapPartitionsRDD[54] at collect at StringIndexer.scala:204), which has no missing parents
[2025-07-11T11:45:55.613+0200] {subprocess.py:93} INFO - 25/07/11 11:45:55 INFO FileScanRDD: Reading File path: file:///home/monica.fernandez/practica_creativa/data/simple_flight_delay_features.jsonl.bz2, range: 4194304-4676447, partition values: [empty row]
[2025-07-11T11:45:55.614+0200] {subprocess.py:93} INFO - 25/07/11 11:45:55 INFO MemoryStore: Block broadcast_23 stored as values in memory (estimated size 18.2 KiB, free 424.3 MiB)
[2025-07-11T11:45:55.615+0200] {subprocess.py:93} INFO - 25/07/11 11:45:55 INFO MemoryStore: Block broadcast_23_piece0 stored as bytes in memory (estimated size 9.0 KiB, free 424.3 MiB)
[2025-07-11T11:45:55.615+0200] {subprocess.py:93} INFO - 25/07/11 11:45:55 INFO BlockManagerInfo: Added broadcast_23_piece0 in memory on 138.4.31.14:45609 (size: 9.0 KiB, free: 433.9 MiB)
[2025-07-11T11:45:55.615+0200] {subprocess.py:93} INFO - 25/07/11 11:45:55 INFO SparkContext: Created broadcast 23 from broadcast at DAGScheduler.scala:1585
[2025-07-11T11:45:55.616+0200] {subprocess.py:93} INFO - 25/07/11 11:45:55 INFO DAGScheduler: Submitting 12 missing tasks from ShuffleMapStage 28 (MapPartitionsRDD[54] at collect at StringIndexer.scala:204) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11))
[2025-07-11T11:45:55.616+0200] {subprocess.py:93} INFO - 25/07/11 11:45:55 INFO TaskSchedulerImpl: Adding task set 28.0 with 12 tasks resource profile 0
[2025-07-11T11:45:55.617+0200] {subprocess.py:93} INFO - 25/07/11 11:45:55 INFO TaskSetManager: Starting task 0.0 in stage 28.0 (TID 43) (138.4.31.14, executor driver, partition 0, ANY, 14843 bytes)
[2025-07-11T11:45:55.623+0200] {subprocess.py:93} INFO - 25/07/11 11:45:55 INFO TaskSetManager: Starting task 1.0 in stage 28.0 (TID 44) (138.4.31.14, executor driver, partition 1, ANY, 14843 bytes)
[2025-07-11T11:45:55.623+0200] {subprocess.py:93} INFO - 25/07/11 11:45:55 INFO Executor: Running task 0.0 in stage 28.0 (TID 43)
[2025-07-11T11:45:55.623+0200] {subprocess.py:93} INFO - 25/07/11 11:45:55 INFO Executor: Running task 1.0 in stage 28.0 (TID 44)
[2025-07-11T11:45:55.693+0200] {subprocess.py:93} INFO - 25/07/11 11:45:55 INFO Executor: Finished task 1.0 in stage 28.0 (TID 44). 1945 bytes result sent to driver
[2025-07-11T11:45:55.700+0200] {subprocess.py:93} INFO - 25/07/11 11:45:55 INFO TaskSetManager: Starting task 2.0 in stage 28.0 (TID 45) (138.4.31.14, executor driver, partition 2, ANY, 14963 bytes)
[2025-07-11T11:45:55.701+0200] {subprocess.py:93} INFO - 25/07/11 11:45:55 INFO TaskSetManager: Finished task 1.0 in stage 28.0 (TID 44) in 84 ms on 138.4.31.14 (executor driver) (1/12)
[2025-07-11T11:45:55.701+0200] {subprocess.py:93} INFO - 25/07/11 11:45:55 INFO Executor: Running task 2.0 in stage 28.0 (TID 45)
[2025-07-11T11:45:55.736+0200] {subprocess.py:93} INFO - 25/07/11 11:45:55 INFO Executor: Finished task 0.0 in stage 28.0 (TID 43). 1988 bytes result sent to driver
[2025-07-11T11:45:55.737+0200] {subprocess.py:93} INFO - 25/07/11 11:45:55 INFO TaskSetManager: Starting task 3.0 in stage 28.0 (TID 46) (138.4.31.14, executor driver, partition 3, ANY, 14843 bytes)
[2025-07-11T11:45:55.738+0200] {subprocess.py:93} INFO - 25/07/11 11:45:55 INFO TaskSetManager: Finished task 0.0 in stage 28.0 (TID 43) in 121 ms on 138.4.31.14 (executor driver) (2/12)
[2025-07-11T11:45:55.738+0200] {subprocess.py:93} INFO - 25/07/11 11:45:55 INFO Executor: Running task 3.0 in stage 28.0 (TID 46)
[2025-07-11T11:45:55.845+0200] {subprocess.py:93} INFO - 25/07/11 11:45:55 INFO Executor: Finished task 2.0 in stage 28.0 (TID 45). 1988 bytes result sent to driver
[2025-07-11T11:45:55.859+0200] {subprocess.py:93} INFO - 25/07/11 11:45:55 INFO TaskSetManager: Starting task 4.0 in stage 28.0 (TID 47) (138.4.31.14, executor driver, partition 4, ANY, 14959 bytes)
[2025-07-11T11:45:55.859+0200] {subprocess.py:93} INFO - 25/07/11 11:45:55 INFO Executor: Running task 4.0 in stage 28.0 (TID 47)
[2025-07-11T11:45:55.859+0200] {subprocess.py:93} INFO - 25/07/11 11:45:55 INFO TaskSetManager: Finished task 2.0 in stage 28.0 (TID 45) in 159 ms on 138.4.31.14 (executor driver) (3/12)
[2025-07-11T11:45:55.860+0200] {subprocess.py:93} INFO - 25/07/11 11:45:55 INFO BlockManagerInfo: Removed broadcast_17_piece0 on 138.4.31.14:45609 in memory (size: 23.3 KiB, free: 433.9 MiB)
[2025-07-11T11:45:55.901+0200] {subprocess.py:93} INFO - 25/07/11 11:45:55 INFO BlockManagerInfo: Removed broadcast_19_piece0 on 138.4.31.14:45609 in memory (size: 4.4 KiB, free: 433.9 MiB)
[2025-07-11T11:45:55.918+0200] {subprocess.py:93} INFO - 25/07/11 11:45:55 INFO Executor: Finished task 3.0 in stage 28.0 (TID 46). 1988 bytes result sent to driver
[2025-07-11T11:45:55.927+0200] {subprocess.py:93} INFO - 25/07/11 11:45:55 INFO TaskSetManager: Starting task 5.0 in stage 28.0 (TID 48) (138.4.31.14, executor driver, partition 5, ANY, 14843 bytes)
[2025-07-11T11:45:55.928+0200] {subprocess.py:93} INFO - 25/07/11 11:45:55 INFO Executor: Running task 5.0 in stage 28.0 (TID 48)
[2025-07-11T11:45:55.928+0200] {subprocess.py:93} INFO - 25/07/11 11:45:55 INFO TaskSetManager: Finished task 3.0 in stage 28.0 (TID 46) in 192 ms on 138.4.31.14 (executor driver) (4/12)
[2025-07-11T11:45:55.998+0200] {subprocess.py:93} INFO - 25/07/11 11:45:55 INFO BlockManagerInfo: Removed broadcast_16_piece0 on 138.4.31.14:45609 in memory (size: 23.6 KiB, free: 434.0 MiB)
[2025-07-11T11:45:56.081+0200] {subprocess.py:93} INFO - 25/07/11 11:45:56 INFO Executor: Finished task 4.0 in stage 28.0 (TID 47). 1945 bytes result sent to driver
[2025-07-11T11:45:56.084+0200] {subprocess.py:93} INFO - 25/07/11 11:45:56 INFO TaskSetManager: Starting task 6.0 in stage 28.0 (TID 49) (138.4.31.14, executor driver, partition 6, ANY, 14843 bytes)
[2025-07-11T11:45:56.086+0200] {subprocess.py:93} INFO - 25/07/11 11:45:56 INFO TaskSetManager: Finished task 4.0 in stage 28.0 (TID 47) in 226 ms on 138.4.31.14 (executor driver) (5/12)
[2025-07-11T11:45:56.087+0200] {subprocess.py:93} INFO - 25/07/11 11:45:56 INFO Executor: Running task 6.0 in stage 28.0 (TID 49)
[2025-07-11T11:45:56.097+0200] {subprocess.py:93} INFO - 25/07/11 11:45:56 INFO Executor: Finished task 5.0 in stage 28.0 (TID 48). 1945 bytes result sent to driver
[2025-07-11T11:45:56.101+0200] {subprocess.py:93} INFO - 25/07/11 11:45:56 INFO TaskSetManager: Starting task 7.0 in stage 28.0 (TID 50) (138.4.31.14, executor driver, partition 7, ANY, 14843 bytes)
[2025-07-11T11:45:56.102+0200] {subprocess.py:93} INFO - 25/07/11 11:45:56 INFO TaskSetManager: Finished task 5.0 in stage 28.0 (TID 48) in 175 ms on 138.4.31.14 (executor driver) (6/12)
[2025-07-11T11:45:56.103+0200] {subprocess.py:93} INFO - 25/07/11 11:45:56 INFO Executor: Running task 7.0 in stage 28.0 (TID 50)
[2025-07-11T11:45:56.124+0200] {subprocess.py:93} INFO - 25/07/11 11:45:56 INFO BlockManagerInfo: Removed broadcast_20_piece0 on 138.4.31.14:45609 in memory (size: 80.9 KiB, free: 434.0 MiB)
[2025-07-11T11:45:56.169+0200] {subprocess.py:93} INFO - 25/07/11 11:45:56 INFO BlockManagerInfo: Removed broadcast_18_piece0 on 138.4.31.14:45609 in memory (size: 36.6 KiB, free: 434.1 MiB)
[2025-07-11T11:45:56.184+0200] {subprocess.py:93} INFO - 25/07/11 11:45:56 INFO Executor: Finished task 6.0 in stage 28.0 (TID 49). 1945 bytes result sent to driver
[2025-07-11T11:45:56.186+0200] {subprocess.py:93} INFO - 25/07/11 11:45:56 INFO TaskSetManager: Starting task 8.0 in stage 28.0 (TID 51) (138.4.31.14, executor driver, partition 8, ANY, 14963 bytes)
[2025-07-11T11:45:56.186+0200] {subprocess.py:93} INFO - 25/07/11 11:45:56 INFO TaskSetManager: Finished task 6.0 in stage 28.0 (TID 49) in 102 ms on 138.4.31.14 (executor driver) (7/12)
[2025-07-11T11:45:56.188+0200] {subprocess.py:93} INFO - 25/07/11 11:45:56 INFO Executor: Running task 8.0 in stage 28.0 (TID 51)
[2025-07-11T11:45:56.240+0200] {subprocess.py:93} INFO - 25/07/11 11:45:56 INFO Executor: Finished task 7.0 in stage 28.0 (TID 50). 1945 bytes result sent to driver
[2025-07-11T11:45:56.242+0200] {subprocess.py:93} INFO - 25/07/11 11:45:56 INFO TaskSetManager: Starting task 9.0 in stage 28.0 (TID 52) (138.4.31.14, executor driver, partition 9, ANY, 14843 bytes)
[2025-07-11T11:45:56.242+0200] {subprocess.py:93} INFO - 25/07/11 11:45:56 INFO TaskSetManager: Finished task 7.0 in stage 28.0 (TID 50) in 141 ms on 138.4.31.14 (executor driver) (8/12)
[2025-07-11T11:45:56.243+0200] {subprocess.py:93} INFO - 25/07/11 11:45:56 INFO Executor: Running task 9.0 in stage 28.0 (TID 52)
[2025-07-11T11:45:56.281+0200] {subprocess.py:93} INFO - 25/07/11 11:45:56 INFO Executor: Finished task 8.0 in stage 28.0 (TID 51). 1945 bytes result sent to driver
[2025-07-11T11:45:56.282+0200] {subprocess.py:93} INFO - 25/07/11 11:45:56 INFO TaskSetManager: Starting task 10.0 in stage 28.0 (TID 53) (138.4.31.14, executor driver, partition 10, ANY, 14843 bytes)
[2025-07-11T11:45:56.283+0200] {subprocess.py:93} INFO - 25/07/11 11:45:56 INFO TaskSetManager: Finished task 8.0 in stage 28.0 (TID 51) in 98 ms on 138.4.31.14 (executor driver) (9/12)
[2025-07-11T11:45:56.284+0200] {subprocess.py:93} INFO - 25/07/11 11:45:56 INFO Executor: Running task 10.0 in stage 28.0 (TID 53)
[2025-07-11T11:45:56.404+0200] {subprocess.py:93} INFO - 25/07/11 11:45:56 INFO Executor: Finished task 9.0 in stage 28.0 (TID 52). 1945 bytes result sent to driver
[2025-07-11T11:45:56.406+0200] {subprocess.py:93} INFO - 25/07/11 11:45:56 INFO TaskSetManager: Starting task 11.0 in stage 28.0 (TID 54) (138.4.31.14, executor driver, partition 11, ANY, 14963 bytes)
[2025-07-11T11:45:56.407+0200] {subprocess.py:93} INFO - 25/07/11 11:45:56 INFO Executor: Running task 11.0 in stage 28.0 (TID 54)
[2025-07-11T11:45:56.408+0200] {subprocess.py:93} INFO - 25/07/11 11:45:56 INFO TaskSetManager: Finished task 9.0 in stage 28.0 (TID 52) in 166 ms on 138.4.31.14 (executor driver) (10/12)
[2025-07-11T11:45:56.438+0200] {subprocess.py:93} INFO - 25/07/11 11:45:56 INFO Executor: Finished task 10.0 in stage 28.0 (TID 53). 1945 bytes result sent to driver
[2025-07-11T11:45:56.439+0200] {subprocess.py:93} INFO - 25/07/11 11:45:56 INFO TaskSetManager: Finished task 10.0 in stage 28.0 (TID 53) in 157 ms on 138.4.31.14 (executor driver) (11/12)
[2025-07-11T11:45:56.504+0200] {subprocess.py:93} INFO - 25/07/11 11:45:56 INFO Executor: Finished task 11.0 in stage 28.0 (TID 54). 1988 bytes result sent to driver
[2025-07-11T11:45:56.506+0200] {subprocess.py:93} INFO - 25/07/11 11:45:56 INFO TaskSetManager: Finished task 11.0 in stage 28.0 (TID 54) in 102 ms on 138.4.31.14 (executor driver) (12/12)
[2025-07-11T11:45:56.508+0200] {subprocess.py:93} INFO - 25/07/11 11:45:56 INFO TaskSchedulerImpl: Removed TaskSet 28.0, whose tasks have all completed, from pool
[2025-07-11T11:45:56.509+0200] {subprocess.py:93} INFO - 25/07/11 11:45:56 INFO DAGScheduler: ShuffleMapStage 28 (collect at StringIndexer.scala:204) finished in 0,894 s
[2025-07-11T11:45:56.509+0200] {subprocess.py:93} INFO - 25/07/11 11:45:56 INFO DAGScheduler: looking for newly runnable stages
[2025-07-11T11:45:56.509+0200] {subprocess.py:93} INFO - 25/07/11 11:45:56 INFO DAGScheduler: running: Set(ShuffleMapStage 27)
[2025-07-11T11:45:56.511+0200] {subprocess.py:93} INFO - 25/07/11 11:45:56 INFO DAGScheduler: waiting: Set()
[2025-07-11T11:45:56.511+0200] {subprocess.py:93} INFO - 25/07/11 11:45:56 INFO DAGScheduler: failed: Set()
[2025-07-11T11:45:56.547+0200] {subprocess.py:93} INFO - 25/07/11 11:45:56 INFO ShufflePartitionsUtil: For shuffle(9), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576
[2025-07-11T11:45:56.596+0200] {subprocess.py:93} INFO - 25/07/11 11:45:56 INFO SparkContext: Starting job: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264
[2025-07-11T11:45:56.600+0200] {subprocess.py:93} INFO - 25/07/11 11:45:56 INFO DAGScheduler: Got job 19 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) with 1 output partitions
[2025-07-11T11:45:56.600+0200] {subprocess.py:93} INFO - 25/07/11 11:45:56 INFO DAGScheduler: Final stage: ResultStage 30 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264)
[2025-07-11T11:45:56.600+0200] {subprocess.py:93} INFO - 25/07/11 11:45:56 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 29)
[2025-07-11T11:45:56.600+0200] {subprocess.py:93} INFO - 25/07/11 11:45:56 INFO DAGScheduler: Missing parents: List()
[2025-07-11T11:45:56.600+0200] {subprocess.py:93} INFO - 25/07/11 11:45:56 INFO DAGScheduler: Submitting ResultStage 30 (MapPartitionsRDD[56] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264), which has no missing parents
[2025-07-11T11:45:56.604+0200] {subprocess.py:93} INFO - 25/07/11 11:45:56 INFO MemoryStore: Block broadcast_24 stored as values in memory (estimated size 8.2 KiB, free 424.9 MiB)
[2025-07-11T11:45:56.605+0200] {subprocess.py:93} INFO - 25/07/11 11:45:56 INFO MemoryStore: Block broadcast_24_piece0 stored as bytes in memory (estimated size 4.2 KiB, free 424.8 MiB)
[2025-07-11T11:45:56.605+0200] {subprocess.py:93} INFO - 25/07/11 11:45:56 INFO BlockManagerInfo: Added broadcast_24_piece0 in memory on 138.4.31.14:45609 (size: 4.2 KiB, free: 434.1 MiB)
[2025-07-11T11:45:56.605+0200] {subprocess.py:93} INFO - 25/07/11 11:45:56 INFO SparkContext: Created broadcast 24 from broadcast at DAGScheduler.scala:1585
[2025-07-11T11:45:56.606+0200] {subprocess.py:93} INFO - 25/07/11 11:45:56 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 30 (MapPartitionsRDD[56] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) (first 15 tasks are for partitions Vector(0))
[2025-07-11T11:45:56.606+0200] {subprocess.py:93} INFO - 25/07/11 11:45:56 INFO TaskSchedulerImpl: Adding task set 30.0 with 1 tasks resource profile 0
[2025-07-11T11:45:56.607+0200] {subprocess.py:93} INFO - 25/07/11 11:45:56 INFO TaskSetManager: Starting task 0.0 in stage 30.0 (TID 55) (138.4.31.14, executor driver, partition 0, NODE_LOCAL, 12968 bytes)
[2025-07-11T11:45:56.610+0200] {subprocess.py:93} INFO - 25/07/11 11:45:56 INFO Executor: Running task 0.0 in stage 30.0 (TID 55)
[2025-07-11T11:45:56.615+0200] {subprocess.py:93} INFO - 25/07/11 11:45:56 INFO ShuffleBlockFetcherIterator: Getting 12 (193.5 KiB) non-empty blocks including 12 (193.5 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2025-07-11T11:45:56.615+0200] {subprocess.py:93} INFO - 25/07/11 11:45:56 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
[2025-07-11T11:45:56.654+0200] {subprocess.py:93} INFO - 25/07/11 11:45:56 INFO Executor: Finished task 0.0 in stage 30.0 (TID 55). 40679 bytes result sent to driver
[2025-07-11T11:45:56.655+0200] {subprocess.py:93} INFO - 25/07/11 11:45:56 INFO TaskSetManager: Finished task 0.0 in stage 30.0 (TID 55) in 48 ms on 138.4.31.14 (executor driver) (1/1)
[2025-07-11T11:45:56.655+0200] {subprocess.py:93} INFO - 25/07/11 11:45:56 INFO TaskSchedulerImpl: Removed TaskSet 30.0, whose tasks have all completed, from pool
[2025-07-11T11:45:56.660+0200] {subprocess.py:93} INFO - 25/07/11 11:45:56 INFO DAGScheduler: ResultStage 30 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) finished in 0,057 s
[2025-07-11T11:45:56.660+0200] {subprocess.py:93} INFO - 25/07/11 11:45:56 INFO DAGScheduler: Job 19 is finished. Cancelling potential speculative or zombie tasks for this job
[2025-07-11T11:45:56.660+0200] {subprocess.py:93} INFO - 25/07/11 11:45:56 INFO TaskSchedulerImpl: Killing all running tasks in stage 30: Stage finished
[2025-07-11T11:45:56.660+0200] {subprocess.py:93} INFO - 25/07/11 11:45:56 INFO DAGScheduler: Job 19 finished: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264, took 0,059506 s
[2025-07-11T11:45:56.668+0200] {subprocess.py:93} INFO - 25/07/11 11:45:56 INFO MemoryStore: Block broadcast_25 stored as values in memory (estimated size 4.3 MiB, free 420.6 MiB)
[2025-07-11T11:45:56.675+0200] {subprocess.py:93} INFO - 25/07/11 11:45:56 INFO MemoryStore: Block broadcast_25_piece0 stored as bytes in memory (estimated size 89.8 KiB, free 420.5 MiB)
[2025-07-11T11:45:56.676+0200] {subprocess.py:93} INFO - 25/07/11 11:45:56 INFO BlockManagerInfo: Added broadcast_25_piece0 in memory on 138.4.31.14:45609 (size: 89.8 KiB, free: 434.0 MiB)
[2025-07-11T11:45:56.678+0200] {subprocess.py:93} INFO - 25/07/11 11:45:56 INFO SparkContext: Created broadcast 25 from $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264
[2025-07-11T11:45:57.264+0200] {subprocess.py:93} INFO - 25/07/11 11:45:57 INFO Executor: Finished task 1.0 in stage 27.0 (TID 42). 1959 bytes result sent to driver
[2025-07-11T11:45:57.266+0200] {subprocess.py:93} INFO - 25/07/11 11:45:57 INFO TaskSetManager: Finished task 1.0 in stage 27.0 (TID 42) in 1663 ms on 138.4.31.14 (executor driver) (1/2)
[2025-07-11T11:45:58.435+0200] {subprocess.py:93} INFO - 25/07/11 11:45:58 INFO BlockManagerInfo: Removed broadcast_24_piece0 on 138.4.31.14:45609 in memory (size: 4.2 KiB, free: 434.0 MiB)
[2025-07-11T11:46:03.269+0200] {subprocess.py:93} INFO - 25/07/11 11:46:03 INFO Executor: Finished task 0.0 in stage 27.0 (TID 41). 1959 bytes result sent to driver
[2025-07-11T11:46:03.271+0200] {subprocess.py:93} INFO - 25/07/11 11:46:03 INFO TaskSetManager: Finished task 0.0 in stage 27.0 (TID 41) in 7669 ms on 138.4.31.14 (executor driver) (2/2)
[2025-07-11T11:46:03.271+0200] {subprocess.py:93} INFO - 25/07/11 11:46:03 INFO TaskSchedulerImpl: Removed TaskSet 27.0, whose tasks have all completed, from pool
[2025-07-11T11:46:03.271+0200] {subprocess.py:93} INFO - 25/07/11 11:46:03 INFO DAGScheduler: ShuffleMapStage 27 (collect at StringIndexer.scala:204) finished in 7,674 s
[2025-07-11T11:46:03.271+0200] {subprocess.py:93} INFO - 25/07/11 11:46:03 INFO DAGScheduler: looking for newly runnable stages
[2025-07-11T11:46:03.271+0200] {subprocess.py:93} INFO - 25/07/11 11:46:03 INFO DAGScheduler: running: Set()
[2025-07-11T11:46:03.271+0200] {subprocess.py:93} INFO - 25/07/11 11:46:03 INFO DAGScheduler: waiting: Set()
[2025-07-11T11:46:03.271+0200] {subprocess.py:93} INFO - 25/07/11 11:46:03 INFO DAGScheduler: failed: Set()
[2025-07-11T11:46:03.275+0200] {subprocess.py:93} INFO - 25/07/11 11:46:03 INFO ShufflePartitionsUtil: For shuffle(8), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576
[2025-07-11T11:46:03.293+0200] {subprocess.py:93} INFO - 25/07/11 11:46:03 INFO CodeGenerator: Code generated in 8.067095 ms
[2025-07-11T11:46:03.297+0200] {subprocess.py:93} INFO - 25/07/11 11:46:03 INFO DAGScheduler: Registering RDD 60 (collect at StringIndexer.scala:204) as input to shuffle 10
[2025-07-11T11:46:03.297+0200] {subprocess.py:93} INFO - 25/07/11 11:46:03 INFO DAGScheduler: Got map stage job 20 (collect at StringIndexer.scala:204) with 1 output partitions
[2025-07-11T11:46:03.297+0200] {subprocess.py:93} INFO - 25/07/11 11:46:03 INFO DAGScheduler: Final stage: ShuffleMapStage 32 (collect at StringIndexer.scala:204)
[2025-07-11T11:46:03.297+0200] {subprocess.py:93} INFO - 25/07/11 11:46:03 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 31)
[2025-07-11T11:46:03.297+0200] {subprocess.py:93} INFO - 25/07/11 11:46:03 INFO DAGScheduler: Missing parents: List()
[2025-07-11T11:46:03.297+0200] {subprocess.py:93} INFO - 25/07/11 11:46:03 INFO DAGScheduler: Submitting ShuffleMapStage 32 (MapPartitionsRDD[60] at collect at StringIndexer.scala:204), which has no missing parents
[2025-07-11T11:46:03.299+0200] {subprocess.py:93} INFO - 25/07/11 11:46:03 INFO MemoryStore: Block broadcast_26 stored as values in memory (estimated size 49.4 KiB, free 420.5 MiB)
[2025-07-11T11:46:03.300+0200] {subprocess.py:93} INFO - 25/07/11 11:46:03 INFO MemoryStore: Block broadcast_26_piece0 stored as bytes in memory (estimated size 23.6 KiB, free 420.4 MiB)
[2025-07-11T11:46:03.300+0200] {subprocess.py:93} INFO - 25/07/11 11:46:03 INFO BlockManagerInfo: Added broadcast_26_piece0 in memory on 138.4.31.14:45609 (size: 23.6 KiB, free: 434.0 MiB)
[2025-07-11T11:46:03.301+0200] {subprocess.py:93} INFO - 25/07/11 11:46:03 INFO SparkContext: Created broadcast 26 from broadcast at DAGScheduler.scala:1585
[2025-07-11T11:46:03.301+0200] {subprocess.py:93} INFO - 25/07/11 11:46:03 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 32 (MapPartitionsRDD[60] at collect at StringIndexer.scala:204) (first 15 tasks are for partitions Vector(0))
[2025-07-11T11:46:03.301+0200] {subprocess.py:93} INFO - 25/07/11 11:46:03 INFO TaskSchedulerImpl: Adding task set 32.0 with 1 tasks resource profile 0
[2025-07-11T11:46:03.302+0200] {subprocess.py:93} INFO - 25/07/11 11:46:03 INFO TaskSetManager: Starting task 0.0 in stage 32.0 (TID 56) (138.4.31.14, executor driver, partition 0, NODE_LOCAL, 12957 bytes)
[2025-07-11T11:46:03.302+0200] {subprocess.py:93} INFO - 25/07/11 11:46:03 INFO Executor: Running task 0.0 in stage 32.0 (TID 56)
[2025-07-11T11:46:03.307+0200] {subprocess.py:93} INFO - 25/07/11 11:46:03 INFO ShuffleBlockFetcherIterator: Getting 2 (887.5 KiB) non-empty blocks including 2 (887.5 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2025-07-11T11:46:03.307+0200] {subprocess.py:93} INFO - 25/07/11 11:46:03 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2025-07-11T11:46:03.317+0200] {subprocess.py:93} INFO - 25/07/11 11:46:03 INFO CodeGenerator: Code generated in 9.243716 ms
[2025-07-11T11:46:03.604+0200] {subprocess.py:93} INFO - 25/07/11 11:46:03 INFO Executor: Finished task 0.0 in stage 32.0 (TID 56). 6487 bytes result sent to driver
[2025-07-11T11:46:03.604+0200] {subprocess.py:93} INFO - 25/07/11 11:46:03 INFO TaskSetManager: Finished task 0.0 in stage 32.0 (TID 56) in 302 ms on 138.4.31.14 (executor driver) (1/1)
[2025-07-11T11:46:03.604+0200] {subprocess.py:93} INFO - 25/07/11 11:46:03 INFO TaskSchedulerImpl: Removed TaskSet 32.0, whose tasks have all completed, from pool
[2025-07-11T11:46:03.605+0200] {subprocess.py:93} INFO - 25/07/11 11:46:03 INFO DAGScheduler: ShuffleMapStage 32 (collect at StringIndexer.scala:204) finished in 0,308 s
[2025-07-11T11:46:03.606+0200] {subprocess.py:93} INFO - 25/07/11 11:46:03 INFO DAGScheduler: looking for newly runnable stages
[2025-07-11T11:46:03.606+0200] {subprocess.py:93} INFO - 25/07/11 11:46:03 INFO DAGScheduler: running: Set()
[2025-07-11T11:46:03.606+0200] {subprocess.py:93} INFO - 25/07/11 11:46:03 INFO DAGScheduler: waiting: Set()
[2025-07-11T11:46:03.606+0200] {subprocess.py:93} INFO - 25/07/11 11:46:03 INFO DAGScheduler: failed: Set()
[2025-07-11T11:46:03.620+0200] {subprocess.py:93} INFO - 25/07/11 11:46:03 INFO SparkContext: Starting job: collect at StringIndexer.scala:204
[2025-07-11T11:46:03.621+0200] {subprocess.py:93} INFO - 25/07/11 11:46:03 INFO DAGScheduler: Got job 21 (collect at StringIndexer.scala:204) with 1 output partitions
[2025-07-11T11:46:03.621+0200] {subprocess.py:93} INFO - 25/07/11 11:46:03 INFO DAGScheduler: Final stage: ResultStage 35 (collect at StringIndexer.scala:204)
[2025-07-11T11:46:03.621+0200] {subprocess.py:93} INFO - 25/07/11 11:46:03 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 34)
[2025-07-11T11:46:03.622+0200] {subprocess.py:93} INFO - 25/07/11 11:46:03 INFO DAGScheduler: Missing parents: List()
[2025-07-11T11:46:03.622+0200] {subprocess.py:93} INFO - 25/07/11 11:46:03 INFO DAGScheduler: Submitting ResultStage 35 (MapPartitionsRDD[63] at collect at StringIndexer.scala:204), which has no missing parents
[2025-07-11T11:46:03.626+0200] {subprocess.py:93} INFO - 25/07/11 11:46:03 INFO MemoryStore: Block broadcast_27 stored as values in memory (estimated size 48.7 KiB, free 420.4 MiB)
[2025-07-11T11:46:03.626+0200] {subprocess.py:93} INFO - 25/07/11 11:46:03 INFO MemoryStore: Block broadcast_27_piece0 stored as bytes in memory (estimated size 23.4 KiB, free 420.4 MiB)
[2025-07-11T11:46:03.626+0200] {subprocess.py:93} INFO - 25/07/11 11:46:03 INFO BlockManagerInfo: Added broadcast_27_piece0 in memory on 138.4.31.14:45609 (size: 23.4 KiB, free: 433.9 MiB)
[2025-07-11T11:46:03.626+0200] {subprocess.py:93} INFO - 25/07/11 11:46:03 INFO SparkContext: Created broadcast 27 from broadcast at DAGScheduler.scala:1585
[2025-07-11T11:46:03.627+0200] {subprocess.py:93} INFO - 25/07/11 11:46:03 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 35 (MapPartitionsRDD[63] at collect at StringIndexer.scala:204) (first 15 tasks are for partitions Vector(0))
[2025-07-11T11:46:03.627+0200] {subprocess.py:93} INFO - 25/07/11 11:46:03 INFO TaskSchedulerImpl: Adding task set 35.0 with 1 tasks resource profile 0
[2025-07-11T11:46:03.628+0200] {subprocess.py:93} INFO - 25/07/11 11:46:03 INFO TaskSetManager: Starting task 0.0 in stage 35.0 (TID 57) (138.4.31.14, executor driver, partition 0, NODE_LOCAL, 12952 bytes)
[2025-07-11T11:46:03.628+0200] {subprocess.py:93} INFO - 25/07/11 11:46:03 INFO Executor: Running task 0.0 in stage 35.0 (TID 57)
[2025-07-11T11:46:03.633+0200] {subprocess.py:93} INFO - 25/07/11 11:46:03 INFO ShuffleBlockFetcherIterator: Getting 1 (3.2 KiB) non-empty blocks including 1 (3.2 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2025-07-11T11:46:03.634+0200] {subprocess.py:93} INFO - 25/07/11 11:46:03 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2025-07-11T11:46:03.650+0200] {subprocess.py:93} INFO - 25/07/11 11:46:03 INFO Executor: Finished task 0.0 in stage 35.0 (TID 57). 10907 bytes result sent to driver
[2025-07-11T11:46:03.651+0200] {subprocess.py:93} INFO - 25/07/11 11:46:03 INFO TaskSetManager: Finished task 0.0 in stage 35.0 (TID 57) in 23 ms on 138.4.31.14 (executor driver) (1/1)
[2025-07-11T11:46:03.651+0200] {subprocess.py:93} INFO - 25/07/11 11:46:03 INFO TaskSchedulerImpl: Removed TaskSet 35.0, whose tasks have all completed, from pool
[2025-07-11T11:46:03.652+0200] {subprocess.py:93} INFO - 25/07/11 11:46:03 INFO DAGScheduler: ResultStage 35 (collect at StringIndexer.scala:204) finished in 0,029 s
[2025-07-11T11:46:03.653+0200] {subprocess.py:93} INFO - 25/07/11 11:46:03 INFO DAGScheduler: Job 21 is finished. Cancelling potential speculative or zombie tasks for this job
[2025-07-11T11:46:03.654+0200] {subprocess.py:93} INFO - 25/07/11 11:46:03 INFO TaskSchedulerImpl: Killing all running tasks in stage 35: Stage finished
[2025-07-11T11:46:03.654+0200] {subprocess.py:93} INFO - 25/07/11 11:46:03 INFO DAGScheduler: Job 21 finished: collect at StringIndexer.scala:204, took 0,031928 s
[2025-07-11T11:46:03.698+0200] {subprocess.py:93} INFO - 25/07/11 11:46:03 INFO FileSystemOverwrite: Path /home/monica.fernandez/practica_creativa/models/string_indexer_model_Dest.bin already exists. It will be overwritten.
[2025-07-11T11:46:03.711+0200] {subprocess.py:93} INFO - 25/07/11 11:46:03 INFO HadoopMapRedCommitProtocol: Using output committer class org.apache.hadoop.mapred.FileOutputCommitter
[2025-07-11T11:46:03.711+0200] {subprocess.py:93} INFO - 25/07/11 11:46:03 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2025-07-11T11:46:03.711+0200] {subprocess.py:93} INFO - 25/07/11 11:46:03 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2025-07-11T11:46:03.757+0200] {subprocess.py:93} INFO - 25/07/11 11:46:03 INFO SparkContext: Starting job: runJob at SparkHadoopWriter.scala:83
[2025-07-11T11:46:03.758+0200] {subprocess.py:93} INFO - 25/07/11 11:46:03 INFO DAGScheduler: Got job 22 (runJob at SparkHadoopWriter.scala:83) with 1 output partitions
[2025-07-11T11:46:03.758+0200] {subprocess.py:93} INFO - 25/07/11 11:46:03 INFO DAGScheduler: Final stage: ResultStage 36 (runJob at SparkHadoopWriter.scala:83)
[2025-07-11T11:46:03.758+0200] {subprocess.py:93} INFO - 25/07/11 11:46:03 INFO DAGScheduler: Parents of final stage: List()
[2025-07-11T11:46:03.758+0200] {subprocess.py:93} INFO - 25/07/11 11:46:03 INFO DAGScheduler: Missing parents: List()
[2025-07-11T11:46:03.758+0200] {subprocess.py:93} INFO - 25/07/11 11:46:03 INFO DAGScheduler: Submitting ResultStage 36 (MapPartitionsRDD[65] at saveAsTextFile at ReadWrite.scala:413), which has no missing parents
[2025-07-11T11:46:03.766+0200] {subprocess.py:93} INFO - 25/07/11 11:46:03 INFO MemoryStore: Block broadcast_28 stored as values in memory (estimated size 101.3 KiB, free 420.3 MiB)
[2025-07-11T11:46:03.767+0200] {subprocess.py:93} INFO - 25/07/11 11:46:03 INFO MemoryStore: Block broadcast_28_piece0 stored as bytes in memory (estimated size 36.5 KiB, free 420.2 MiB)
[2025-07-11T11:46:03.768+0200] {subprocess.py:93} INFO - 25/07/11 11:46:03 INFO BlockManagerInfo: Added broadcast_28_piece0 in memory on 138.4.31.14:45609 (size: 36.5 KiB, free: 433.9 MiB)
[2025-07-11T11:46:03.768+0200] {subprocess.py:93} INFO - 25/07/11 11:46:03 INFO SparkContext: Created broadcast 28 from broadcast at DAGScheduler.scala:1585
[2025-07-11T11:46:03.768+0200] {subprocess.py:93} INFO - 25/07/11 11:46:03 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 36 (MapPartitionsRDD[65] at saveAsTextFile at ReadWrite.scala:413) (first 15 tasks are for partitions Vector(0))
[2025-07-11T11:46:03.768+0200] {subprocess.py:93} INFO - 25/07/11 11:46:03 INFO TaskSchedulerImpl: Adding task set 36.0 with 1 tasks resource profile 0
[2025-07-11T11:46:03.769+0200] {subprocess.py:93} INFO - 25/07/11 11:46:03 INFO TaskSetManager: Starting task 0.0 in stage 36.0 (TID 58) (138.4.31.14, executor driver, partition 0, PROCESS_LOCAL, 13276 bytes)
[2025-07-11T11:46:03.770+0200] {subprocess.py:93} INFO - 25/07/11 11:46:03 INFO Executor: Running task 0.0 in stage 36.0 (TID 58)
[2025-07-11T11:46:03.776+0200] {subprocess.py:93} INFO - 25/07/11 11:46:03 INFO HadoopMapRedCommitProtocol: Using output committer class org.apache.hadoop.mapred.FileOutputCommitter
[2025-07-11T11:46:03.776+0200] {subprocess.py:93} INFO - 25/07/11 11:46:03 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2025-07-11T11:46:03.776+0200] {subprocess.py:93} INFO - 25/07/11 11:46:03 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2025-07-11T11:46:03.803+0200] {subprocess.py:93} INFO - 25/07/11 11:46:03 INFO FileOutputCommitter: Saved output of task 'attempt_202507111146031237374036262088963_0065_m_000000_0' to file:/home/monica.fernandez/practica_creativa/models/string_indexer_model_Dest.bin/metadata/_temporary/0/task_202507111146031237374036262088963_0065_m_000000
[2025-07-11T11:46:03.803+0200] {subprocess.py:93} INFO - 25/07/11 11:46:03 INFO SparkHadoopMapRedUtil: attempt_202507111146031237374036262088963_0065_m_000000_0: Committed. Elapsed time: 1 ms.
[2025-07-11T11:46:03.804+0200] {subprocess.py:93} INFO - 25/07/11 11:46:03 INFO Executor: Finished task 0.0 in stage 36.0 (TID 58). 1170 bytes result sent to driver
[2025-07-11T11:46:03.805+0200] {subprocess.py:93} INFO - 25/07/11 11:46:03 INFO TaskSetManager: Finished task 0.0 in stage 36.0 (TID 58) in 36 ms on 138.4.31.14 (executor driver) (1/1)
[2025-07-11T11:46:03.805+0200] {subprocess.py:93} INFO - 25/07/11 11:46:03 INFO TaskSchedulerImpl: Removed TaskSet 36.0, whose tasks have all completed, from pool
[2025-07-11T11:46:03.806+0200] {subprocess.py:93} INFO - 25/07/11 11:46:03 INFO DAGScheduler: ResultStage 36 (runJob at SparkHadoopWriter.scala:83) finished in 0,047 s
[2025-07-11T11:46:03.806+0200] {subprocess.py:93} INFO - 25/07/11 11:46:03 INFO DAGScheduler: Job 22 is finished. Cancelling potential speculative or zombie tasks for this job
[2025-07-11T11:46:03.806+0200] {subprocess.py:93} INFO - 25/07/11 11:46:03 INFO TaskSchedulerImpl: Killing all running tasks in stage 36: Stage finished
[2025-07-11T11:46:03.806+0200] {subprocess.py:93} INFO - 25/07/11 11:46:03 INFO DAGScheduler: Job 22 finished: runJob at SparkHadoopWriter.scala:83, took 0,049160 s
[2025-07-11T11:46:03.806+0200] {subprocess.py:93} INFO - 25/07/11 11:46:03 INFO SparkHadoopWriter: Start to commit write Job job_202507111146031237374036262088963_0065.
[2025-07-11T11:46:03.826+0200] {subprocess.py:93} INFO - 25/07/11 11:46:03 INFO SparkHadoopWriter: Write Job job_202507111146031237374036262088963_0065 committed. Elapsed time: 19 ms.
[2025-07-11T11:46:03.871+0200] {subprocess.py:93} INFO - 25/07/11 11:46:03 INFO DAGScheduler: Registering RDD 68 (parquet at StringIndexer.scala:499) as input to shuffle 11
[2025-07-11T11:46:03.871+0200] {subprocess.py:93} INFO - 25/07/11 11:46:03 INFO DAGScheduler: Got map stage job 23 (parquet at StringIndexer.scala:499) with 1 output partitions
[2025-07-11T11:46:03.871+0200] {subprocess.py:93} INFO - 25/07/11 11:46:03 INFO DAGScheduler: Final stage: ShuffleMapStage 37 (parquet at StringIndexer.scala:499)
[2025-07-11T11:46:03.872+0200] {subprocess.py:93} INFO - 25/07/11 11:46:03 INFO DAGScheduler: Parents of final stage: List()
[2025-07-11T11:46:03.872+0200] {subprocess.py:93} INFO - 25/07/11 11:46:03 INFO DAGScheduler: Missing parents: List()
[2025-07-11T11:46:03.872+0200] {subprocess.py:93} INFO - 25/07/11 11:46:03 INFO DAGScheduler: Submitting ShuffleMapStage 37 (MapPartitionsRDD[68] at parquet at StringIndexer.scala:499), which has no missing parents
[2025-07-11T11:46:03.872+0200] {subprocess.py:93} INFO - 25/07/11 11:46:03 INFO MemoryStore: Block broadcast_29 stored as values in memory (estimated size 8.1 KiB, free 420.2 MiB)
[2025-07-11T11:46:03.873+0200] {subprocess.py:93} INFO - 25/07/11 11:46:03 INFO MemoryStore: Block broadcast_29_piece0 stored as bytes in memory (estimated size 4.4 KiB, free 420.2 MiB)
[2025-07-11T11:46:03.874+0200] {subprocess.py:93} INFO - 25/07/11 11:46:03 INFO BlockManagerInfo: Added broadcast_29_piece0 in memory on 138.4.31.14:45609 (size: 4.4 KiB, free: 433.9 MiB)
[2025-07-11T11:46:03.874+0200] {subprocess.py:93} INFO - 25/07/11 11:46:03 INFO SparkContext: Created broadcast 29 from broadcast at DAGScheduler.scala:1585
[2025-07-11T11:46:03.875+0200] {subprocess.py:93} INFO - 25/07/11 11:46:03 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 37 (MapPartitionsRDD[68] at parquet at StringIndexer.scala:499) (first 15 tasks are for partitions Vector(0))
[2025-07-11T11:46:03.875+0200] {subprocess.py:93} INFO - 25/07/11 11:46:03 INFO TaskSchedulerImpl: Adding task set 37.0 with 1 tasks resource profile 0
[2025-07-11T11:46:03.877+0200] {subprocess.py:93} INFO - 25/07/11 11:46:03 INFO TaskSetManager: Starting task 0.0 in stage 37.0 (TID 59) (138.4.31.14, executor driver, partition 0, PROCESS_LOCAL, 18200 bytes)
[2025-07-11T11:46:03.877+0200] {subprocess.py:93} INFO - 25/07/11 11:46:03 INFO Executor: Running task 0.0 in stage 37.0 (TID 59)
[2025-07-11T11:46:03.880+0200] {subprocess.py:93} INFO - 25/07/11 11:46:03 INFO Executor: Finished task 0.0 in stage 37.0 (TID 59). 1628 bytes result sent to driver
[2025-07-11T11:46:03.881+0200] {subprocess.py:93} INFO - 25/07/11 11:46:03 INFO TaskSetManager: Finished task 0.0 in stage 37.0 (TID 59) in 5 ms on 138.4.31.14 (executor driver) (1/1)
[2025-07-11T11:46:03.881+0200] {subprocess.py:93} INFO - 25/07/11 11:46:03 INFO TaskSchedulerImpl: Removed TaskSet 37.0, whose tasks have all completed, from pool
[2025-07-11T11:46:03.882+0200] {subprocess.py:93} INFO - 25/07/11 11:46:03 INFO DAGScheduler: ShuffleMapStage 37 (parquet at StringIndexer.scala:499) finished in 0,011 s
[2025-07-11T11:46:03.882+0200] {subprocess.py:93} INFO - 25/07/11 11:46:03 INFO DAGScheduler: looking for newly runnable stages
[2025-07-11T11:46:03.883+0200] {subprocess.py:93} INFO - 25/07/11 11:46:03 INFO DAGScheduler: running: Set()
[2025-07-11T11:46:03.883+0200] {subprocess.py:93} INFO - 25/07/11 11:46:03 INFO DAGScheduler: waiting: Set()
[2025-07-11T11:46:03.883+0200] {subprocess.py:93} INFO - 25/07/11 11:46:03 INFO DAGScheduler: failed: Set()
[2025-07-11T11:46:03.888+0200] {subprocess.py:93} INFO - 25/07/11 11:46:03 INFO ParquetUtils: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
[2025-07-11T11:46:03.889+0200] {subprocess.py:93} INFO - 25/07/11 11:46:03 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2025-07-11T11:46:03.889+0200] {subprocess.py:93} INFO - 25/07/11 11:46:03 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2025-07-11T11:46:03.889+0200] {subprocess.py:93} INFO - 25/07/11 11:46:03 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[2025-07-11T11:46:03.889+0200] {subprocess.py:93} INFO - 25/07/11 11:46:03 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2025-07-11T11:46:03.889+0200] {subprocess.py:93} INFO - 25/07/11 11:46:03 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2025-07-11T11:46:03.889+0200] {subprocess.py:93} INFO - 25/07/11 11:46:03 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[2025-07-11T11:46:03.913+0200] {subprocess.py:93} INFO - 25/07/11 11:46:03 INFO SparkContext: Starting job: parquet at StringIndexer.scala:499
[2025-07-11T11:46:03.915+0200] {subprocess.py:93} INFO - 25/07/11 11:46:03 INFO DAGScheduler: Got job 24 (parquet at StringIndexer.scala:499) with 1 output partitions
[2025-07-11T11:46:03.916+0200] {subprocess.py:93} INFO - 25/07/11 11:46:03 INFO DAGScheduler: Final stage: ResultStage 39 (parquet at StringIndexer.scala:499)
[2025-07-11T11:46:03.916+0200] {subprocess.py:93} INFO - 25/07/11 11:46:03 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 38)
[2025-07-11T11:46:03.916+0200] {subprocess.py:93} INFO - 25/07/11 11:46:03 INFO DAGScheduler: Missing parents: List()
[2025-07-11T11:46:03.916+0200] {subprocess.py:93} INFO - 25/07/11 11:46:03 INFO DAGScheduler: Submitting ResultStage 39 (MapPartitionsRDD[70] at parquet at StringIndexer.scala:499), which has no missing parents
[2025-07-11T11:46:03.941+0200] {subprocess.py:93} INFO - 25/07/11 11:46:03 INFO MemoryStore: Block broadcast_30 stored as values in memory (estimated size 230.2 KiB, free 420.0 MiB)
[2025-07-11T11:46:03.948+0200] {subprocess.py:93} INFO - 25/07/11 11:46:03 INFO MemoryStore: Block broadcast_30_piece0 stored as bytes in memory (estimated size 81.0 KiB, free 419.9 MiB)
[2025-07-11T11:46:03.949+0200] {subprocess.py:93} INFO - 25/07/11 11:46:03 INFO BlockManagerInfo: Added broadcast_30_piece0 in memory on 138.4.31.14:45609 (size: 81.0 KiB, free: 433.8 MiB)
[2025-07-11T11:46:03.949+0200] {subprocess.py:93} INFO - 25/07/11 11:46:03 INFO SparkContext: Created broadcast 30 from broadcast at DAGScheduler.scala:1585
[2025-07-11T11:46:03.951+0200] {subprocess.py:93} INFO - 25/07/11 11:46:03 INFO BlockManagerInfo: Removed broadcast_29_piece0 on 138.4.31.14:45609 in memory (size: 4.4 KiB, free: 433.8 MiB)
[2025-07-11T11:46:03.952+0200] {subprocess.py:93} INFO - 25/07/11 11:46:03 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 39 (MapPartitionsRDD[70] at parquet at StringIndexer.scala:499) (first 15 tasks are for partitions Vector(0))
[2025-07-11T11:46:03.952+0200] {subprocess.py:93} INFO - 25/07/11 11:46:03 INFO TaskSchedulerImpl: Adding task set 39.0 with 1 tasks resource profile 0
[2025-07-11T11:46:03.955+0200] {subprocess.py:93} INFO - 25/07/11 11:46:03 INFO TaskSetManager: Starting task 0.0 in stage 39.0 (TID 60) (138.4.31.14, executor driver, partition 0, NODE_LOCAL, 12952 bytes)
[2025-07-11T11:46:03.955+0200] {subprocess.py:93} INFO - 25/07/11 11:46:03 INFO Executor: Running task 0.0 in stage 39.0 (TID 60)
[2025-07-11T11:46:03.957+0200] {subprocess.py:93} INFO - 25/07/11 11:46:03 INFO BlockManagerInfo: Removed broadcast_28_piece0 on 138.4.31.14:45609 in memory (size: 36.5 KiB, free: 433.9 MiB)
[2025-07-11T11:46:03.969+0200] {subprocess.py:93} INFO - 25/07/11 11:46:03 INFO ShuffleBlockFetcherIterator: Getting 1 (2.9 KiB) non-empty blocks including 1 (2.9 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2025-07-11T11:46:03.969+0200] {subprocess.py:93} INFO - 25/07/11 11:46:03 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2025-07-11T11:46:03.970+0200] {subprocess.py:93} INFO - 25/07/11 11:46:03 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2025-07-11T11:46:03.970+0200] {subprocess.py:93} INFO - 25/07/11 11:46:03 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2025-07-11T11:46:03.970+0200] {subprocess.py:93} INFO - 25/07/11 11:46:03 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[2025-07-11T11:46:03.970+0200] {subprocess.py:93} INFO - 25/07/11 11:46:03 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2025-07-11T11:46:03.970+0200] {subprocess.py:93} INFO - 25/07/11 11:46:03 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2025-07-11T11:46:03.970+0200] {subprocess.py:93} INFO - 25/07/11 11:46:03 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[2025-07-11T11:46:03.970+0200] {subprocess.py:93} INFO - 25/07/11 11:46:03 INFO CodecConfig: Compression: SNAPPY
[2025-07-11T11:46:03.971+0200] {subprocess.py:93} INFO - 25/07/11 11:46:03 INFO CodecConfig: Compression: SNAPPY
[2025-07-11T11:46:03.973+0200] {subprocess.py:93} INFO - 25/07/11 11:46:03 INFO ParquetOutputFormat: ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]
[2025-07-11T11:46:03.973+0200] {subprocess.py:93} INFO - 25/07/11 11:46:03 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
[2025-07-11T11:46:03.974+0200] {subprocess.py:93} INFO - {
[2025-07-11T11:46:03.974+0200] {subprocess.py:93} INFO -   "type" : "struct",
[2025-07-11T11:46:03.974+0200] {subprocess.py:93} INFO -   "fields" : [ {
[2025-07-11T11:46:03.974+0200] {subprocess.py:93} INFO -     "name" : "labelsArray",
[2025-07-11T11:46:03.974+0200] {subprocess.py:93} INFO -     "type" : {
[2025-07-11T11:46:03.974+0200] {subprocess.py:93} INFO -       "type" : "array",
[2025-07-11T11:46:03.974+0200] {subprocess.py:93} INFO -       "elementType" : {
[2025-07-11T11:46:03.974+0200] {subprocess.py:93} INFO -         "type" : "array",
[2025-07-11T11:46:03.974+0200] {subprocess.py:93} INFO -         "elementType" : "string",
[2025-07-11T11:46:03.974+0200] {subprocess.py:93} INFO -         "containsNull" : true
[2025-07-11T11:46:03.977+0200] {subprocess.py:93} INFO -       },
[2025-07-11T11:46:03.977+0200] {subprocess.py:93} INFO -       "containsNull" : true
[2025-07-11T11:46:03.977+0200] {subprocess.py:93} INFO -     },
[2025-07-11T11:46:03.978+0200] {subprocess.py:93} INFO -     "nullable" : true,
[2025-07-11T11:46:03.978+0200] {subprocess.py:93} INFO -     "metadata" : { }
[2025-07-11T11:46:03.978+0200] {subprocess.py:93} INFO -   } ]
[2025-07-11T11:46:03.978+0200] {subprocess.py:93} INFO - }
[2025-07-11T11:46:03.978+0200] {subprocess.py:93} INFO - and corresponding Parquet message type:
[2025-07-11T11:46:03.978+0200] {subprocess.py:93} INFO - message spark_schema {
[2025-07-11T11:46:03.978+0200] {subprocess.py:93} INFO -   optional group labelsArray (LIST) {
[2025-07-11T11:46:03.978+0200] {subprocess.py:93} INFO -     repeated group list {
[2025-07-11T11:46:03.978+0200] {subprocess.py:93} INFO -       optional group element (LIST) {
[2025-07-11T11:46:03.978+0200] {subprocess.py:93} INFO -         repeated group list {
[2025-07-11T11:46:03.978+0200] {subprocess.py:93} INFO -           optional binary element (STRING);
[2025-07-11T11:46:03.978+0200] {subprocess.py:93} INFO -         }
[2025-07-11T11:46:03.978+0200] {subprocess.py:93} INFO -       }
[2025-07-11T11:46:03.978+0200] {subprocess.py:93} INFO -     }
[2025-07-11T11:46:03.978+0200] {subprocess.py:93} INFO -   }
[2025-07-11T11:46:03.978+0200] {subprocess.py:93} INFO - }
[2025-07-11T11:46:03.978+0200] {subprocess.py:93} INFO - 
[2025-07-11T11:46:03.979+0200] {subprocess.py:93} INFO - 
[2025-07-11T11:46:03.981+0200] {subprocess.py:93} INFO - 25/07/11 11:46:03 INFO BlockManagerInfo: Removed broadcast_26_piece0 on 138.4.31.14:45609 in memory (size: 23.6 KiB, free: 433.9 MiB)
[2025-07-11T11:46:03.985+0200] {subprocess.py:93} INFO - 25/07/11 11:46:03 INFO BlockManagerInfo: Removed broadcast_27_piece0 on 138.4.31.14:45609 in memory (size: 23.4 KiB, free: 433.9 MiB)
[2025-07-11T11:46:04.015+0200] {subprocess.py:93} INFO - 25/07/11 11:46:04 INFO FileOutputCommitter: Saved output of task 'attempt_20250711114603374240118990590871_0039_m_000000_60' to file:/home/monica.fernandez/practica_creativa/models/string_indexer_model_Dest.bin/data/_temporary/0/task_20250711114603374240118990590871_0039_m_000000
[2025-07-11T11:46:04.015+0200] {subprocess.py:93} INFO - 25/07/11 11:46:04 INFO SparkHadoopMapRedUtil: attempt_20250711114603374240118990590871_0039_m_000000_60: Committed. Elapsed time: 1 ms.
[2025-07-11T11:46:04.020+0200] {subprocess.py:93} INFO - 25/07/11 11:46:04 INFO Executor: Finished task 0.0 in stage 39.0 (TID 60). 4740 bytes result sent to driver
[2025-07-11T11:46:04.021+0200] {subprocess.py:93} INFO - 25/07/11 11:46:04 INFO TaskSetManager: Finished task 0.0 in stage 39.0 (TID 60) in 66 ms on 138.4.31.14 (executor driver) (1/1)
[2025-07-11T11:46:04.022+0200] {subprocess.py:93} INFO - 25/07/11 11:46:04 INFO TaskSchedulerImpl: Removed TaskSet 39.0, whose tasks have all completed, from pool
[2025-07-11T11:46:04.022+0200] {subprocess.py:93} INFO - 25/07/11 11:46:04 INFO DAGScheduler: ResultStage 39 (parquet at StringIndexer.scala:499) finished in 0,105 s
[2025-07-11T11:46:04.022+0200] {subprocess.py:93} INFO - 25/07/11 11:46:04 INFO DAGScheduler: Job 24 is finished. Cancelling potential speculative or zombie tasks for this job
[2025-07-11T11:46:04.022+0200] {subprocess.py:93} INFO - 25/07/11 11:46:04 INFO TaskSchedulerImpl: Killing all running tasks in stage 39: Stage finished
[2025-07-11T11:46:04.023+0200] {subprocess.py:93} INFO - 25/07/11 11:46:04 INFO DAGScheduler: Job 24 finished: parquet at StringIndexer.scala:499, took 0,109644 s
[2025-07-11T11:46:04.024+0200] {subprocess.py:93} INFO - 25/07/11 11:46:04 INFO FileFormatWriter: Start to commit write Job e0bb5dc1-5e14-4454-b9f3-b673779c27cd.
[2025-07-11T11:46:04.050+0200] {subprocess.py:93} INFO - 25/07/11 11:46:04 INFO FileFormatWriter: Write Job e0bb5dc1-5e14-4454-b9f3-b673779c27cd committed. Elapsed time: 25 ms.
[2025-07-11T11:46:04.051+0200] {subprocess.py:93} INFO - 25/07/11 11:46:04 INFO FileFormatWriter: Finished processing stats for write job e0bb5dc1-5e14-4454-b9f3-b673779c27cd.
[2025-07-11T11:46:04.148+0200] {subprocess.py:93} INFO - 25/07/11 11:46:04 INFO V2ScanRelationPushDown:
[2025-07-11T11:46:04.148+0200] {subprocess.py:93} INFO - Pushing operators to origin_dest_distances
[2025-07-11T11:46:04.148+0200] {subprocess.py:93} INFO - Pushed Filters:
[2025-07-11T11:46:04.148+0200] {subprocess.py:93} INFO - Post-Scan Filters:
[2025-07-11T11:46:04.149+0200] {subprocess.py:93} INFO - 
[2025-07-11T11:46:04.154+0200] {subprocess.py:93} INFO - 25/07/11 11:46:04 INFO V2ScanRelationPushDown:
[2025-07-11T11:46:04.155+0200] {subprocess.py:93} INFO - Output: origin#42, dest#43
[2025-07-11T11:46:04.155+0200] {subprocess.py:93} INFO - 
[2025-07-11T11:46:04.187+0200] {subprocess.py:93} INFO - 25/07/11 11:46:04 INFO FileSourceStrategy: Pushed Filters:
[2025-07-11T11:46:04.187+0200] {subprocess.py:93} INFO - 25/07/11 11:46:04 INFO FileSourceStrategy: Post-Scan Filters:
[2025-07-11T11:46:04.263+0200] {subprocess.py:93} INFO - 25/07/11 11:46:04 INFO CodeGenerator: Code generated in 22.485813 ms
[2025-07-11T11:46:04.266+0200] {subprocess.py:93} INFO - 25/07/11 11:46:04 INFO MemoryStore: Block broadcast_31 stored as values in memory (estimated size 209.9 KiB, free 420.0 MiB)
[2025-07-11T11:46:04.296+0200] {subprocess.py:93} INFO - 25/07/11 11:46:04 INFO MemoryStore: Block broadcast_31_piece0 stored as bytes in memory (estimated size 37.7 KiB, free 420.0 MiB)
[2025-07-11T11:46:04.296+0200] {subprocess.py:93} INFO - 25/07/11 11:46:04 INFO BlockManagerInfo: Added broadcast_31_piece0 in memory on 138.4.31.14:45609 (size: 37.7 KiB, free: 433.9 MiB)
[2025-07-11T11:46:04.297+0200] {subprocess.py:93} INFO - 25/07/11 11:46:04 INFO SparkContext: Created broadcast 31 from collect at StringIndexer.scala:204
[2025-07-11T11:46:04.299+0200] {subprocess.py:93} INFO - 25/07/11 11:46:04 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
[2025-07-11T11:46:04.305+0200] {subprocess.py:93} INFO - 25/07/11 11:46:04 INFO DAGScheduler: Registering RDD 74 (collect at StringIndexer.scala:204) as input to shuffle 12
[2025-07-11T11:46:04.306+0200] {subprocess.py:93} INFO - 25/07/11 11:46:04 INFO DAGScheduler: Got map stage job 25 (collect at StringIndexer.scala:204) with 2 output partitions
[2025-07-11T11:46:04.306+0200] {subprocess.py:93} INFO - 25/07/11 11:46:04 INFO DAGScheduler: Final stage: ShuffleMapStage 40 (collect at StringIndexer.scala:204)
[2025-07-11T11:46:04.307+0200] {subprocess.py:93} INFO - 25/07/11 11:46:04 INFO DAGScheduler: Parents of final stage: List()
[2025-07-11T11:46:04.307+0200] {subprocess.py:93} INFO - 25/07/11 11:46:04 INFO DAGScheduler: Missing parents: List()
[2025-07-11T11:46:04.307+0200] {subprocess.py:93} INFO - 25/07/11 11:46:04 INFO DAGScheduler: Submitting ShuffleMapStage 40 (MapPartitionsRDD[74] at collect at StringIndexer.scala:204), which has no missing parents
[2025-07-11T11:46:04.315+0200] {subprocess.py:93} INFO - 25/07/11 11:46:04 INFO MemoryStore: Block broadcast_32 stored as values in memory (estimated size 18.0 KiB, free 420.0 MiB)
[2025-07-11T11:46:04.316+0200] {subprocess.py:93} INFO - 25/07/11 11:46:04 INFO MemoryStore: Block broadcast_32_piece0 stored as bytes in memory (estimated size 8.6 KiB, free 419.9 MiB)
[2025-07-11T11:46:04.316+0200] {subprocess.py:93} INFO - 25/07/11 11:46:04 INFO BlockManagerInfo: Added broadcast_32_piece0 in memory on 138.4.31.14:45609 (size: 8.6 KiB, free: 433.9 MiB)
[2025-07-11T11:46:04.316+0200] {subprocess.py:93} INFO - 25/07/11 11:46:04 INFO SparkContext: Created broadcast 32 from broadcast at DAGScheduler.scala:1585
[2025-07-11T11:46:04.318+0200] {subprocess.py:93} INFO - 25/07/11 11:46:04 INFO DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 40 (MapPartitionsRDD[74] at collect at StringIndexer.scala:204) (first 15 tasks are for partitions Vector(0, 1))
[2025-07-11T11:46:04.318+0200] {subprocess.py:93} INFO - 25/07/11 11:46:04 INFO TaskSchedulerImpl: Adding task set 40.0 with 2 tasks resource profile 0
[2025-07-11T11:46:04.319+0200] {subprocess.py:93} INFO - 25/07/11 11:46:04 INFO TaskSetManager: Starting task 0.0 in stage 40.0 (TID 61) (138.4.31.14, executor driver, partition 0, PROCESS_LOCAL, 13595 bytes)
[2025-07-11T11:46:04.320+0200] {subprocess.py:93} INFO - 25/07/11 11:46:04 INFO TaskSetManager: Starting task 1.0 in stage 40.0 (TID 62) (138.4.31.14, executor driver, partition 1, PROCESS_LOCAL, 13595 bytes)
[2025-07-11T11:46:04.320+0200] {subprocess.py:93} INFO - 25/07/11 11:46:04 INFO Executor: Running task 0.0 in stage 40.0 (TID 61)
[2025-07-11T11:46:04.320+0200] {subprocess.py:93} INFO - 25/07/11 11:46:04 INFO Executor: Running task 1.0 in stage 40.0 (TID 62)
[2025-07-11T11:46:04.344+0200] {subprocess.py:93} INFO - 25/07/11 11:46:04 INFO CodeGenerator: Code generated in 31.075656 ms
[2025-07-11T11:46:04.344+0200] {subprocess.py:93} INFO - 25/07/11 11:46:04 INFO CodeGenerator: Code generated in 17.898364 ms
[2025-07-11T11:46:04.346+0200] {subprocess.py:93} INFO - 25/07/11 11:46:04 INFO FileScanRDD: Reading File path: file:///home/monica.fernandez/practica_creativa/data/simple_flight_delay_features.jsonl.bz2, range: 0-4194304, partition values: [empty row]
[2025-07-11T11:46:04.352+0200] {subprocess.py:93} INFO - 25/07/11 11:46:04 INFO DAGScheduler: Registering RDD 78 (collect at StringIndexer.scala:204) as input to shuffle 13
[2025-07-11T11:46:04.352+0200] {subprocess.py:93} INFO - 25/07/11 11:46:04 INFO DAGScheduler: Got map stage job 26 (collect at StringIndexer.scala:204) with 12 output partitions
[2025-07-11T11:46:04.352+0200] {subprocess.py:93} INFO - 25/07/11 11:46:04 INFO DAGScheduler: Final stage: ShuffleMapStage 41 (collect at StringIndexer.scala:204)
[2025-07-11T11:46:04.352+0200] {subprocess.py:93} INFO - 25/07/11 11:46:04 INFO DAGScheduler: Parents of final stage: List()
[2025-07-11T11:46:04.352+0200] {subprocess.py:93} INFO - 25/07/11 11:46:04 INFO DAGScheduler: Missing parents: List()
[2025-07-11T11:46:04.352+0200] {subprocess.py:93} INFO - 25/07/11 11:46:04 INFO DAGScheduler: Submitting ShuffleMapStage 41 (MapPartitionsRDD[78] at collect at StringIndexer.scala:204), which has no missing parents
[2025-07-11T11:46:04.354+0200] {subprocess.py:93} INFO - 25/07/11 11:46:04 INFO FileScanRDD: Reading File path: file:///home/monica.fernandez/practica_creativa/data/simple_flight_delay_features.jsonl.bz2, range: 4194304-4676447, partition values: [empty row]
[2025-07-11T11:46:04.358+0200] {subprocess.py:93} INFO - 25/07/11 11:46:04 INFO MemoryStore: Block broadcast_33 stored as values in memory (estimated size 18.2 KiB, free 419.9 MiB)
[2025-07-11T11:46:04.359+0200] {subprocess.py:93} INFO - 25/07/11 11:46:04 INFO MemoryStore: Block broadcast_33_piece0 stored as bytes in memory (estimated size 9.0 KiB, free 419.9 MiB)
[2025-07-11T11:46:04.359+0200] {subprocess.py:93} INFO - 25/07/11 11:46:04 INFO BlockManagerInfo: Added broadcast_33_piece0 in memory on 138.4.31.14:45609 (size: 9.0 KiB, free: 433.8 MiB)
[2025-07-11T11:46:04.359+0200] {subprocess.py:93} INFO - 25/07/11 11:46:04 INFO SparkContext: Created broadcast 33 from broadcast at DAGScheduler.scala:1585
[2025-07-11T11:46:04.359+0200] {subprocess.py:93} INFO - 25/07/11 11:46:04 INFO DAGScheduler: Submitting 12 missing tasks from ShuffleMapStage 41 (MapPartitionsRDD[78] at collect at StringIndexer.scala:204) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11))
[2025-07-11T11:46:04.360+0200] {subprocess.py:93} INFO - 25/07/11 11:46:04 INFO TaskSchedulerImpl: Adding task set 41.0 with 12 tasks resource profile 0
[2025-07-11T11:46:04.367+0200] {subprocess.py:93} INFO - 25/07/11 11:46:04 INFO TaskSetManager: Starting task 0.0 in stage 41.0 (TID 63) (138.4.31.14, executor driver, partition 0, ANY, 14963 bytes)
[2025-07-11T11:46:04.367+0200] {subprocess.py:93} INFO - 25/07/11 11:46:04 INFO TaskSetManager: Starting task 1.0 in stage 41.0 (TID 64) (138.4.31.14, executor driver, partition 1, ANY, 14843 bytes)
[2025-07-11T11:46:04.369+0200] {subprocess.py:93} INFO - 25/07/11 11:46:04 INFO Executor: Running task 0.0 in stage 41.0 (TID 63)
[2025-07-11T11:46:04.369+0200] {subprocess.py:93} INFO - 25/07/11 11:46:04 INFO Executor: Running task 1.0 in stage 41.0 (TID 64)
[2025-07-11T11:46:04.377+0200] {subprocess.py:93} INFO - 25/07/11 11:46:04 INFO BlockManagerInfo: Removed broadcast_30_piece0 on 138.4.31.14:45609 in memory (size: 81.0 KiB, free: 433.9 MiB)
[2025-07-11T11:46:04.401+0200] {subprocess.py:93} INFO - 25/07/11 11:46:04 INFO CodeGenerator: Code generated in 15.603907 ms
[2025-07-11T11:46:04.484+0200] {subprocess.py:93} INFO - 25/07/11 11:46:04 INFO Executor: Finished task 0.0 in stage 41.0 (TID 63). 1945 bytes result sent to driver
[2025-07-11T11:46:04.485+0200] {subprocess.py:93} INFO - 25/07/11 11:46:04 INFO TaskSetManager: Starting task 2.0 in stage 41.0 (TID 65) (138.4.31.14, executor driver, partition 2, ANY, 14843 bytes)
[2025-07-11T11:46:04.485+0200] {subprocess.py:93} INFO - 25/07/11 11:46:04 INFO TaskSetManager: Finished task 0.0 in stage 41.0 (TID 63) in 119 ms on 138.4.31.14 (executor driver) (1/12)
[2025-07-11T11:46:04.486+0200] {subprocess.py:93} INFO - 25/07/11 11:46:04 INFO Executor: Running task 2.0 in stage 41.0 (TID 65)
[2025-07-11T11:46:04.496+0200] {subprocess.py:93} INFO - 25/07/11 11:46:04 INFO Executor: Finished task 1.0 in stage 41.0 (TID 64). 1988 bytes result sent to driver
[2025-07-11T11:46:04.497+0200] {subprocess.py:93} INFO - 25/07/11 11:46:04 INFO TaskSetManager: Starting task 3.0 in stage 41.0 (TID 66) (138.4.31.14, executor driver, partition 3, ANY, 14843 bytes)
[2025-07-11T11:46:04.498+0200] {subprocess.py:93} INFO - 25/07/11 11:46:04 INFO TaskSetManager: Finished task 1.0 in stage 41.0 (TID 64) in 131 ms on 138.4.31.14 (executor driver) (2/12)
[2025-07-11T11:46:04.499+0200] {subprocess.py:93} INFO - 25/07/11 11:46:04 INFO Executor: Running task 3.0 in stage 41.0 (TID 66)
[2025-07-11T11:46:04.648+0200] {subprocess.py:93} INFO - 25/07/11 11:46:04 INFO Executor: Finished task 2.0 in stage 41.0 (TID 65). 1945 bytes result sent to driver
[2025-07-11T11:46:04.651+0200] {subprocess.py:93} INFO - 25/07/11 11:46:04 INFO TaskSetManager: Starting task 4.0 in stage 41.0 (TID 67) (138.4.31.14, executor driver, partition 4, ANY, 14843 bytes)
[2025-07-11T11:46:04.651+0200] {subprocess.py:93} INFO - 25/07/11 11:46:04 INFO TaskSetManager: Finished task 2.0 in stage 41.0 (TID 65) in 167 ms on 138.4.31.14 (executor driver) (3/12)
[2025-07-11T11:46:04.652+0200] {subprocess.py:93} INFO - 25/07/11 11:46:04 INFO Executor: Running task 4.0 in stage 41.0 (TID 67)
[2025-07-11T11:46:04.667+0200] {subprocess.py:93} INFO - 25/07/11 11:46:04 INFO Executor: Finished task 3.0 in stage 41.0 (TID 66). 1945 bytes result sent to driver
[2025-07-11T11:46:04.667+0200] {subprocess.py:93} INFO - 25/07/11 11:46:04 INFO TaskSetManager: Starting task 5.0 in stage 41.0 (TID 68) (138.4.31.14, executor driver, partition 5, ANY, 14963 bytes)
[2025-07-11T11:46:04.667+0200] {subprocess.py:93} INFO - 25/07/11 11:46:04 INFO TaskSetManager: Finished task 3.0 in stage 41.0 (TID 66) in 167 ms on 138.4.31.14 (executor driver) (4/12)
[2025-07-11T11:46:04.668+0200] {subprocess.py:93} INFO - 25/07/11 11:46:04 INFO Executor: Running task 5.0 in stage 41.0 (TID 68)
[2025-07-11T11:46:04.718+0200] {subprocess.py:93} INFO - 25/07/11 11:46:04 INFO Executor: Finished task 4.0 in stage 41.0 (TID 67). 1945 bytes result sent to driver
[2025-07-11T11:46:04.734+0200] {subprocess.py:93} INFO - 25/07/11 11:46:04 INFO TaskSetManager: Starting task 6.0 in stage 41.0 (TID 69) (138.4.31.14, executor driver, partition 6, ANY, 14959 bytes)
[2025-07-11T11:46:04.734+0200] {subprocess.py:93} INFO - 25/07/11 11:46:04 INFO Executor: Running task 6.0 in stage 41.0 (TID 69)
[2025-07-11T11:46:04.734+0200] {subprocess.py:93} INFO - 25/07/11 11:46:04 INFO TaskSetManager: Finished task 4.0 in stage 41.0 (TID 67) in 81 ms on 138.4.31.14 (executor driver) (5/12)
[2025-07-11T11:46:04.771+0200] {subprocess.py:93} INFO - 25/07/11 11:46:04 INFO Executor: Finished task 5.0 in stage 41.0 (TID 68). 1945 bytes result sent to driver
[2025-07-11T11:46:04.771+0200] {subprocess.py:93} INFO - 25/07/11 11:46:04 INFO TaskSetManager: Starting task 7.0 in stage 41.0 (TID 70) (138.4.31.14, executor driver, partition 7, ANY, 14843 bytes)
[2025-07-11T11:46:04.771+0200] {subprocess.py:93} INFO - 25/07/11 11:46:04 INFO TaskSetManager: Finished task 5.0 in stage 41.0 (TID 68) in 101 ms on 138.4.31.14 (executor driver) (6/12)
[2025-07-11T11:46:04.771+0200] {subprocess.py:93} INFO - 25/07/11 11:46:04 INFO Executor: Running task 7.0 in stage 41.0 (TID 70)
[2025-07-11T11:46:04.821+0200] {subprocess.py:93} INFO - 25/07/11 11:46:04 INFO Executor: Finished task 6.0 in stage 41.0 (TID 69). 1945 bytes result sent to driver
[2025-07-11T11:46:04.822+0200] {subprocess.py:93} INFO - 25/07/11 11:46:04 INFO TaskSetManager: Starting task 8.0 in stage 41.0 (TID 71) (138.4.31.14, executor driver, partition 8, ANY, 14843 bytes)
[2025-07-11T11:46:04.822+0200] {subprocess.py:93} INFO - 25/07/11 11:46:04 INFO TaskSetManager: Finished task 6.0 in stage 41.0 (TID 69) in 103 ms on 138.4.31.14 (executor driver) (7/12)
[2025-07-11T11:46:04.822+0200] {subprocess.py:93} INFO - 25/07/11 11:46:04 INFO Executor: Running task 8.0 in stage 41.0 (TID 71)
[2025-07-11T11:46:04.827+0200] {subprocess.py:93} INFO - 25/07/11 11:46:04 INFO Executor: Finished task 7.0 in stage 41.0 (TID 70). 1945 bytes result sent to driver
[2025-07-11T11:46:04.827+0200] {subprocess.py:93} INFO - 25/07/11 11:46:04 INFO TaskSetManager: Finished task 7.0 in stage 41.0 (TID 70) in 64 ms on 138.4.31.14 (executor driver) (8/12)
[2025-07-11T11:46:04.828+0200] {subprocess.py:93} INFO - 25/07/11 11:46:04 INFO TaskSetManager: Starting task 9.0 in stage 41.0 (TID 72) (138.4.31.14, executor driver, partition 9, ANY, 14963 bytes)
[2025-07-11T11:46:04.829+0200] {subprocess.py:93} INFO - 25/07/11 11:46:04 INFO Executor: Running task 9.0 in stage 41.0 (TID 72)
[2025-07-11T11:46:04.945+0200] {subprocess.py:93} INFO - 25/07/11 11:46:04 INFO Executor: Finished task 9.0 in stage 41.0 (TID 72). 2031 bytes result sent to driver
[2025-07-11T11:46:04.945+0200] {subprocess.py:93} INFO - 25/07/11 11:46:04 INFO Executor: Finished task 8.0 in stage 41.0 (TID 71). 1988 bytes result sent to driver
[2025-07-11T11:46:04.946+0200] {subprocess.py:93} INFO - 25/07/11 11:46:04 INFO TaskSetManager: Starting task 10.0 in stage 41.0 (TID 73) (138.4.31.14, executor driver, partition 10, ANY, 14843 bytes)
[2025-07-11T11:46:04.946+0200] {subprocess.py:93} INFO - 25/07/11 11:46:04 INFO TaskSetManager: Finished task 8.0 in stage 41.0 (TID 71) in 124 ms on 138.4.31.14 (executor driver) (9/12)
[2025-07-11T11:46:04.953+0200] {subprocess.py:93} INFO - 25/07/11 11:46:04 INFO Executor: Running task 10.0 in stage 41.0 (TID 73)
[2025-07-11T11:46:04.955+0200] {subprocess.py:93} INFO - 25/07/11 11:46:04 INFO TaskSetManager: Starting task 11.0 in stage 41.0 (TID 74) (138.4.31.14, executor driver, partition 11, ANY, 14843 bytes)
[2025-07-11T11:46:04.956+0200] {subprocess.py:93} INFO - 25/07/11 11:46:04 INFO TaskSetManager: Finished task 9.0 in stage 41.0 (TID 72) in 128 ms on 138.4.31.14 (executor driver) (10/12)
[2025-07-11T11:46:04.957+0200] {subprocess.py:93} INFO - 25/07/11 11:46:04 INFO Executor: Running task 11.0 in stage 41.0 (TID 74)
[2025-07-11T11:46:05.020+0200] {subprocess.py:93} INFO - 25/07/11 11:46:05 INFO Executor: Finished task 11.0 in stage 41.0 (TID 74). 1945 bytes result sent to driver
[2025-07-11T11:46:05.021+0200] {subprocess.py:93} INFO - 25/07/11 11:46:05 INFO TaskSetManager: Finished task 11.0 in stage 41.0 (TID 74) in 67 ms on 138.4.31.14 (executor driver) (11/12)
[2025-07-11T11:46:05.041+0200] {subprocess.py:93} INFO - 25/07/11 11:46:05 INFO Executor: Finished task 10.0 in stage 41.0 (TID 73). 1945 bytes result sent to driver
[2025-07-11T11:46:05.043+0200] {subprocess.py:93} INFO - 25/07/11 11:46:05 INFO TaskSetManager: Finished task 10.0 in stage 41.0 (TID 73) in 99 ms on 138.4.31.14 (executor driver) (12/12)
[2025-07-11T11:46:05.043+0200] {subprocess.py:93} INFO - 25/07/11 11:46:05 INFO TaskSchedulerImpl: Removed TaskSet 41.0, whose tasks have all completed, from pool
[2025-07-11T11:46:05.044+0200] {subprocess.py:93} INFO - 25/07/11 11:46:05 INFO DAGScheduler: ShuffleMapStage 41 (collect at StringIndexer.scala:204) finished in 0,690 s
[2025-07-11T11:46:05.044+0200] {subprocess.py:93} INFO - 25/07/11 11:46:05 INFO DAGScheduler: looking for newly runnable stages
[2025-07-11T11:46:05.044+0200] {subprocess.py:93} INFO - 25/07/11 11:46:05 INFO DAGScheduler: running: Set(ShuffleMapStage 40)
[2025-07-11T11:46:05.044+0200] {subprocess.py:93} INFO - 25/07/11 11:46:05 INFO DAGScheduler: waiting: Set()
[2025-07-11T11:46:05.044+0200] {subprocess.py:93} INFO - 25/07/11 11:46:05 INFO DAGScheduler: failed: Set()
[2025-07-11T11:46:05.078+0200] {subprocess.py:93} INFO - 25/07/11 11:46:05 INFO ShufflePartitionsUtil: For shuffle(13), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576
[2025-07-11T11:46:05.172+0200] {subprocess.py:93} INFO - 25/07/11 11:46:05 INFO BlockManagerInfo: Removed broadcast_11_piece0 on 138.4.31.14:45609 in memory (size: 37.7 KiB, free: 434.0 MiB)
[2025-07-11T11:46:05.176+0200] {subprocess.py:93} INFO - 25/07/11 11:46:05 INFO SparkContext: Starting job: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264
[2025-07-11T11:46:05.179+0200] {subprocess.py:93} INFO - 25/07/11 11:46:05 INFO DAGScheduler: Got job 27 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) with 1 output partitions
[2025-07-11T11:46:05.179+0200] {subprocess.py:93} INFO - 25/07/11 11:46:05 INFO DAGScheduler: Final stage: ResultStage 43 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264)
[2025-07-11T11:46:05.179+0200] {subprocess.py:93} INFO - 25/07/11 11:46:05 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 42)
[2025-07-11T11:46:05.179+0200] {subprocess.py:93} INFO - 25/07/11 11:46:05 INFO DAGScheduler: Missing parents: List()
[2025-07-11T11:46:05.180+0200] {subprocess.py:93} INFO - 25/07/11 11:46:05 INFO DAGScheduler: Submitting ResultStage 43 (MapPartitionsRDD[80] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264), which has no missing parents
[2025-07-11T11:46:05.180+0200] {subprocess.py:93} INFO - 25/07/11 11:46:05 INFO MemoryStore: Block broadcast_34 stored as values in memory (estimated size 8.2 KiB, free 420.5 MiB)
[2025-07-11T11:46:05.181+0200] {subprocess.py:93} INFO - 25/07/11 11:46:05 INFO MemoryStore: Block broadcast_34_piece0 stored as bytes in memory (estimated size 4.2 KiB, free 420.5 MiB)
[2025-07-11T11:46:05.181+0200] {subprocess.py:93} INFO - 25/07/11 11:46:05 INFO BlockManagerInfo: Added broadcast_34_piece0 in memory on 138.4.31.14:45609 (size: 4.2 KiB, free: 434.0 MiB)
[2025-07-11T11:46:05.185+0200] {subprocess.py:93} INFO - 25/07/11 11:46:05 INFO SparkContext: Created broadcast 34 from broadcast at DAGScheduler.scala:1585
[2025-07-11T11:46:05.185+0200] {subprocess.py:93} INFO - 25/07/11 11:46:05 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 43 (MapPartitionsRDD[80] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) (first 15 tasks are for partitions Vector(0))
[2025-07-11T11:46:05.185+0200] {subprocess.py:93} INFO - 25/07/11 11:46:05 INFO TaskSchedulerImpl: Adding task set 43.0 with 1 tasks resource profile 0
[2025-07-11T11:46:05.195+0200] {subprocess.py:93} INFO - 25/07/11 11:46:05 INFO TaskSetManager: Starting task 0.0 in stage 43.0 (TID 75) (138.4.31.14, executor driver, partition 0, NODE_LOCAL, 12968 bytes)
[2025-07-11T11:46:05.207+0200] {subprocess.py:93} INFO - 25/07/11 11:46:05 INFO Executor: Running task 0.0 in stage 43.0 (TID 75)
[2025-07-11T11:46:05.216+0200] {subprocess.py:93} INFO - 25/07/11 11:46:05 INFO ShuffleBlockFetcherIterator: Getting 12 (193.5 KiB) non-empty blocks including 12 (193.5 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2025-07-11T11:46:05.217+0200] {subprocess.py:93} INFO - 25/07/11 11:46:05 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 16 ms
[2025-07-11T11:46:05.258+0200] {subprocess.py:93} INFO - 25/07/11 11:46:05 INFO Executor: Finished task 0.0 in stage 43.0 (TID 75). 40654 bytes result sent to driver
[2025-07-11T11:46:05.259+0200] {subprocess.py:93} INFO - 25/07/11 11:46:05 INFO TaskSetManager: Finished task 0.0 in stage 43.0 (TID 75) in 65 ms on 138.4.31.14 (executor driver) (1/1)
[2025-07-11T11:46:05.259+0200] {subprocess.py:93} INFO - 25/07/11 11:46:05 INFO TaskSchedulerImpl: Removed TaskSet 43.0, whose tasks have all completed, from pool
[2025-07-11T11:46:05.263+0200] {subprocess.py:93} INFO - 25/07/11 11:46:05 INFO BlockManagerInfo: Removed broadcast_22_piece0 on 138.4.31.14:45609 in memory (size: 7.0 KiB, free: 434.0 MiB)
[2025-07-11T11:46:05.268+0200] {subprocess.py:93} INFO - 25/07/11 11:46:05 INFO DAGScheduler: ResultStage 43 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) finished in 0,089 s
[2025-07-11T11:46:05.268+0200] {subprocess.py:93} INFO - 25/07/11 11:46:05 INFO DAGScheduler: Job 27 is finished. Cancelling potential speculative or zombie tasks for this job
[2025-07-11T11:46:05.268+0200] {subprocess.py:93} INFO - 25/07/11 11:46:05 INFO TaskSchedulerImpl: Killing all running tasks in stage 43: Stage finished
[2025-07-11T11:46:05.270+0200] {subprocess.py:93} INFO - 25/07/11 11:46:05 INFO DAGScheduler: Job 27 finished: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264, took 0,091794 s
[2025-07-11T11:46:05.298+0200] {subprocess.py:93} INFO - 25/07/11 11:46:05 INFO MemoryStore: Block broadcast_35 stored as values in memory (estimated size 4.3 MiB, free 416.2 MiB)
[2025-07-11T11:46:05.302+0200] {subprocess.py:93} INFO - 25/07/11 11:46:05 INFO MemoryStore: Block broadcast_35_piece0 stored as bytes in memory (estimated size 89.9 KiB, free 416.1 MiB)
[2025-07-11T11:46:05.309+0200] {subprocess.py:93} INFO - 25/07/11 11:46:05 INFO BlockManagerInfo: Added broadcast_35_piece0 in memory on 138.4.31.14:45609 (size: 89.9 KiB, free: 433.9 MiB)
[2025-07-11T11:46:05.309+0200] {subprocess.py:93} INFO - 25/07/11 11:46:05 INFO SparkContext: Created broadcast 35 from $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264
[2025-07-11T11:46:05.370+0200] {subprocess.py:93} INFO - 25/07/11 11:46:05 INFO BlockManagerInfo: Removed broadcast_2_piece0 on 138.4.31.14:45609 in memory (size: 7.1 KiB, free: 433.9 MiB)
[2025-07-11T11:46:05.466+0200] {subprocess.py:93} INFO - 25/07/11 11:46:05 INFO BlockManagerInfo: Removed broadcast_15_piece0 on 138.4.31.14:45609 in memory (size: 89.9 KiB, free: 434.0 MiB)
[2025-07-11T11:46:05.497+0200] {subprocess.py:93} INFO - 25/07/11 11:46:05 INFO BlockManagerInfo: Removed broadcast_23_piece0 on 138.4.31.14:45609 in memory (size: 9.0 KiB, free: 434.0 MiB)
[2025-07-11T11:46:05.513+0200] {subprocess.py:93} INFO - 25/07/11 11:46:05 INFO BlockManagerInfo: Removed broadcast_3_piece0 on 138.4.31.14:45609 in memory (size: 9.0 KiB, free: 434.0 MiB)
[2025-07-11T11:46:05.555+0200] {subprocess.py:93} INFO - 25/07/11 11:46:05 INFO BlockManagerInfo: Removed broadcast_5_piece0 on 138.4.31.14:45609 in memory (size: 90.0 KiB, free: 434.1 MiB)
[2025-07-11T11:46:05.618+0200] {subprocess.py:93} INFO - 25/07/11 11:46:05 INFO BlockManagerInfo: Removed broadcast_13_piece0 on 138.4.31.14:45609 in memory (size: 9.0 KiB, free: 434.1 MiB)
[2025-07-11T11:46:05.628+0200] {subprocess.py:93} INFO - 25/07/11 11:46:05 INFO BlockManagerInfo: Removed broadcast_12_piece0 on 138.4.31.14:45609 in memory (size: 7.0 KiB, free: 434.1 MiB)
[2025-07-11T11:46:05.632+0200] {subprocess.py:93} INFO - 25/07/11 11:46:05 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 138.4.31.14:45609 in memory (size: 37.7 KiB, free: 434.1 MiB)
[2025-07-11T11:46:05.961+0200] {subprocess.py:93} INFO - 25/07/11 11:46:05 INFO Executor: Finished task 1.0 in stage 40.0 (TID 62). 2031 bytes result sent to driver
[2025-07-11T11:46:05.962+0200] {subprocess.py:93} INFO - 25/07/11 11:46:05 INFO TaskSetManager: Finished task 1.0 in stage 40.0 (TID 62) in 1643 ms on 138.4.31.14 (executor driver) (1/2)
[2025-07-11T11:46:06.375+0200] {subprocess.py:93} INFO - 25/07/11 11:46:06 INFO BlockManagerInfo: Removed broadcast_34_piece0 on 138.4.31.14:45609 in memory (size: 4.2 KiB, free: 434.1 MiB)
[2025-07-11T11:46:11.895+0200] {subprocess.py:93} INFO - 25/07/11 11:46:11 INFO Executor: Finished task 0.0 in stage 40.0 (TID 61). 2031 bytes result sent to driver
[2025-07-11T11:46:11.895+0200] {subprocess.py:93} INFO - 25/07/11 11:46:11 INFO TaskSetManager: Finished task 0.0 in stage 40.0 (TID 61) in 7576 ms on 138.4.31.14 (executor driver) (2/2)
[2025-07-11T11:46:11.895+0200] {subprocess.py:93} INFO - 25/07/11 11:46:11 INFO TaskSchedulerImpl: Removed TaskSet 40.0, whose tasks have all completed, from pool
[2025-07-11T11:46:11.896+0200] {subprocess.py:93} INFO - 25/07/11 11:46:11 INFO DAGScheduler: ShuffleMapStage 40 (collect at StringIndexer.scala:204) finished in 7,591 s
[2025-07-11T11:46:11.896+0200] {subprocess.py:93} INFO - 25/07/11 11:46:11 INFO DAGScheduler: looking for newly runnable stages
[2025-07-11T11:46:11.896+0200] {subprocess.py:93} INFO - 25/07/11 11:46:11 INFO DAGScheduler: running: Set()
[2025-07-11T11:46:11.896+0200] {subprocess.py:93} INFO - 25/07/11 11:46:11 INFO DAGScheduler: waiting: Set()
[2025-07-11T11:46:11.896+0200] {subprocess.py:93} INFO - 25/07/11 11:46:11 INFO DAGScheduler: failed: Set()
[2025-07-11T11:46:11.900+0200] {subprocess.py:93} INFO - 25/07/11 11:46:11 INFO ShufflePartitionsUtil: For shuffle(12), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576
[2025-07-11T11:46:11.917+0200] {subprocess.py:93} INFO - 25/07/11 11:46:11 INFO CodeGenerator: Code generated in 7.095738 ms
[2025-07-11T11:46:11.921+0200] {subprocess.py:93} INFO - 25/07/11 11:46:11 INFO DAGScheduler: Registering RDD 84 (collect at StringIndexer.scala:204) as input to shuffle 14
[2025-07-11T11:46:11.921+0200] {subprocess.py:93} INFO - 25/07/11 11:46:11 INFO DAGScheduler: Got map stage job 28 (collect at StringIndexer.scala:204) with 1 output partitions
[2025-07-11T11:46:11.921+0200] {subprocess.py:93} INFO - 25/07/11 11:46:11 INFO DAGScheduler: Final stage: ShuffleMapStage 45 (collect at StringIndexer.scala:204)
[2025-07-11T11:46:11.921+0200] {subprocess.py:93} INFO - 25/07/11 11:46:11 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 44)
[2025-07-11T11:46:11.921+0200] {subprocess.py:93} INFO - 25/07/11 11:46:11 INFO DAGScheduler: Missing parents: List()
[2025-07-11T11:46:11.921+0200] {subprocess.py:93} INFO - 25/07/11 11:46:11 INFO DAGScheduler: Submitting ShuffleMapStage 45 (MapPartitionsRDD[84] at collect at StringIndexer.scala:204), which has no missing parents
[2025-07-11T11:46:11.926+0200] {subprocess.py:93} INFO - 25/07/11 11:46:11 INFO MemoryStore: Block broadcast_36 stored as values in memory (estimated size 50.8 KiB, free 425.1 MiB)
[2025-07-11T11:46:11.927+0200] {subprocess.py:93} INFO - 25/07/11 11:46:11 INFO MemoryStore: Block broadcast_36_piece0 stored as bytes in memory (estimated size 24.1 KiB, free 425.1 MiB)
[2025-07-11T11:46:11.927+0200] {subprocess.py:93} INFO - 25/07/11 11:46:11 INFO BlockManagerInfo: Added broadcast_36_piece0 in memory on 138.4.31.14:45609 (size: 24.1 KiB, free: 434.1 MiB)
[2025-07-11T11:46:11.928+0200] {subprocess.py:93} INFO - 25/07/11 11:46:11 INFO SparkContext: Created broadcast 36 from broadcast at DAGScheduler.scala:1585
[2025-07-11T11:46:11.928+0200] {subprocess.py:93} INFO - 25/07/11 11:46:11 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 45 (MapPartitionsRDD[84] at collect at StringIndexer.scala:204) (first 15 tasks are for partitions Vector(0))
[2025-07-11T11:46:11.928+0200] {subprocess.py:93} INFO - 25/07/11 11:46:11 INFO TaskSchedulerImpl: Adding task set 45.0 with 1 tasks resource profile 0
[2025-07-11T11:46:11.929+0200] {subprocess.py:93} INFO - 25/07/11 11:46:11 INFO TaskSetManager: Starting task 0.0 in stage 45.0 (TID 76) (138.4.31.14, executor driver, partition 0, NODE_LOCAL, 12957 bytes)
[2025-07-11T11:46:11.929+0200] {subprocess.py:93} INFO - 25/07/11 11:46:11 INFO Executor: Running task 0.0 in stage 45.0 (TID 76)
[2025-07-11T11:46:11.934+0200] {subprocess.py:93} INFO - 25/07/11 11:46:11 INFO ShuffleBlockFetcherIterator: Getting 2 (1220.4 KiB) non-empty blocks including 2 (1220.4 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2025-07-11T11:46:11.934+0200] {subprocess.py:93} INFO - 25/07/11 11:46:11 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2025-07-11T11:46:11.942+0200] {subprocess.py:93} INFO - 25/07/11 11:46:11 INFO CodeGenerator: Code generated in 7.470694 ms
[2025-07-11T11:46:12.236+0200] {subprocess.py:93} INFO - 25/07/11 11:46:12 INFO Executor: Finished task 0.0 in stage 45.0 (TID 76). 6500 bytes result sent to driver
[2025-07-11T11:46:12.237+0200] {subprocess.py:93} INFO - 25/07/11 11:46:12 INFO TaskSetManager: Finished task 0.0 in stage 45.0 (TID 76) in 307 ms on 138.4.31.14 (executor driver) (1/1)
[2025-07-11T11:46:12.237+0200] {subprocess.py:93} INFO - 25/07/11 11:46:12 INFO TaskSchedulerImpl: Removed TaskSet 45.0, whose tasks have all completed, from pool
[2025-07-11T11:46:12.238+0200] {subprocess.py:93} INFO - 25/07/11 11:46:12 INFO DAGScheduler: ShuffleMapStage 45 (collect at StringIndexer.scala:204) finished in 0,316 s
[2025-07-11T11:46:12.238+0200] {subprocess.py:93} INFO - 25/07/11 11:46:12 INFO DAGScheduler: looking for newly runnable stages
[2025-07-11T11:46:12.238+0200] {subprocess.py:93} INFO - 25/07/11 11:46:12 INFO DAGScheduler: running: Set()
[2025-07-11T11:46:12.238+0200] {subprocess.py:93} INFO - 25/07/11 11:46:12 INFO DAGScheduler: waiting: Set()
[2025-07-11T11:46:12.238+0200] {subprocess.py:93} INFO - 25/07/11 11:46:12 INFO DAGScheduler: failed: Set()
[2025-07-11T11:46:12.254+0200] {subprocess.py:93} INFO - 25/07/11 11:46:12 INFO SparkContext: Starting job: collect at StringIndexer.scala:204
[2025-07-11T11:46:12.255+0200] {subprocess.py:93} INFO - 25/07/11 11:46:12 INFO DAGScheduler: Got job 29 (collect at StringIndexer.scala:204) with 1 output partitions
[2025-07-11T11:46:12.255+0200] {subprocess.py:93} INFO - 25/07/11 11:46:12 INFO DAGScheduler: Final stage: ResultStage 48 (collect at StringIndexer.scala:204)
[2025-07-11T11:46:12.255+0200] {subprocess.py:93} INFO - 25/07/11 11:46:12 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 47)
[2025-07-11T11:46:12.255+0200] {subprocess.py:93} INFO - 25/07/11 11:46:12 INFO DAGScheduler: Missing parents: List()
[2025-07-11T11:46:12.255+0200] {subprocess.py:93} INFO - 25/07/11 11:46:12 INFO DAGScheduler: Submitting ResultStage 48 (MapPartitionsRDD[87] at collect at StringIndexer.scala:204), which has no missing parents
[2025-07-11T11:46:12.258+0200] {subprocess.py:93} INFO - 25/07/11 11:46:12 INFO MemoryStore: Block broadcast_37 stored as values in memory (estimated size 50.0 KiB, free 425.1 MiB)
[2025-07-11T11:46:12.259+0200] {subprocess.py:93} INFO - 25/07/11 11:46:12 INFO MemoryStore: Block broadcast_37_piece0 stored as bytes in memory (estimated size 23.8 KiB, free 425.0 MiB)
[2025-07-11T11:46:12.259+0200] {subprocess.py:93} INFO - 25/07/11 11:46:12 INFO BlockManagerInfo: Added broadcast_37_piece0 in memory on 138.4.31.14:45609 (size: 23.8 KiB, free: 434.1 MiB)
[2025-07-11T11:46:12.259+0200] {subprocess.py:93} INFO - 25/07/11 11:46:12 INFO SparkContext: Created broadcast 37 from broadcast at DAGScheduler.scala:1585
[2025-07-11T11:46:12.260+0200] {subprocess.py:93} INFO - 25/07/11 11:46:12 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 48 (MapPartitionsRDD[87] at collect at StringIndexer.scala:204) (first 15 tasks are for partitions Vector(0))
[2025-07-11T11:46:12.260+0200] {subprocess.py:93} INFO - 25/07/11 11:46:12 INFO TaskSchedulerImpl: Adding task set 48.0 with 1 tasks resource profile 0
[2025-07-11T11:46:12.261+0200] {subprocess.py:93} INFO - 25/07/11 11:46:12 INFO TaskSetManager: Starting task 0.0 in stage 48.0 (TID 77) (138.4.31.14, executor driver, partition 0, NODE_LOCAL, 12952 bytes)
[2025-07-11T11:46:12.261+0200] {subprocess.py:93} INFO - 25/07/11 11:46:12 INFO Executor: Running task 0.0 in stage 48.0 (TID 77)
[2025-07-11T11:46:12.268+0200] {subprocess.py:93} INFO - 25/07/11 11:46:12 INFO ShuffleBlockFetcherIterator: Getting 1 (46.5 KiB) non-empty blocks including 1 (46.5 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2025-07-11T11:46:12.268+0200] {subprocess.py:93} INFO - 25/07/11 11:46:12 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2025-07-11T11:46:12.298+0200] {subprocess.py:93} INFO - 25/07/11 11:46:12 INFO BlockManagerInfo: Removed broadcast_36_piece0 on 138.4.31.14:45609 in memory (size: 24.1 KiB, free: 434.1 MiB)
[2025-07-11T11:46:12.298+0200] {subprocess.py:93} INFO - 25/07/11 11:46:12 INFO Executor: Finished task 0.0 in stage 48.0 (TID 77). 53422 bytes result sent to driver
[2025-07-11T11:46:12.299+0200] {subprocess.py:93} INFO - 25/07/11 11:46:12 INFO TaskSetManager: Finished task 0.0 in stage 48.0 (TID 77) in 39 ms on 138.4.31.14 (executor driver) (1/1)
[2025-07-11T11:46:12.299+0200] {subprocess.py:93} INFO - 25/07/11 11:46:12 INFO TaskSchedulerImpl: Removed TaskSet 48.0, whose tasks have all completed, from pool
[2025-07-11T11:46:12.299+0200] {subprocess.py:93} INFO - 25/07/11 11:46:12 INFO DAGScheduler: ResultStage 48 (collect at StringIndexer.scala:204) finished in 0,043 s
[2025-07-11T11:46:12.299+0200] {subprocess.py:93} INFO - 25/07/11 11:46:12 INFO DAGScheduler: Job 29 is finished. Cancelling potential speculative or zombie tasks for this job
[2025-07-11T11:46:12.299+0200] {subprocess.py:93} INFO - 25/07/11 11:46:12 INFO TaskSchedulerImpl: Killing all running tasks in stage 48: Stage finished
[2025-07-11T11:46:12.299+0200] {subprocess.py:93} INFO - 25/07/11 11:46:12 INFO DAGScheduler: Job 29 finished: collect at StringIndexer.scala:204, took 0,045372 s
[2025-07-11T11:46:12.361+0200] {subprocess.py:93} INFO - 25/07/11 11:46:12 INFO FileSystemOverwrite: Path /home/monica.fernandez/practica_creativa/models/string_indexer_model_Route.bin already exists. It will be overwritten.
[2025-07-11T11:46:12.373+0200] {subprocess.py:93} INFO - 25/07/11 11:46:12 INFO HadoopMapRedCommitProtocol: Using output committer class org.apache.hadoop.mapred.FileOutputCommitter
[2025-07-11T11:46:12.373+0200] {subprocess.py:93} INFO - 25/07/11 11:46:12 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2025-07-11T11:46:12.374+0200] {subprocess.py:93} INFO - 25/07/11 11:46:12 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2025-07-11T11:46:12.401+0200] {subprocess.py:93} INFO - 25/07/11 11:46:12 INFO SparkContext: Starting job: runJob at SparkHadoopWriter.scala:83
[2025-07-11T11:46:12.402+0200] {subprocess.py:93} INFO - 25/07/11 11:46:12 INFO DAGScheduler: Got job 30 (runJob at SparkHadoopWriter.scala:83) with 1 output partitions
[2025-07-11T11:46:12.402+0200] {subprocess.py:93} INFO - 25/07/11 11:46:12 INFO DAGScheduler: Final stage: ResultStage 49 (runJob at SparkHadoopWriter.scala:83)
[2025-07-11T11:46:12.402+0200] {subprocess.py:93} INFO - 25/07/11 11:46:12 INFO DAGScheduler: Parents of final stage: List()
[2025-07-11T11:46:12.402+0200] {subprocess.py:93} INFO - 25/07/11 11:46:12 INFO DAGScheduler: Missing parents: List()
[2025-07-11T11:46:12.402+0200] {subprocess.py:93} INFO - 25/07/11 11:46:12 INFO DAGScheduler: Submitting ResultStage 49 (MapPartitionsRDD[89] at saveAsTextFile at ReadWrite.scala:413), which has no missing parents
[2025-07-11T11:46:12.410+0200] {subprocess.py:93} INFO - 25/07/11 11:46:12 INFO MemoryStore: Block broadcast_38 stored as values in memory (estimated size 101.3 KiB, free 425.0 MiB)
[2025-07-11T11:46:12.412+0200] {subprocess.py:93} INFO - 25/07/11 11:46:12 INFO MemoryStore: Block broadcast_38_piece0 stored as bytes in memory (estimated size 36.5 KiB, free 425.0 MiB)
[2025-07-11T11:46:12.412+0200] {subprocess.py:93} INFO - 25/07/11 11:46:12 INFO BlockManagerInfo: Added broadcast_38_piece0 in memory on 138.4.31.14:45609 (size: 36.5 KiB, free: 434.1 MiB)
[2025-07-11T11:46:12.413+0200] {subprocess.py:93} INFO - 25/07/11 11:46:12 INFO SparkContext: Created broadcast 38 from broadcast at DAGScheduler.scala:1585
[2025-07-11T11:46:12.414+0200] {subprocess.py:93} INFO - 25/07/11 11:46:12 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 49 (MapPartitionsRDD[89] at saveAsTextFile at ReadWrite.scala:413) (first 15 tasks are for partitions Vector(0))
[2025-07-11T11:46:12.414+0200] {subprocess.py:93} INFO - 25/07/11 11:46:12 INFO TaskSchedulerImpl: Adding task set 49.0 with 1 tasks resource profile 0
[2025-07-11T11:46:12.415+0200] {subprocess.py:93} INFO - 25/07/11 11:46:12 INFO TaskSetManager: Starting task 0.0 in stage 49.0 (TID 78) (138.4.31.14, executor driver, partition 0, PROCESS_LOCAL, 13278 bytes)
[2025-07-11T11:46:12.415+0200] {subprocess.py:93} INFO - 25/07/11 11:46:12 INFO Executor: Running task 0.0 in stage 49.0 (TID 78)
[2025-07-11T11:46:12.420+0200] {subprocess.py:93} INFO - 25/07/11 11:46:12 INFO HadoopMapRedCommitProtocol: Using output committer class org.apache.hadoop.mapred.FileOutputCommitter
[2025-07-11T11:46:12.420+0200] {subprocess.py:93} INFO - 25/07/11 11:46:12 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2025-07-11T11:46:12.420+0200] {subprocess.py:93} INFO - 25/07/11 11:46:12 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2025-07-11T11:46:12.445+0200] {subprocess.py:93} INFO - 25/07/11 11:46:12 INFO FileOutputCommitter: Saved output of task 'attempt_202507111146125950071379345093286_0089_m_000000_0' to file:/home/monica.fernandez/practica_creativa/models/string_indexer_model_Route.bin/metadata/_temporary/0/task_202507111146125950071379345093286_0089_m_000000
[2025-07-11T11:46:12.446+0200] {subprocess.py:93} INFO - 25/07/11 11:46:12 INFO SparkHadoopMapRedUtil: attempt_202507111146125950071379345093286_0089_m_000000_0: Committed. Elapsed time: 1 ms.
[2025-07-11T11:46:12.449+0200] {subprocess.py:93} INFO - 25/07/11 11:46:12 INFO Executor: Finished task 0.0 in stage 49.0 (TID 78). 1170 bytes result sent to driver
[2025-07-11T11:46:12.450+0200] {subprocess.py:93} INFO - 25/07/11 11:46:12 INFO TaskSetManager: Finished task 0.0 in stage 49.0 (TID 78) in 36 ms on 138.4.31.14 (executor driver) (1/1)
[2025-07-11T11:46:12.450+0200] {subprocess.py:93} INFO - 25/07/11 11:46:12 INFO TaskSchedulerImpl: Removed TaskSet 49.0, whose tasks have all completed, from pool
[2025-07-11T11:46:12.450+0200] {subprocess.py:93} INFO - 25/07/11 11:46:12 INFO DAGScheduler: ResultStage 49 (runJob at SparkHadoopWriter.scala:83) finished in 0,047 s
[2025-07-11T11:46:12.450+0200] {subprocess.py:93} INFO - 25/07/11 11:46:12 INFO DAGScheduler: Job 30 is finished. Cancelling potential speculative or zombie tasks for this job
[2025-07-11T11:46:12.450+0200] {subprocess.py:93} INFO - 25/07/11 11:46:12 INFO TaskSchedulerImpl: Killing all running tasks in stage 49: Stage finished
[2025-07-11T11:46:12.451+0200] {subprocess.py:93} INFO - 25/07/11 11:46:12 INFO DAGScheduler: Job 30 finished: runJob at SparkHadoopWriter.scala:83, took 0,048911 s
[2025-07-11T11:46:12.451+0200] {subprocess.py:93} INFO - 25/07/11 11:46:12 INFO SparkHadoopWriter: Start to commit write Job job_202507111146125950071379345093286_0089.
[2025-07-11T11:46:12.471+0200] {subprocess.py:93} INFO - 25/07/11 11:46:12 INFO SparkHadoopWriter: Write Job job_202507111146125950071379345093286_0089 committed. Elapsed time: 20 ms.
[2025-07-11T11:46:12.499+0200] {subprocess.py:93} INFO - 25/07/11 11:46:12 INFO DAGScheduler: Registering RDD 92 (parquet at StringIndexer.scala:499) as input to shuffle 15
[2025-07-11T11:46:12.499+0200] {subprocess.py:93} INFO - 25/07/11 11:46:12 INFO DAGScheduler: Got map stage job 31 (parquet at StringIndexer.scala:499) with 1 output partitions
[2025-07-11T11:46:12.499+0200] {subprocess.py:93} INFO - 25/07/11 11:46:12 INFO DAGScheduler: Final stage: ShuffleMapStage 50 (parquet at StringIndexer.scala:499)
[2025-07-11T11:46:12.499+0200] {subprocess.py:93} INFO - 25/07/11 11:46:12 INFO DAGScheduler: Parents of final stage: List()
[2025-07-11T11:46:12.499+0200] {subprocess.py:93} INFO - 25/07/11 11:46:12 INFO DAGScheduler: Missing parents: List()
[2025-07-11T11:46:12.500+0200] {subprocess.py:93} INFO - 25/07/11 11:46:12 INFO DAGScheduler: Submitting ShuffleMapStage 50 (MapPartitionsRDD[92] at parquet at StringIndexer.scala:499), which has no missing parents
[2025-07-11T11:46:12.500+0200] {subprocess.py:93} INFO - 25/07/11 11:46:12 INFO MemoryStore: Block broadcast_39 stored as values in memory (estimated size 8.1 KiB, free 425.0 MiB)
[2025-07-11T11:46:12.501+0200] {subprocess.py:93} INFO - 25/07/11 11:46:12 INFO MemoryStore: Block broadcast_39_piece0 stored as bytes in memory (estimated size 4.4 KiB, free 425.0 MiB)
[2025-07-11T11:46:12.501+0200] {subprocess.py:93} INFO - 25/07/11 11:46:12 INFO BlockManagerInfo: Added broadcast_39_piece0 in memory on 138.4.31.14:45609 (size: 4.4 KiB, free: 434.1 MiB)
[2025-07-11T11:46:12.502+0200] {subprocess.py:93} INFO - 25/07/11 11:46:12 INFO SparkContext: Created broadcast 39 from broadcast at DAGScheduler.scala:1585
[2025-07-11T11:46:12.502+0200] {subprocess.py:93} INFO - 25/07/11 11:46:12 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 50 (MapPartitionsRDD[92] at parquet at StringIndexer.scala:499) (first 15 tasks are for partitions Vector(0))
[2025-07-11T11:46:12.502+0200] {subprocess.py:93} INFO - 25/07/11 11:46:12 INFO TaskSchedulerImpl: Adding task set 50.0 with 1 tasks resource profile 0
[2025-07-11T11:46:12.503+0200] {subprocess.py:93} INFO - 25/07/11 11:46:12 INFO TaskSetManager: Starting task 0.0 in stage 50.0 (TID 79) (138.4.31.14, executor driver, partition 0, PROCESS_LOCAL, 80641 bytes)
[2025-07-11T11:46:12.504+0200] {subprocess.py:93} INFO - 25/07/11 11:46:12 INFO Executor: Running task 0.0 in stage 50.0 (TID 79)
[2025-07-11T11:46:12.507+0200] {subprocess.py:93} INFO - 25/07/11 11:46:12 INFO Executor: Finished task 0.0 in stage 50.0 (TID 79). 1628 bytes result sent to driver
[2025-07-11T11:46:12.508+0200] {subprocess.py:93} INFO - 25/07/11 11:46:12 INFO TaskSetManager: Finished task 0.0 in stage 50.0 (TID 79) in 6 ms on 138.4.31.14 (executor driver) (1/1)
[2025-07-11T11:46:12.509+0200] {subprocess.py:93} INFO - 25/07/11 11:46:12 INFO TaskSchedulerImpl: Removed TaskSet 50.0, whose tasks have all completed, from pool
[2025-07-11T11:46:12.509+0200] {subprocess.py:93} INFO - 25/07/11 11:46:12 INFO DAGScheduler: ShuffleMapStage 50 (parquet at StringIndexer.scala:499) finished in 0,010 s
[2025-07-11T11:46:12.509+0200] {subprocess.py:93} INFO - 25/07/11 11:46:12 INFO DAGScheduler: looking for newly runnable stages
[2025-07-11T11:46:12.509+0200] {subprocess.py:93} INFO - 25/07/11 11:46:12 INFO DAGScheduler: running: Set()
[2025-07-11T11:46:12.509+0200] {subprocess.py:93} INFO - 25/07/11 11:46:12 INFO DAGScheduler: waiting: Set()
[2025-07-11T11:46:12.509+0200] {subprocess.py:93} INFO - 25/07/11 11:46:12 INFO DAGScheduler: failed: Set()
[2025-07-11T11:46:12.515+0200] {subprocess.py:93} INFO - 25/07/11 11:46:12 INFO ParquetUtils: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
[2025-07-11T11:46:12.515+0200] {subprocess.py:93} INFO - 25/07/11 11:46:12 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2025-07-11T11:46:12.515+0200] {subprocess.py:93} INFO - 25/07/11 11:46:12 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2025-07-11T11:46:12.515+0200] {subprocess.py:93} INFO - 25/07/11 11:46:12 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[2025-07-11T11:46:12.516+0200] {subprocess.py:93} INFO - 25/07/11 11:46:12 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2025-07-11T11:46:12.516+0200] {subprocess.py:93} INFO - 25/07/11 11:46:12 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2025-07-11T11:46:12.516+0200] {subprocess.py:93} INFO - 25/07/11 11:46:12 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[2025-07-11T11:46:12.533+0200] {subprocess.py:93} INFO - 25/07/11 11:46:12 INFO SparkContext: Starting job: parquet at StringIndexer.scala:499
[2025-07-11T11:46:12.533+0200] {subprocess.py:93} INFO - 25/07/11 11:46:12 INFO DAGScheduler: Got job 32 (parquet at StringIndexer.scala:499) with 1 output partitions
[2025-07-11T11:46:12.533+0200] {subprocess.py:93} INFO - 25/07/11 11:46:12 INFO DAGScheduler: Final stage: ResultStage 52 (parquet at StringIndexer.scala:499)
[2025-07-11T11:46:12.534+0200] {subprocess.py:93} INFO - 25/07/11 11:46:12 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 51)
[2025-07-11T11:46:12.536+0200] {subprocess.py:93} INFO - 25/07/11 11:46:12 INFO DAGScheduler: Missing parents: List()
[2025-07-11T11:46:12.537+0200] {subprocess.py:93} INFO - 25/07/11 11:46:12 INFO DAGScheduler: Submitting ResultStage 52 (MapPartitionsRDD[94] at parquet at StringIndexer.scala:499), which has no missing parents
[2025-07-11T11:46:12.551+0200] {subprocess.py:93} INFO - 25/07/11 11:46:12 INFO MemoryStore: Block broadcast_40 stored as values in memory (estimated size 230.2 KiB, free 424.7 MiB)
[2025-07-11T11:46:12.552+0200] {subprocess.py:93} INFO - 25/07/11 11:46:12 INFO MemoryStore: Block broadcast_40_piece0 stored as bytes in memory (estimated size 81.0 KiB, free 424.7 MiB)
[2025-07-11T11:46:12.552+0200] {subprocess.py:93} INFO - 25/07/11 11:46:12 INFO BlockManagerInfo: Added broadcast_40_piece0 in memory on 138.4.31.14:45609 (size: 81.0 KiB, free: 434.0 MiB)
[2025-07-11T11:46:12.553+0200] {subprocess.py:93} INFO - 25/07/11 11:46:12 INFO SparkContext: Created broadcast 40 from broadcast at DAGScheduler.scala:1585
[2025-07-11T11:46:12.553+0200] {subprocess.py:93} INFO - 25/07/11 11:46:12 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 52 (MapPartitionsRDD[94] at parquet at StringIndexer.scala:499) (first 15 tasks are for partitions Vector(0))
[2025-07-11T11:46:12.553+0200] {subprocess.py:93} INFO - 25/07/11 11:46:12 INFO TaskSchedulerImpl: Adding task set 52.0 with 1 tasks resource profile 0
[2025-07-11T11:46:12.554+0200] {subprocess.py:93} INFO - 25/07/11 11:46:12 INFO TaskSetManager: Starting task 0.0 in stage 52.0 (TID 80) (138.4.31.14, executor driver, partition 0, NODE_LOCAL, 12952 bytes)
[2025-07-11T11:46:12.554+0200] {subprocess.py:93} INFO - 25/07/11 11:46:12 INFO Executor: Running task 0.0 in stage 52.0 (TID 80)
[2025-07-11T11:46:12.565+0200] {subprocess.py:93} INFO - 25/07/11 11:46:12 INFO ShuffleBlockFetcherIterator: Getting 1 (42.2 KiB) non-empty blocks including 1 (42.2 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2025-07-11T11:46:12.565+0200] {subprocess.py:93} INFO - 25/07/11 11:46:12 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2025-07-11T11:46:12.566+0200] {subprocess.py:93} INFO - 25/07/11 11:46:12 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2025-07-11T11:46:12.567+0200] {subprocess.py:93} INFO - 25/07/11 11:46:12 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2025-07-11T11:46:12.567+0200] {subprocess.py:93} INFO - 25/07/11 11:46:12 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[2025-07-11T11:46:12.567+0200] {subprocess.py:93} INFO - 25/07/11 11:46:12 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2025-07-11T11:46:12.567+0200] {subprocess.py:93} INFO - 25/07/11 11:46:12 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2025-07-11T11:46:12.567+0200] {subprocess.py:93} INFO - 25/07/11 11:46:12 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[2025-07-11T11:46:12.567+0200] {subprocess.py:93} INFO - 25/07/11 11:46:12 INFO CodecConfig: Compression: SNAPPY
[2025-07-11T11:46:12.567+0200] {subprocess.py:93} INFO - 25/07/11 11:46:12 INFO CodecConfig: Compression: SNAPPY
[2025-07-11T11:46:12.568+0200] {subprocess.py:93} INFO - 25/07/11 11:46:12 INFO ParquetOutputFormat: ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]
[2025-07-11T11:46:12.569+0200] {subprocess.py:93} INFO - 25/07/11 11:46:12 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
[2025-07-11T11:46:12.569+0200] {subprocess.py:93} INFO - {
[2025-07-11T11:46:12.569+0200] {subprocess.py:93} INFO -   "type" : "struct",
[2025-07-11T11:46:12.569+0200] {subprocess.py:93} INFO -   "fields" : [ {
[2025-07-11T11:46:12.569+0200] {subprocess.py:93} INFO -     "name" : "labelsArray",
[2025-07-11T11:46:12.569+0200] {subprocess.py:93} INFO -     "type" : {
[2025-07-11T11:46:12.570+0200] {subprocess.py:93} INFO -       "type" : "array",
[2025-07-11T11:46:12.570+0200] {subprocess.py:93} INFO -       "elementType" : {
[2025-07-11T11:46:12.570+0200] {subprocess.py:93} INFO -         "type" : "array",
[2025-07-11T11:46:12.570+0200] {subprocess.py:93} INFO -         "elementType" : "string",
[2025-07-11T11:46:12.570+0200] {subprocess.py:93} INFO -         "containsNull" : true
[2025-07-11T11:46:12.570+0200] {subprocess.py:93} INFO -       },
[2025-07-11T11:46:12.570+0200] {subprocess.py:93} INFO -       "containsNull" : true
[2025-07-11T11:46:12.570+0200] {subprocess.py:93} INFO -     },
[2025-07-11T11:46:12.570+0200] {subprocess.py:93} INFO -     "nullable" : true,
[2025-07-11T11:46:12.570+0200] {subprocess.py:93} INFO -     "metadata" : { }
[2025-07-11T11:46:12.570+0200] {subprocess.py:93} INFO -   } ]
[2025-07-11T11:46:12.570+0200] {subprocess.py:93} INFO - }
[2025-07-11T11:46:12.570+0200] {subprocess.py:93} INFO - and corresponding Parquet message type:
[2025-07-11T11:46:12.570+0200] {subprocess.py:93} INFO - message spark_schema {
[2025-07-11T11:46:12.570+0200] {subprocess.py:93} INFO -   optional group labelsArray (LIST) {
[2025-07-11T11:46:12.570+0200] {subprocess.py:93} INFO -     repeated group list {
[2025-07-11T11:46:12.570+0200] {subprocess.py:93} INFO -       optional group element (LIST) {
[2025-07-11T11:46:12.570+0200] {subprocess.py:93} INFO -         repeated group list {
[2025-07-11T11:46:12.571+0200] {subprocess.py:93} INFO -           optional binary element (STRING);
[2025-07-11T11:46:12.571+0200] {subprocess.py:93} INFO -         }
[2025-07-11T11:46:12.571+0200] {subprocess.py:93} INFO -       }
[2025-07-11T11:46:12.571+0200] {subprocess.py:93} INFO -     }
[2025-07-11T11:46:12.571+0200] {subprocess.py:93} INFO -   }
[2025-07-11T11:46:12.571+0200] {subprocess.py:93} INFO - }
[2025-07-11T11:46:12.571+0200] {subprocess.py:93} INFO - 
[2025-07-11T11:46:12.571+0200] {subprocess.py:93} INFO - 
[2025-07-11T11:46:12.620+0200] {subprocess.py:93} INFO - 25/07/11 11:46:12 INFO FileOutputCommitter: Saved output of task 'attempt_202507111146122499182091739306913_0052_m_000000_80' to file:/home/monica.fernandez/practica_creativa/models/string_indexer_model_Route.bin/data/_temporary/0/task_202507111146122499182091739306913_0052_m_000000
[2025-07-11T11:46:12.620+0200] {subprocess.py:93} INFO - 25/07/11 11:46:12 INFO SparkHadoopMapRedUtil: attempt_202507111146122499182091739306913_0052_m_000000_80: Committed. Elapsed time: 1 ms.
[2025-07-11T11:46:12.621+0200] {subprocess.py:93} INFO - 25/07/11 11:46:12 INFO Executor: Finished task 0.0 in stage 52.0 (TID 80). 4740 bytes result sent to driver
[2025-07-11T11:46:12.621+0200] {subprocess.py:93} INFO - 25/07/11 11:46:12 INFO TaskSetManager: Finished task 0.0 in stage 52.0 (TID 80) in 67 ms on 138.4.31.14 (executor driver) (1/1)
[2025-07-11T11:46:12.622+0200] {subprocess.py:93} INFO - 25/07/11 11:46:12 INFO TaskSchedulerImpl: Removed TaskSet 52.0, whose tasks have all completed, from pool
[2025-07-11T11:46:12.623+0200] {subprocess.py:93} INFO - 25/07/11 11:46:12 INFO DAGScheduler: ResultStage 52 (parquet at StringIndexer.scala:499) finished in 0,087 s
[2025-07-11T11:46:12.624+0200] {subprocess.py:93} INFO - 25/07/11 11:46:12 INFO DAGScheduler: Job 32 is finished. Cancelling potential speculative or zombie tasks for this job
[2025-07-11T11:46:12.624+0200] {subprocess.py:93} INFO - 25/07/11 11:46:12 INFO TaskSchedulerImpl: Killing all running tasks in stage 52: Stage finished
[2025-07-11T11:46:12.624+0200] {subprocess.py:93} INFO - 25/07/11 11:46:12 INFO DAGScheduler: Job 32 finished: parquet at StringIndexer.scala:499, took 0,089710 s
[2025-07-11T11:46:12.624+0200] {subprocess.py:93} INFO - 25/07/11 11:46:12 INFO FileFormatWriter: Start to commit write Job a6f409be-503d-4eb9-bcef-e1b8a8a1b34e.
[2025-07-11T11:46:12.643+0200] {subprocess.py:93} INFO - 25/07/11 11:46:12 INFO FileFormatWriter: Write Job a6f409be-503d-4eb9-bcef-e1b8a8a1b34e committed. Elapsed time: 19 ms.
[2025-07-11T11:46:12.643+0200] {subprocess.py:93} INFO - 25/07/11 11:46:12 INFO FileFormatWriter: Finished processing stats for write job a6f409be-503d-4eb9-bcef-e1b8a8a1b34e.
[2025-07-11T11:46:12.827+0200] {subprocess.py:93} INFO - Traceback (most recent call last):
[2025-07-11T11:46:12.827+0200] {subprocess.py:93} INFO -   File "/home/monica.fernandez/practica_creativa/resources/train_spark_mllib_model.py", line 147, in <module>
[2025-07-11T11:46:12.828+0200] {subprocess.py:93} INFO -     main(sys.argv[1] if len(sys.argv) > 1 else ".")
[2025-07-11T11:46:12.828+0200] {subprocess.py:93} INFO -   File "/home/monica.fernandez/practica_creativa/resources/train_spark_mllib_model.py", line 97, in main
[2025-07-11T11:46:12.828+0200] {subprocess.py:93} INFO -     final_vectorized_features = vector_assembler.transform(ml_bucketized_features)
[2025-07-11T11:46:12.828+0200] {subprocess.py:93} INFO -                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2025-07-11T11:46:12.828+0200] {subprocess.py:93} INFO -   File "/home/monica.fernandez/.sdkman/candidates/spark/current/python/lib/pyspark.zip/pyspark/ml/base.py", line 262, in transform
[2025-07-11T11:46:12.833+0200] {subprocess.py:93} INFO -   File "/home/monica.fernandez/.sdkman/candidates/spark/current/python/lib/pyspark.zip/pyspark/ml/wrapper.py", line 398, in _transform
[2025-07-11T11:46:12.834+0200] {subprocess.py:93} INFO -   File "/home/monica.fernandez/.sdkman/candidates/spark/current/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py", line 1322, in __call__
[2025-07-11T11:46:12.835+0200] {subprocess.py:93} INFO -   File "/home/monica.fernandez/.sdkman/candidates/spark/current/python/lib/pyspark.zip/pyspark/errors/exceptions/captured.py", line 185, in deco
[2025-07-11T11:46:12.842+0200] {subprocess.py:93} INFO - pyspark.errors.exceptions.captured.AnalysisException: [AMBIGUOUS_REFERENCE] Reference `Distance` is ambiguous, could be: [`Distance`, `Distance`].
[2025-07-11T11:46:13.629+0200] {subprocess.py:93} INFO - 25/07/11 11:46:13 INFO SparkContext: Invoking stop() from shutdown hook
[2025-07-11T11:46:13.630+0200] {subprocess.py:93} INFO - 25/07/11 11:46:13 INFO SparkContext: SparkContext is stopping with exitCode 0.
[2025-07-11T11:46:13.653+0200] {subprocess.py:93} INFO - 25/07/11 11:46:13 INFO SparkUI: Stopped Spark web UI at http://138.4.31.14:4041
[2025-07-11T11:46:13.656+0200] {subprocess.py:93} INFO - 25/07/11 11:46:13 INFO CassandraConnector: Disconnected from Cassandra cluster.
[2025-07-11T11:46:13.656+0200] {subprocess.py:93} INFO - 25/07/11 11:46:13 INFO SerialShutdownHooks: Successfully executed shutdown hook: Clearing session cache for C* connector
[2025-07-11T11:46:13.678+0200] {subprocess.py:93} INFO - 25/07/11 11:46:13 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
[2025-07-11T11:46:13.703+0200] {subprocess.py:93} INFO - 25/07/11 11:46:13 INFO MemoryStore: MemoryStore cleared
[2025-07-11T11:46:13.705+0200] {subprocess.py:93} INFO - 25/07/11 11:46:13 INFO BlockManager: BlockManager stopped
[2025-07-11T11:46:13.713+0200] {subprocess.py:93} INFO - 25/07/11 11:46:13 INFO BlockManagerMaster: BlockManagerMaster stopped
[2025-07-11T11:46:13.718+0200] {subprocess.py:93} INFO - 25/07/11 11:46:13 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
[2025-07-11T11:46:13.735+0200] {subprocess.py:93} INFO - 25/07/11 11:46:13 INFO SparkContext: Successfully stopped SparkContext
[2025-07-11T11:46:13.735+0200] {subprocess.py:93} INFO - 25/07/11 11:46:13 INFO ShutdownHookManager: Shutdown hook called
[2025-07-11T11:46:13.736+0200] {subprocess.py:93} INFO - 25/07/11 11:46:13 INFO ShutdownHookManager: Deleting directory /tmp/spark-36b5bbfe-8223-445b-b376-dec57e3c841e
[2025-07-11T11:46:13.742+0200] {subprocess.py:93} INFO - 25/07/11 11:46:13 INFO ShutdownHookManager: Deleting directory /tmp/spark-36b5bbfe-8223-445b-b376-dec57e3c841e/pyspark-887e968c-2e82-478f-99b9-8c7529e02360
[2025-07-11T11:46:13.748+0200] {subprocess.py:93} INFO - 25/07/11 11:46:13 INFO ShutdownHookManager: Deleting directory /tmp/spark-1c29c01a-f762-4672-a3ad-080360467fa4
[2025-07-11T11:46:13.907+0200] {subprocess.py:97} INFO - Command exited with return code 1
[2025-07-11T11:46:13.926+0200] {taskinstance.py:1768} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/monica.fernandez/practica_creativa/venv-airflow/lib/python3.11/site-packages/airflow/operators/bash.py", line 196, in execute
    raise AirflowException(
airflow.exceptions.AirflowException: Bash command failed. The command returned a non-zero exit code 1.
[2025-07-11T11:46:13.931+0200] {taskinstance.py:1318} INFO - Marking task as UP_FOR_RETRY. dag_id=agile_data_science_batch_prediction_model_training, task_id=pyspark_train_classifier_model, execution_date=20250711T094508, start_date=20250711T094514, end_date=20250711T094613
[2025-07-11T11:46:13.956+0200] {standard_task_runner.py:100} ERROR - Failed to execute job 137 for task pyspark_train_classifier_model (Bash command failed. The command returned a non-zero exit code 1.; 88248)
[2025-07-11T11:46:14.004+0200] {local_task_job.py:208} INFO - Task exited with return code 1
[2025-07-11T11:46:14.039+0200] {taskinstance.py:2578} INFO - 0 downstream tasks scheduled from follow-on schedule check
